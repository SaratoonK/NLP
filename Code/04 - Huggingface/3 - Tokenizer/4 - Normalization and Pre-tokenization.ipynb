{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyt5g3zwusxe"
   },
   "source": [
    "# Normalization and pre-tokenization\n",
    "\n",
    "Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we‚Äôll first take a look at the preprocessing that each tokenizer applies to text. Here‚Äôs a high-level overview of the steps in the tokenization pipeline:\n",
    "\n",
    "<img src = \"../../figures/pretoken.png\" >\n",
    "\n",
    "Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: normalization and pre-tokenization.\n",
    "\n",
    "The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you‚Äôre familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n",
    "\n",
    "The ü§ó Transformers tokenizer has an attribute called backend_tokenizer that provides access to the underlying tokenizer from the ü§ó Tokenizers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7OyFsPousxk",
    "outputId": "4abdf57c-5979-4f9e-f71e-5a0f9b94c44b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'tokenizers.Tokenizer'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The normalizer attribute of the tokenizer object has a normalize_str() method that we can use to see how the normalization is performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVkkq-lTusxm",
    "outputId": "8a9497fd-177c-49c9-cd1b-1e065dbf0c2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, since we picked the bert-base-uncased checkpoint, the normalization applied lowercasing and removed the accents.\n",
    "\n",
    "As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That‚Äôs where the pre-tokenization step comes in. As we saw in Chapter 2, a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n",
    "\n",
    "To see how a fast tokenizer performs pre-tokenization, we can use the pre_tokenize_str() method of the pre_tokenizer attribute of the tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDFhEPppusxn",
    "outputId": "c5522d0b-4478-4178-84f6-9f6ddea751a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between are and you to account for that.\n",
    "\n",
    "Since we‚Äôre using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUgvHxQlusxn",
    "outputId": "aa177ff9-af38-4835-c0b3-2bed232c3985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it will split on whitespace and punctuation as well, \n",
    "#but it will keep the spaces and replace them with a ƒ† symbol, \n",
    "#enabling it to recover the original spaces if we decode the \n",
    "#tokens:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\n",
    "\n",
    "For a last example, let‚Äôs have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhc2m7qMusxo",
    "outputId": "0c8db1ad-bd0f-4ef2-d778-89de289f1bfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (_), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before Hello) and ignored the double space between are and you.\n",
    "\n",
    "Now that we‚Äôve seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We‚Äôll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we‚Äôll examine how the three main algorithms used for subword tokenization work.\n",
    "\n",
    "SentencePiece\n",
    "\n",
    "SentencePiece is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, ‚ñÅ. Used in conjunction with the Unigram algorithm (see section 7), it doesn‚Äôt even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\n",
    "\n",
    "The other main feature of SentencePiece is reversible tokenization: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the _s with spaces ‚Äî this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n",
    "\n",
    "Algorithm overview\n",
    "\n",
    "In the following sections, we‚Äôll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here‚Äôs a quick overview of how they each work. Don‚Äôt hesitate to come back to this table after reading each of the next sections if it doesn‚Äôt make sense to you yet.\n",
    "\n",
    "Model\tBPE\tWordPiece\tUnigram\n",
    "Training\tStarts from a small vocabulary and learns rules to merge tokens\tStarts from a small vocabulary and learns rules to merge tokens\tStarts from a large vocabulary and learns rules to remove tokens\n",
    "Training step\tMerges the tokens corresponding to the most common pair\tMerges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent\tRemoves all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\n",
    "Learns\tMerge rules and a vocabulary\tJust a vocabulary\tA vocabulary with a score for each token\n",
    "Encoding\tSplits a word into characters and applies the merges learned during training\tFinds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word\tFinds the most likely split into tokens, using the scores learned during training\n",
    "Now let‚Äôs dive into BPE!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Normalization and pre-tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
