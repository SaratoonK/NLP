{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGZVcxyZt1Wq"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## 3 - Big Data\n",
    "\n",
    "Nowadays it is not uncommon to find yourself working with multi-gigabyte datasets, especially if you‚Äôre planning to pretrain a transformer like BERT or GPT-2 from scratch. In these cases, even loading the data can be a challenge. For example, the WebText corpus used to pretrain GPT-2 consists of over 8 million documents and 40 GB of text ‚Äî loading this into your laptop‚Äôs RAM is likely to give it a heart attack!\n",
    "\n",
    "Fortunately, ü§ó Datasets has been designed to overcome these limitations. It frees you from memory management problems by treating datasets as memory-mapped files, and from hard drive limits by streaming the entries in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzGVURt7t1Wt"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCbTZ5tCt1Wv"
   },
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Pile?\n",
    "\n",
    "The Pile is an English text corpus that was created by EleutherAI for training large-scale language models. It includes a diverse range of datasets, spanning scientific articles, GitHub code repositories, and filtered web text. The training corpus is available in 14 GB chunks, and you can also download several of the individual components. Let‚Äôs start by taking a look at the PubMed Abstracts dataset, which is a corpus of abstracts from 15 million biomedical publications on PubMed. The dataset is in JSON Lines format and is compressed using the zstandard library, so first we need to install that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzLPEmZSt1Ww"
   },
   "outputs": [],
   "source": [
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the dataset using the method for remote files that we learned in section 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5Cjo1lBt1Wx",
    "outputId": "4c4d9053-59d4-41da-f2a2-19cf1004472c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['meta', 'text'],\n",
       "    num_rows: 15518009\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 15,518,009 rows and 2 columns in our dataset ‚Äî that‚Äôs a lot!\n",
    "\n",
    "Let‚Äôs inspect the contents of the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adQ_f_M1t1Wy",
    "outputId": "b2b19faf-bf06-4831-a2f2-dd6ba9f5680f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       " 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this looks like the abstract from a medical article. Now let‚Äôs see how much RAM we‚Äôve used to load the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The magic of memory mapping\n",
    "\n",
    "A simple way to measure memory usage in Python is with the psutil library, which can be installed with pip as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH2yW3jCt1Wz"
   },
   "outputs": [],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides a Process class that allows us to check the memory usage of the current process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1HceC9Zt1W0",
    "outputId": "ed27eebf-1191-4351-e18f-bf22eb348ce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAM used: 5678.33 MB"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the rss attribute refers to the resident set size, which is the fraction of memory that a process occupies in RAM. This measurement also includes the memory used by the Python interpreter and the libraries we‚Äôve loaded, so the actual amount of memory used to load the dataset is a bit smaller. For comparison, let‚Äôs see how large the dataset is on disk, using the dataset_size attribute. Since the result is expressed in bytes like before, we need to manually convert it to gigabytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3L_dnI4-t1W1",
    "outputId": "7e9075f9-f96a-4eee-da9f-77889ed35f2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of files in dataset : 20979437051\n",
       "Dataset size (cache file) : 19.54 GB"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n",
    "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice ‚Äî despite it being almost 20 GB large, we‚Äôre able to load and access the dataset with much less RAM!\n",
    "\n",
    "‚úèÔ∏è Try it out! Pick one of the subsets from the Pile that is larger than your laptop or desktop‚Äôs RAM, load it with ü§ó Datasets, and measure the amount of RAM used. Note that to get an accurate measurement, you‚Äôll want to do this in a new process. You can find the decompressed sizes of each subset in Table 1 of the Pile paper.\n",
    "If you‚Äôre familiar with Pandas, this result might come as a surprise because of Wes Kinney‚Äôs famous rule of thumb that you typically need 5 to 10 times as much RAM as the size of your dataset. So how does ü§ó Datasets solve this memory management problem? ü§ó Datasets treats each dataset as a memory-mapped file, which provides a mapping between RAM and filesystem storage that allows the library to access and operate on elements of the dataset without needing to fully load it into memory.\n",
    "\n",
    "Memory-mapped files can also be shared across multiple processes, which enables methods like Dataset.map() to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the Apache Arrow memory format and pyarrow library, which make the data loading and processing lightning fast. (For more details about Apache Arrow and comparisons to Pandas, check out Dejan Simic‚Äôs blog post.) To see this in action, let‚Äôs run a little speed test by iterating over all the elements in the PubMed Abstracts dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPx2csvJt1W2",
    "outputId": "2de09170-9aad-43b0-b383-958aef6318c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(pubmed_dataset), batch_size):\n",
    "    _ = pubmed_dataset[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we‚Äôve used Python‚Äôs timeit module to measure the execution time taken by code_snippet. You‚Äôll typically be able to iterate over a dataset at speed of a few tenths of a GB/s to several GB/s. This works great for the vast majority of applications, but sometimes you‚Äôll have to work with a dataset that is too large to even store on your laptop‚Äôs hard drive. For example, if we tried to download the Pile in its entirety, we‚Äôd need 825 GB of free disk space! To handle these cases, ü§ó Datasets provides a streaming feature that allows us to download and access elements on the fly, without needing to download the whole dataset. Let‚Äôs take a look at how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming datasets\n",
    "\n",
    "To enable dataset streaming you just need to pass the streaming=True argument to the load_dataset() function. For example, let‚Äôs load the PubMed Abstracts dataset again, but in streaming mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lcoqWEot1W3"
   },
   "outputs": [],
   "source": [
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the familiar Dataset that we‚Äôve encountered elsewhere in this chapter, the object returned with streaming=True is an IterableDataset. As the name suggests, to access the elements of an IterableDataset we need to iterate over it. We can access the first element of our streamed dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCrImefDt1W3",
    "outputId": "0fad9792-fe91-4c44-d84e-fb16c260b0d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       " 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(pubmed_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements from a streamed dataset can be processed on the fly using IterableDataset.map(), which is useful during training if you need to tokenize the inputs. The process is exactly the same as the one we used to tokenize our dataset in Chapter 3, with the only difference being that outputs are returned one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPvuOb4pt1W4",
    "outputId": "be6cdee6-2afe-4490-ab19-db6db45a69cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "next(iter(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also shuffle a streamed dataset using IterableDataset.shuffle(), but unlike Dataset.shuffle() this only shuffles the elements in a predefined buffer_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viW1uxjot1W5",
    "outputId": "9d0e89e9-8cae-4785-9e89-5de1c11ac150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 11410799, 'language': 'eng'},\n",
       " 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we selected a random example from the first 10,000 examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the IterableDataset.take() and IterableDataset.skip() functions, which act in a similar way to Dataset.select(). For example, to select the first 5 examples in the PubMed Abstracts dataset we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8O8T-SCt1W5",
    "outputId": "d7194dc6-7ab6-4e26-bb61-58feb6d972a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
       " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
       "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},\n",
       " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
       "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea ...\"},\n",
       " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
       "  'text': 'Oxygen concentrators and cylinders ...'},\n",
       " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
       "  'text': 'Oxygen supply in rural africa: a personal experience ...'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can use the IterableDataset.skip() function to create training and validation splits from a shuffled dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTlZdH2Ft1W6"
   },
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus. ü§ó Datasets provides an interleave_datasets() function that converts a list of IterableDataset objects into a single IterableDataset, where the elements of the new dataset are obtained by alternating among the source examples. This function is especially useful when you‚Äôre trying to combine large datasets, so as an example let‚Äôs stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miykjB7gt1W6",
    "outputId": "c6094367-c3e9-4405-873b-18ff9cc3473e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'case_ID': '110921.json',\n",
       "  'case_jurisdiction': 'scotus.tar.gz',\n",
       "  'date_created': '2010-04-28T17:12:49Z'},\n",
       " 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law_dataset_streamed = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "next(iter(law_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is large enough to stress the RAM of most laptops, yet we‚Äôve been able to load and access it without breaking a sweat! Let‚Äôs now combine the examples from the FreeLaw and PubMed Abstracts datasets with the interleave_datasets() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKBkpkHjt1W7",
    "outputId": "190e5fd2-cc46-4228-f653-f2add11cb0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
       " {'meta': {'case_ID': '110921.json',\n",
       "   'case_jurisdiction': 'scotus.tar.gz',\n",
       "   'date_created': '2010-04-28T17:12:49Z'},\n",
       "  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\n",
    "list(islice(combined_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we‚Äôve used the islice() function from Python‚Äôs itertools module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets.\n",
    "\n",
    "Finally, if you want to stream the Pile in its 825 GB entirety, you can grab all the prepared files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUT7dfv1t1W8",
    "outputId": "6e3ccfb2-0661-4b31-b082-b1ea57d1ea91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pile_set_name': 'Pile-CC'},\n",
       " 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "next(iter(pile_dataset[\"train\"]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Big data? ü§ó Datasets to the rescue!",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
