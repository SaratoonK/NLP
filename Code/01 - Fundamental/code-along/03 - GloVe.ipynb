{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.0', '1.12.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the sentences / corpus\n",
    "#corpus is defined as a set of documents\n",
    "#document is basically a bunch of sentence(s)\n",
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\", \n",
    "          \"dog cat animal\", \"cat dog animal\", \"cat animal dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#usually you use spaCy / NLTK to tokenize (but we gonna do this later on, we gonna have spaCy)\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized  #we called each of this as \"tokens\", NOT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numericalize\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten this (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruit': 0, 'animal': 1, 'cat': 2, 'apple': 3, 'banana': 4, 'dog': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exists in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6  #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fruit',\n",
       " 1: 'animal',\n",
       " 2: 'cat',\n",
       " 3: 'apple',\n",
       " 4: 'banana',\n",
       " 5: 'dog',\n",
       " 6: '<UNK>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index2word dictionary\n",
    "#2 min    \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'animal', 'cat', 'apple', 'banana', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence matrix\n",
    "\n",
    "Count the occurrences of pair of words using window size of 1 (you can use 2, 3, 4, up to you.)\n",
    "\n",
    "E.g., Dog loves to eat meat.     \n",
    "\n",
    "['dog', 'loves', 1], ['loves', 'to', 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Counter to first count stuffs\n",
    "from collections import Counter\n",
    "\n",
    "# print(corpus_tokenized)\n",
    "\n",
    "#count the frequency of each word....\n",
    "#we somehow need this to calculate the probability Pi\n",
    "X_i = Counter(flatten(corpus_tokenized)) #merge all list....(flatten is a function I define.....)\n",
    "\n",
    "# X_i['apple'] #get the probability of apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a skipgram of window size 1\n",
    "skip_grams = []\n",
    "\n",
    "#loop through each corpus\n",
    "for sent in corpus_tokenized:  #['apple', 'banana', 'fruit']\n",
    "    #loop through each word from 1 to n-1 (because 0 and n has no context window)\n",
    "    for i in range(1, len(sent)-1):\n",
    "        target  = sent[i]\n",
    "        context = [sent[i+1], sent[i-1]]\n",
    "        #append(i, i+1) and append(i, i-1)\n",
    "        for c in context:\n",
    "            skip_grams.append((target, c))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banana', 'fruit'),\n",
       " ('banana', 'apple'),\n",
       " ('apple', 'fruit'),\n",
       " ('apple', 'banana'),\n",
       " ('fruit', 'apple'),\n",
       " ('fruit', 'banana'),\n",
       " ('cat', 'animal'),\n",
       " ('cat', 'dog'),\n",
       " ('dog', 'animal'),\n",
       " ('dog', 'cat'),\n",
       " ('animal', 'dog'),\n",
       " ('animal', 'cat')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('banana', 'fruit'): 1,\n",
       "         ('banana', 'apple'): 1,\n",
       "         ('apple', 'fruit'): 1,\n",
       "         ('apple', 'banana'): 1,\n",
       "         ('fruit', 'apple'): 1,\n",
       "         ('fruit', 'banana'): 1,\n",
       "         ('cat', 'animal'): 1,\n",
       "         ('cat', 'dog'): 1,\n",
       "         ('dog', 'animal'): 1,\n",
       "         ('dog', 'cat'): 1,\n",
       "         ('animal', 'dog'): 1,\n",
       "         ('animal', 'cat'): 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since we have these occurrences, we can count, to make our co-occurrence matrix!!!\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_skipgram[('banana', 'animal')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighting function f\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words.\n",
    "\n",
    "<img src = \"../figures/glove_weighting_func.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):   #why we need w_i and w_j, because we can try its co-occurrences, if it's too big, we scale it down\n",
    "    \n",
    "    #check whether the co-occurrences between these two word exists???\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #why one, so that the probability thingy won't break...(label smoothing)\n",
    "        \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can have\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'banana'\n",
    "w_j  = 'fruit'\n",
    "w_j2 = 'chaky'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))   #scales from 1 to 0.0316\n",
    "print(weighting(w_i, w_j2, X_ik_skipgram))  #the paper says that f(0) = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'animal', 'cat', 'apple', 'banana', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing....(stability issues (especially when divide something))\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    \n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('fruit', 'apple'): 2,\n",
       " ('apple', 'fruit'): 2,\n",
       " ('fruit', 'banana'): 2,\n",
       " ('banana', 'fruit'): 2,\n",
       " ('animal', 'cat'): 2,\n",
       " ('cat', 'animal'): 2,\n",
       " ('animal', 'dog'): 2,\n",
       " ('dog', 'animal'): 2,\n",
       " ('cat', 'dog'): 2,\n",
       " ('dog', 'cat'): 2,\n",
       " ('apple', 'banana'): 2,\n",
       " ('banana', 'apple'): 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting_dic  #give small probability to never-occurred is called \"label smoothing\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'fruit']\n",
      "['banana', 'apple', 'fruit']\n",
      "['banana', 'fruit', 'apple']\n",
      "['dog', 'cat', 'animal']\n",
      "['cat', 'dog', 'animal']\n",
      "['cat', 'animal', 'dog']\n"
     ]
    }
   ],
   "source": [
    "for c in corpus_tokenized:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banana', 'fruit'),\n",
       " ('banana', 'apple'),\n",
       " ('apple', 'fruit'),\n",
       " ('apple', 'banana'),\n",
       " ('fruit', 'apple'),\n",
       " ('fruit', 'banana'),\n",
       " ('cat', 'animal'),\n",
       " ('cat', 'dog'),\n",
       " ('dog', 'animal'),\n",
       " ('dog', 'cat'),\n",
       " ('animal', 'dog'),\n",
       " ('animal', 'cat')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"../figures/glove.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #loop through this skipgram, and change it id  because when sending model, it must number\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indexes\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weighting = [] #f(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "        #return xi, xj\n",
    "        #return cooc Xij\n",
    "        #return weighting\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "random_batch(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized)\n",
    "input #center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label #context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50, 100, or 300\n",
    "model = Skipgram(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)  #LongTensor basically means integer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is different\n",
    "torch.LongTensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([2])  #put shape (, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should give one number\n",
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 2 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: ??\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is really the related stuff are close to each other, and vice versa?\n",
    "\n",
    "The most fun part:  Will \"banana\" closer to \"fruit\" than \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_center_embed = model.embedding_center_word(banana)\n",
    "banana_outisde_embed = model.embedding_outside_word(banana)\n",
    "\n",
    "banana_embed = (banana_center_embed + banana_outisde_embed) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  embed[0][0].item(), embed[0][1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find embedding of fruit, cat\n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help me plot fruit cat banana on matplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity\n",
    "\n",
    "How do (from scratch) calculate cosine similarity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
