{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## AllenNLP\n",
    "\n",
    "AllenNLP is an open source library for building deep learning models for natural language processing, developed by the Allen Institute for Artificial Intelligence. It is built on top of PyTorch and is designed to support researchers, engineers, students, etc., who wish to build high quality deep NLP models with ease. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments.\n",
    "\n",
    "In a nutshell, AllenNLP is\n",
    "\n",
    "- a library with well-thought-out abstractions encapsulating the common data and model operations that are done in NLP research\n",
    "- a commandline tool for training PyTorch models\n",
    "- a collection of pre-trained models that you can use to make predictions\n",
    "- a collection of readable reference implementations of common / recent NLP models\n",
    "- an experiment framework for doing replicable science\n",
    "- a way to demo your research\n",
    "- open source and community driven\n",
    "\n",
    "In part 1, geared towards someone who is brand new to the library, we give you a quick walk-through of main AllenNLP concepts and features. We'll build a complete, working NLP model (a text classifier) along the way.\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "\n",
    "### Fields\n",
    "\n",
    "The first step for building an NLP model is to define its input and output. In AllenNLP, each training example is represented by an `Instance` object. An `Instance` consists of one or more `Fields`, where each `Field` represents one piece of data used by your model, either as an input or an output. `Fields` will get converted to tensors and fed to your model.\n",
    "\n",
    "For text classification, the input and the output are very simple. The model takes a `TextField` that represents the input text and predicts its label, which is represented by a `LabelField`.\n",
    "\n",
    "Note that AllenNLP use the **type hint** features in Python 3, by specifying a colon.  This internally helps AllenNLP do many magic, such as automatically construct the embedder and encoder from a configuration file using these type annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#a bit about type hint\n",
    "\n",
    "def some_func(text: str):\n",
    "    print(text)\n",
    "    \n",
    "some_func(\"hello world\")\n",
    "some_func(3)  #won't error, because this is type hinting.  Mostly used by editors to check errors before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import LabelField,  TextField\n",
    "\n",
    "# Inputs\n",
    "text: TextField\n",
    "\n",
    "# Outputs\n",
    "label: LabelField"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data\n",
    "\n",
    "The first step for building an NLP application is to read the dataset and represent it with some internal data structure.\n",
    "\n",
    "AllenNLP uses `DatasetReaders` to read the data, whose job it is to transform raw data files into `Instances` that match the input / output spec. \n",
    "\n",
    "AllenNLP assume the dataset has a simple data file format: `[text] [TAB] [label]`, for example:\n",
    "\n",
    "- I like this movie a lot! [TAB] positive\n",
    "\n",
    "- This was a monstrous waste of time [TAB] negative\n",
    "\n",
    "- AllenNLP is amazing [TAB] positive\n",
    "\n",
    "- Why does this have to be so complicated? [TAB] negative\n",
    "\n",
    "- This sentence expresses no sentiment [TAB] neutral\n",
    "\n",
    "You can implement your own `DatasetReader` by inheriting from the `DatasetReader` class. At minimum, you need to override the `_read()` method, which reads the input dataset and yields `Instances`.\n",
    "\n",
    "Note that we are making the label parameter of `text_to_instance()` optional. During training and evaluation, all the instances were labeled, i.e., they included the `LabelFields` that contain gold labels. However, when you are making predictions for unseen inputs, the instances are unlabeled. By making the label parameter optional the dataset reader can support both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"classification-tsv\")\n",
    "class ClassificationTsvReader(DatasetReader):\n",
    "    def __init__(self, max_tokens: int = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = SpacyTokenizer()\n",
    "        self.token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if self.max_tokens:\n",
    "            tokens = tokens[: self.max_tokens]\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": text_field}\n",
    "        if label:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(file_path, \"r\") as lines:\n",
    "            for line in lines:\n",
    "                text, sentiment = line.strip().split(\"\\t\")\n",
    "                yield self.text_to_instance(text, sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimal DatasetReader that will return a list of classification Instances when you call `reader.read(file)`. This reader will take each line in the input file, split the text into words using a tokenizer (the SpacyTokenizer shown here relies on spaCy), and represent those words as tensors using a word id in a vocabulary we construct for you.\n",
    "\n",
    "Pay special attention to the text and label keys that are used in the fields dictionary passed to the `Instance` - these keys will be used as parameter names when passing tensors into your `Model` later.\n",
    "\n",
    "Ideally, the output label would be **optional** when we create the`Instances`, so that we can use the same code to make **predictions on unlabeled data (say, in a demo)**.\n",
    "\n",
    "There are lots of places where this could be made better for a more flexible and fully-featured reader but let's keep it simple for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Designing the model\n",
    "\n",
    "The next thing we need is a `Model` that will take a batch of `Instances`, predict the outputs from the inputs, and compute a loss.\n",
    "\n",
    "Also, remember that we used these names (`text` and `label`) for the fields in the `DatasetReader.` AllenNLP passes those fields by name to the model code, so we need to use the same names in our model.\n",
    "\n",
    "Conceptually, a generic model for classifying text does the following:\n",
    "\n",
    "- Get some features corresponding to each word in your input\n",
    "- Combine those word-level features into a document-level feature vector\n",
    "- Classify that document-level feature vector into one of your labels.\n",
    "\n",
    "In AllenNLP, we make each of these conceptual steps into a generic abstraction that you can use in your code, so that you can have a very flexible model that can use different concrete components for each step.\n",
    "\n",
    "#### First step: Token IDs\n",
    "\n",
    "<img src = \"../../../figures/allentokenid.svg\">\n",
    "\n",
    "The first step is changing the strings in the input text into token ids. This is handled by the `SingleIdTokenIndexer` that we used previously, during part of our data processing pipeline that you don't have to write code for.\n",
    "\n",
    "#### Second step: Embedding\n",
    "\n",
    "<img src = \"../../../figures/allenembedding.svg\">\n",
    "\n",
    "Apply an Embedding function that converts each token ID that we got as input into a vector.\n",
    "\n",
    "#### Third step: Seq2Vec encoder\n",
    "\n",
    "<img src = \"../../figures/allenseq2vec.svg\">\n",
    "\n",
    "Next we apply some function that takes the sequence of vectors for each input token and squashes it into a single vector. Before the days of pretrained language models like BERT, this was typically an LSTM or convolutional encoder. With BERT we might just take the embedding of the [CLS] token.\n",
    "\n",
    "#### Fourth step: Predict\n",
    "\n",
    "<img src = \"../../../figures/allendist.svg\">\n",
    "\n",
    "Finally, we take that single feature vector (for each `Instance` in the batch), and classify it as a label, which will give us a categorical probability distribution over our label space.\n",
    "\n",
    "### 3. Implementing the model\n",
    "\n",
    "Now that we know what our model is going to do, we need to implement it. First, we'll say a few words about how `Models` work in AllenNLP:\n",
    "\n",
    "- An AllenNLP Model is just a **PyTorch Module**\n",
    "- It implements a `forward()` method, and requires the output to be a **dictionary**\n",
    "- Its output contains a loss key during training, which is used to optimize the model\n",
    "\n",
    "Our training loop takes a batch of `Instances`, passes it through `Model.forward()`, grabs the `loss` key from the resulting dictionary, and uses backprop to compute gradients and update the model's parameters. You don't have to implement the training loop—all this will be taken care of by AllenNLP (though you can if you want to).\n",
    "\n",
    "#### 3.1 Constructor\n",
    "\n",
    "In the `Model` constructor, we need to instantiate all of the parameters that we will want to train. In AllenNLP, we recommend taking most of these parameters as constructor arguments, so that we can configure the behavior of our model without changing the model code itself, and so that we can think at a higher level about what our model is doing. Let's look at different components:\n",
    "\n",
    "`Vocabulary` manages mappings between vocabulary items (such as words and labels) and their integer IDs. In our prebuilt training loop, the vocabulary gets created by AllenNLP after reading your training data, then passed to the `Model` when it gets constructed. We'll find all tokens and labels that you use and assign them all integer IDs in separate namespaces.\n",
    "\n",
    "To get an initial word embedding, we'll use AllenNLP's `TextFieldEmbedder`. This abstraction takes the tensors created by a `TextField` and embeds each one. This is our most complex abstraction, because there are a lot of ways to do this particular operation in NLP, and we want to be able to switch between these without changing our code. We won't go into the details here; All you need to know for now is that you apply this to the text parameter you get in `forward()`, and you get out a tensor that has a single embedding vector for each input token, with shape `(batch_size, num_tokens, embedding_dim)`.\n",
    "\n",
    "To squash our sequence of token vectors into a single vector, we use AllenNLP's `Seq2VecEncoder` abstraction. As the name implies, this encapsulates an operation that takes a sequence of vectors and returns a single vector. Because all of our modules operate on batched input, this will take a tensor shaped like `(batch_size, num_tokens, embedding_dim)` and return a tensor shaped like `(batch_size, encoding_dim)`.\n",
    "\n",
    "In AllenNLP, you implement the logic to compute the metrics in your `Model` class. AllenNLP includes an abstraction called `Metric` that gives some useful functionality for tracking metrics during training. Here, we'll be using an accuracy `Metric`, `CategoricalAccuracy`, which computes the fraction of instances for which our model predicted the label correctly.\n",
    "\n",
    "#### 3.2 forward\n",
    "\n",
    "In `forward`, we use the parameters that we created in our constructor to transform the inputs into outputs. After we've predicted the outputs, we compute some loss function based on how close we got to the true outputs, and then return that loss (along with whatever else we want) so that we can use it to train the parameters.\n",
    "\n",
    "The first thing to notice is the inputs to this function. The way the AllenNLP training loop works is that we will take the field names that you used in your `DatasetReader` and give you a batch of instances with those same field names in `forward`. So, because we used `text` and `label` as our field names, we need to name our arguments to `forward` the same way.\n",
    "\n",
    "Second, notice the types of these arguments. Each type of `Field` knows how to convert itself into a `torch.Tensor`, then create a batched torch.Tensor from all of the `Fields` with the same name from a batch of `Instances`. The types you see for text and label are the tensors produced by `TextField` and `LabelField`. The important part to know is that our `TextFieldEmbedder`, which we created in the constructor, expects this type of object as input and will return an embedded tensor as output.\n",
    "\n",
    "The first actual modeling operation that we do is embed the text, getting a vector for each input token. Notice here that we're not specifying anything about how that operation is done, just that a `TextFieldEmbedder` that we got in our constructor is going to do it. This lets us be very flexible later, changing between various kinds of embedding methods or pretrained representations (including ELMo and BERT) without changing our model code.\n",
    "\n",
    "After we have embedded our text, we next have to squash the sequence of vectors (one per token) into a single vector for the whole text. We do that using the `Seq2VecEncoder` that we got as a constructor argument. In order to behave properly when we're batching pieces of text together that could have different lengths, we need to mask elements in the `embedded_text` tensor that are only there due to padding. We use a utility function to get a mask from the `TextField` output, then pass that mask into the encoder.\n",
    "\n",
    "At the end of these lines, we have a single vector for each instance in the batch.\n",
    "\n",
    "The last step of our model is to take the vector for each instance in the batch and predict a label for it. Our classifier is a `torch.nn.Linear` layer that gives a score (commonly called a logit) for each possible label. We normalize those scores using a `softmax` operation to get a probability distribution over labels that we can return to a consumer of this model. For computing the loss, PyTorch has a built in function that computes the cross entropy between the logits that we predict and the true label distribution, and we use that as our loss function.\n",
    "\n",
    "Then, for each forward pass, you need to update the metric by feeding the prediction and the gold labels in `self.accuracy`.  The way metrics work in AllenNLP is that, behind the scenes, each `Metric` instance holds \"counts\" that are necessary and sufficient to compute the metric. For accuracy, these counts are the number of total predictions as well as the number of correct predictions. These counts get updated after every call to the instance itself, i.e., the `self.accuracy(logits, label)` line. You can pull out the computed metric by calling `get_metrics()` with a flag specifying whether to reset the counts. This allows you to compute the metric over the entire training or validation dataset.  AllenNLP's default training loop will call this method at the appropriate times and provide logging information with current metric values.  Thus you don't have to explicitly called it.\n",
    "\n",
    "And that's it! This is all you need for a simple classifier. After you've written a `DatasetReader` and `Model`, AllenNLP takes care of the rest: connecting your input files to the dataset reader, intelligently batching together your instances and feeding them to the model, and optimizing the model's parameters by using backprop on the loss.\n",
    "\n",
    "Note that kn order to support prediction, first you need to make the label parameter optional by specifying a default value of `None`. This will let you feed unlabeled instances to the model. Second, you need to compute the loss and accuracy only when the label is supplied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
    "from allennlp.data import TextFieldTensors\n",
    "from allennlp.nn import util\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "@Model.register('simple_classifier')\n",
    "class SimpleClassifier(Model):\n",
    "    \n",
    "    ##constructor\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        num_labels = vocab.get_vocab_size(\"labels\") \n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    ##forward    \n",
    "    def forward(self,\n",
    "                text: TextFieldTensors,\n",
    "                label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        # need to set label = None, in case we are predicting\n",
    "        \n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        output = {\"probs\": probs}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            # Shape: (1,)\n",
    "            output[\"loss\"] = torch.nn.functional.cross_entropy(logits, label)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Putting everything together\n",
    "\n",
    "In this section we'll put together a simple example of reading in data, feeding it to the model, and training the model, using your own python script instead of allennlp train. While we recommend using allennlp train for most use cases, it's easier to understand the introduction to the training loop. Once you get a handle on this, switching to using allennlp built in command should be easy, if you want to.\n",
    "\n",
    "\n",
    "#### Testing your dataset reader\n",
    "\n",
    "In the first example, we'll simply instantiate the dataset reader, read the movie review dataset using it, and inspect the AllenNLP Instances produced by the dataset reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t text: TextField of length 64 with text: \n",
      " \t\t[it, is, movies, like, these, that, make, a, jaded, movie, viewer, thankful, for, the, invention,\n",
      "\t\tof, the, timex, indiglo, watch, ., based, on, the, late, 1960, 's, television, show, by, the, same,\n",
      "\t\tname, ,, the, mod, squad, tells, the, tale, of, three, reformed, criminals, under, the, employ, of,\n",
      "\t\tthe, police, to, go, undercover, ., however, ,, things, go, wrong, as, evidence, gets, stolen, and]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: neg in namespace: 'labels'. \n",
      "\n",
      "Instance with fields:\n",
      " \t text: TextField of length 64 with text: \n",
      " \t\t[\", quest, for, camelot, \", is, warner, bros, ., ', first, feature, -, length, ,, fully, -,\n",
      "\t\tanimated, attempt, to, steal, clout, from, disney, 's, cartoon, empire, ,, but, the, mouse, has, no,\n",
      "\t\treason, to, be, worried, ., the, only, other, recent, challenger, to, their, throne, was, last,\n",
      "\t\tfall, 's, promising, ,, if, flawed, ,, 20th, century, fox, production, \", anastasia, ,, \", but]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: neg in namespace: 'labels'. \n",
      "\n",
      "Instance with fields:\n",
      " \t text: TextField of length 64 with text: \n",
      " \t\t[synopsis, :, a, mentally, unstable, man, undergoing, psychotherapy, saves, a, boy, from, a,\n",
      "\t\tpotentially, fatal, accident, and, then, falls, in, love, with, the, boy, 's, mother, ,, a,\n",
      "\t\tfledgling, restauranteur, ., unsuccessfully, attempting, to, gain, the, woman, 's, favor, ,, he,\n",
      "\t\ttakes, pictures, of, her, and, kills, a, number, of, people, in, his, way, ., comments, :, stalked,\n",
      "\t\tis, yet, another, in, a, seemingly]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: neg in namespace: 'labels'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_reader = ClassificationTsvReader(max_tokens=64)\n",
    "instances = list(dataset_reader.read(\"../data/imdb/train.tsv\"))\n",
    "\n",
    "for instance in instances[:3]:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two tokens:  [synopsis, :]\n",
      "Label:  neg\n"
     ]
    }
   ],
   "source": [
    "#you can access the instance information like this\n",
    "print(\"First two tokens: \", instance['text'][:2])\n",
    "print(\"Label: \", instance['label'].label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feeding instances to the model\n",
    "\n",
    "The `Model` needs to have a `Vocabulary` computed from data before we can build it, but we don't really want to put the details of our model construction inside our training loop function. So to keep things sane, we'll pull out the model building into a separate function that we call inside the main training function.\n",
    "\n",
    "When you run this, you should see the outputs returned from the model. Each returned dict includes the `loss` key as well as the `probs` key, which contains probabilities for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = ClassificationTsvReader(max_tokens=64)\n",
    "    print(\"Reading data\")\n",
    "    instances = list(dataset_reader.read(\"../data/imdb/train.tsv\"))\n",
    "\n",
    "    vocab = build_vocab(instances)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    outputs = model.forward_on_instances(instances[:4])\n",
    "    print(outputs)\n",
    "\n",
    "\n",
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)\n",
    "\n",
    "\n",
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    embedder = BasicTextFieldEmbedder(\n",
    "        {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)}\n",
    "    )\n",
    "    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
    "    return SimpleClassifier(vocab, embedder, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Building the vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb9b335a4be4062960fbe9e4f537ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'probs': array([0.52598524, 0.47401473], dtype=float32)}, {'probs': array([0.5340886 , 0.46591136], dtype=float32)}, {'probs': array([0.5437718 , 0.45622817], dtype=float32)}, {'probs': array([0.5525191, 0.4474809], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "run_training_loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model\n",
    "\n",
    "Finally, we'll run backpropagation and train the model. AllenNLP uses a `Trainer` for this, which is responsible for connecting necessary components (including your model, optimizer, instances, data loader, etc.) and executing the training loop.\n",
    "\n",
    "When you run this, the `Trainer` goes over the training data five times (`num_epochs=5`). After each epoch, AllenNLP runs your model against the validation set to monitor how well (or badly) it's doing. This is useful if you want to do, e.g., early stopping, and for monitoring in general. Observe that the training loss decreases gradually—this is a sign that your model and the training pipeline are doing what they are supposed to do (that is, to minimize the loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Building the vocabulary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a2710effcd434883146dddfb0119f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5069637c8b140f1af9dd8e0a741e2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/dsai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747bea2fddd9491981331a67d5157080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc54505a7774ebf875e27d2c072aeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe219e919d2442b59611fcf0db8b051f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe17ced1fc7419790641c694b66d9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4056c44b28894c11b51860e9ec0a39d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b24d3b326469c9db5cee8eecd2798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9185443af6845a598b6914674500dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Iterable, List, Tuple\n",
    "import tempfile\n",
    "\n",
    "from allennlp.data import DataLoader\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.training.gradient_descent_trainer import GradientDescentTrainer\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "\n",
    "from allennlp.training.optimizers import AdamOptimizer\n",
    "\n",
    "def build_dataset_reader() -> DatasetReader:\n",
    "    return ClassificationTsvReader()\n",
    "\n",
    "def read_data(reader: DatasetReader) -> Tuple[List[Instance], List[Instance]]:\n",
    "    print(\"Reading data\")\n",
    "    training_data   = list(reader.read(\"../data/imdb/train.tsv\"))\n",
    "    validation_data = list(reader.read(\"../data/imdb/dev.tsv\"))\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def run_training_loop():\n",
    "    dataset_reader = build_dataset_reader()\n",
    "\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "    train_loader.index_with(vocab)\n",
    "    dev_loader.index_with(vocab)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this guide, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_trainer(model, serialization_dir, train_loader, dev_loader)\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "\n",
    "    return model, dataset_reader\n",
    "\n",
    "\n",
    "def build_data_loaders(\n",
    "    train_data: List[Instance],\n",
    "    dev_data: List[Instance],\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_loader = SimpleDataLoader(train_data, 8, shuffle=True)\n",
    "    dev_loader   = SimpleDataLoader(dev_data, 8, shuffle=False)\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "\n",
    "def build_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    ") -> Trainer:\n",
    "    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer = AdamOptimizer(parameters)  # type: ignore\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=5,\n",
    "        optimizer=optimizer,\n",
    "        patience=2 #after 2 epochs with no improvement, stop (basically early stopping)\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "model, dataset_reader = run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d072e20d823c465bac40c03bb0e5711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.875, 'loss': 0.32959430426359176}\n"
     ]
    }
   ],
   "source": [
    "from allennlp.training.util import evaluate\n",
    "\n",
    "# Now we can evaluate the model on a new dataset.\n",
    "test_data = list(dataset_reader.read(\"../data/imdb/test.tsv\"))\n",
    "data_loader = SimpleDataLoader(test_data, 8)\n",
    "data_loader.index_with(model.vocab)\n",
    "\n",
    "results = evaluate(model, data_loader)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For making predictions, AllenNLP uses `Predictors`, which are a thin wrapper around your trained model. A `Predictor`'s main job is to take a JSON representation of an instance, convert it to an `Instance` using the dataset reader (the `text_to_instance`), pass it through the model, and return the prediction in a JSON serializable format.\n",
    "\n",
    "In order to build a `Predictor` for your task, you only need to inherit from `Predictor` and implement a few methods (see predict() and _json_to_instances() below)—the rest will be taken care of by the base class.\n",
    "\n",
    "AllenNLP provides implementations of `Predictors` for common tasks. In fact, it includes `TextClassifierPredictor`, a generic `Predictor` for text classification tasks, so you don't even need to write your own! Here, we are writing one from scratch solely for demonstration, but you should always check whether the predictor for your task is already there.\n",
    "\n",
    "To implement, wrap the model with a `SentenceClassifierPredictor` to make predictions for new instances. Because the returned result (`output['probs']`) is just an array of probabilities for class labels, we use `vocab.get_token_from_index()` to convert a label ID back to its label string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 0.497977614402771), ('pos', 0.502022385597229)]\n",
      "[('neg', 0.5784565806388855), ('pos', 0.4215433895587921)]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "from allennlp.common.util import JsonDict\n",
    "\n",
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)\n",
    "\n",
    "vocab = model.vocab\n",
    "predictor = SentenceClassifierPredictor(model=model, dataset_reader=dataset_reader)\n",
    "\n",
    "output = predictor.predict(\"A good movie!\")\n",
    "print(\n",
    "    [\n",
    "        (vocab.get_token_from_index(label_id, \"labels\"), prob)\n",
    "        for label_id, prob in enumerate(output[\"probs\"])\n",
    "    ]\n",
    ")\n",
    "output = predictor.predict(\"This was a monstrous waste of time.\")\n",
    "print(\n",
    "    [\n",
    "        (vocab.get_token_from_index(label_id, \"labels\"), prob)\n",
    "        for label_id, prob in enumerate(output[\"probs\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the model with command line\n",
    "\n",
    "Ok, we've seen how to set up a simple training loop.As you can see, there are many boilerplate code.\n",
    "\n",
    "AllenNLP have a built-in training script that handles all of these things for you and makes it so the only code that you have to write are your `DatasetReader` and `Model` classes. Instead of writing all of the `build_*` methods that we had above, we write a `JSON` configuration file specifying all necessary parameters. Our training script takes those parameters, creates all of the objects in the right order, and runs the training loop.\n",
    "\n",
    "#### Configuration files\n",
    "\n",
    "In a nutshell, configuration files in allennlp just take constructor parameters for various objects and put them into a JSON dictionary. Recall that we had a `build_model` method that looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(vocab: Vocabulary) -> Model:\n",
    "#     print(\"Building the model\")\n",
    "#     vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "#     embedder = BasicTextFieldEmbedder(\n",
    "#         {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)})\n",
    "#     encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
    "#     return SimpleClassifier(vocab, embedder, encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets converted into a JSON dictionary that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model\": {\n",
    "#     \"type\": \"simple_classifier\",\n",
    "#     \"embedder\": {\n",
    "#         \"token_embedders\": {\n",
    "#             \"tokens\": {\n",
    "#                 \"type\": \"embedding\",\n",
    "#                 \"embedding_dim\": 10\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"encoder\": {\n",
    "#         \"type\": \"bag_of_embeddings\",\n",
    "#         \"embedding_dim\": 10\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor parameters to all of the objects that were created in `build_model` are translated directly to keys in this dictionary. AllenNLP relies on the type annotations in the model's constructor code in order to construct these objects correctly.\n",
    "\n",
    "There are two special things to note: first, to select a particular subclass of a base type (e.g., `SimpleClassifier` as a subclass of Model, or `BagOfEmbeddingsEncoder` as a subclass of `Seq2VecEncoder`) we need an additional \"type\": \"`simple_classifier`\" key. The string \"`simple_classifier`\" comes from the call to `Model.register`\n",
    "\n",
    "Second, the vocab argument is missing here. That's for the same reason that vocab was an argument to the `build_model` method, not constructed inside it—the vocabulary gets constructed separately, based on data, then passed in to the model. Generally, the sequential dependencies between objects that show up as arguments to your `build_*` methods are left out of the configuration file, as they are handled in a different way. Again, there's a lot more detail which we will cover later.\n",
    "\n",
    "We do this not just for the model, but for the `dataset reader`, the `data loaders`, the `trainer`, and everything else that goes into a training loop. This gives us a single JSON file that holds all of the configuration for an experiment that was run (we actually use a superset of JSON called `Jsonnet`, which supports fancier features like variables and imports, but a plain JSON file works too).\n",
    "\n",
    "For our simple classifier, that configuration file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"dataset_reader\" : {\n",
    "#         \"type\": \"classification-tsv\",\n",
    "#         \"token_indexers\": {\n",
    "#             \"tokens\": {\n",
    "#                 \"type\": \"single_id\"\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"train_data_path\": \"../data/imdb/train.tsv\",\n",
    "#     \"validation_data_path\": \"../data/imdb/dev.tsv\",\n",
    "#     \"model\": {\n",
    "#         \"type\": \"simple_classifier\",\n",
    "#         \"embedder\": {\n",
    "#             \"token_embedders\": {\n",
    "#                 \"tokens\": {\n",
    "#                     \"type\": \"embedding\",\n",
    "#                     \"embedding_dim\": 10\n",
    "#                 }\n",
    "#             }\n",
    "#         },\n",
    "#         \"encoder\": {\n",
    "#             \"type\": \"bag_of_embeddings\",\n",
    "#             \"embedding_dim\": 10\n",
    "#         }\n",
    "#     },\n",
    "#     \"data_loader\": {\n",
    "#         \"batch_size\": 8,\n",
    "#         \"shuffle\": true\n",
    "#     },\n",
    "#     \"trainer\": {\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"num_epochs\": 5\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this configuration file, we can train the model by running \n",
    "\n",
    "    allennlp train [config.json] -s [serialization_directory] \n",
    "\n",
    "from a command line. In order for your dataset reader, model, and other custom components to be recognized by the allennlp command, the calls to `.register()` have to be run, which happens when the classes are imported. So you typically have to also add the flag `--include-package [my_python_module]`, or use allennlp's plugin functionality, when you run this command. There is more detail on how this works in the chapter on configuration files.\n",
    "\n",
    "Note:  add `\"cuda_device\": 0` inside the trainer config, if you want to use cuda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"trainer\": {\n",
    "#         ...\n",
    "#         \"cuda_device\": 0\n",
    "#         ...\n",
    "#     }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple GPUs, create a new `\"distributed\":` and add the list of GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"trainer\": {\n",
    "#         ...\n",
    "#     },\n",
    "# \"distributed\": {\n",
    "#        \"cuda_devices\": [0, 1, 2, 3]\n",
    "#     }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will use PyTorch's DistributedDataParallel to aggregate losses and synchronize parameter updates across multiple GPUs. The speedup you get, however, might not be exactly proportional to the number of GPUs due to due to synchronization and overhead.\n",
    "\n",
    "Last note:  When you are evaluating and making predictions with your model, you can specify the `--cuda-device` option from the command line to make your model run on GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do it!\n",
    "\n",
    "To train:\n",
    "\n",
    "    allennlp train classification.jsonnet -s model --include-package my_text_classifier\n",
    "\n",
    "We can also evaluate.  Note that the evaluate command takes the model saved in the previous serialization directory, which is `model` specified in `-s`.\n",
    "\n",
    "    allennlp evaluate model/model.tar.gz ../data/imdb/test.tsv --include-package my_text_classifier\n",
    "\n",
    "Last, we can try to predict\n",
    "\n",
    "    allennlp predict model/model.tar.gz ../data/imdb/test.jsonl --include-package my_text_classifier --predictor sentence_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Running a demo\n",
    "\n",
    "First, install the allennlp-server\n",
    "\n",
    "    pip install allennlp-server\n",
    "\n",
    "Last you can spin up the server by running:\n",
    "\n",
    "    allen serve \\\n",
    "    --archive-path model/model.tar.gz \\\n",
    "    --predictor sentence_classifier \\\n",
    "    --field-name sentence \\\n",
    "    --include-package my_text_classifier\n",
    "\n",
    "Note that you need to specify the name of the field(s) to accept input. You can access `localhost:8000` in your browser to see the simple demo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Try BERT\n",
    "\n",
    "AllenNLP supports many pre-trained models out of the box.  You can try change the `classification.jsonnet` like this:\n",
    "\n",
    "First, instead of using normal tokenizer, we gonna use BERT tokenzier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"dataset_reader\" : {\n",
    "#         \"type\": \"classification-tsv\",\n",
    "#         \"tokenizer\": {\n",
    "#             \"type\": \"pretrained_transformer\",\n",
    "#             \"model_name\": bert_model,\n",
    "#         },\n",
    "#         \"token_indexers\": {\n",
    "#             \"bert\": {\n",
    "#                 \"type\": \"pretrained_transformer\",\n",
    "#                 \"model_name\": bert_model,\n",
    "#             }\n",
    "#         },\n",
    "#         \"max_tokens\": 512\n",
    "#     }, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use BERT embedding as well as BERT encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model\": {\n",
    "#         \"type\": \"simple_classifier\",\n",
    "#         \"embedder\": {\n",
    "#             \"token_embedders\": {\n",
    "#                 \"bert\": {\n",
    "#                     \"type\": \"pretrained_transformer\",\n",
    "#                     \"model_name\": bert_model\n",
    "#                 }\n",
    "#             }\n",
    "#         },\n",
    "#         \"encoder\": {\n",
    "#             \"type\": \"bert_pooler\",\n",
    "#             \"pretrained_model\": bert_model\n",
    "#         }\n",
    "#     },"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, you need to:\n",
    "\n",
    "- Use a `PretrainedTransformerTokenizer` (\"`pretrained_transformer`\"), which tokenizes the string into wordpieces and adds special tokens like [CLS] and [SEP]\n",
    "- Use a `PretrainedTransformerIndexer` (\"`pretrained_transformer`\"), which converts those wordpieces into ids using BERT's vocabulary\n",
    "- Replace the embedder layer with a `PretrainedTransformerEmbedder` (\"`pretrained_transformer`\"), which uses a pretrained BERT model to embed the tokens, returning the top layer from BERT\n",
    "- Replace the encoder with a `BertPooler` (\"`bert_pooler`\"), which adds another (pretrained) linear layer on top of the [CLS] token and returns the result\n",
    "\n",
    "Also note that we switched the optimizer to use `AdamW` from HuggingFace's Transformers library.\n",
    "\n",
    "The tokenizer and the embedder are thin wrappers around **HuggingFace's Transformers** library, so switching between different transformer architectures (BERT, RoBERTa, XLNet, etc.) is as simple as changing the `model_name` parameter in the config file.\n",
    "\n",
    "I have already created this config file called `classification_bert.jsonnet`.  Try it.  You should see increased accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
