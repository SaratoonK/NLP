{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## NER using AllenNLP\n",
    "\n",
    "The problem statement of NER is as follows:\n",
    "\n",
    "Given \n",
    "\n",
    "    Bill Gates and Paul Allen, founders of Microsoft, started selling software in 1975 in New Mexico.\n",
    "\n",
    "We want to get\n",
    "\n",
    "    [Bill Gates PER] and [Paul Allen PER], founders of [Microsoft ORG], started selling software in [1975 DATE] in [New Mexico LOC].\n",
    "\n",
    "To do this, there are two steps:\n",
    "\n",
    "- First, as a segmentation task, where we attempt to find and classify segments that match entities, and assign some NULL or O label to the in-between stuff. Thus, our label space would be {PER, ORG, DATE, LOC, O}.\n",
    "- Second, as a token-level tagging task. This one requires a bit more thought — it’s not clear from the start how we associate entities with each other. But if you introduce a slightly modified label space, you can reconstruct the entities.\n",
    "\n",
    "To do this, each entity type (e.g. PER, LOC) gets split into two labels: B-PER, denoting “this is a new person entity” and I-PER, denoting, “I’m continuing the previous person entity”. On the above sentence, every token would be tagged like so:\n",
    "\n",
    "    [Bill B-PER] [Gates I-PER] and [Paul B-PER] [Allen I-PER], founders of [Microsoft B-ORG], started selling software in [1975 B-DATE] in [New B-LOC] [Mexico I-LOC].\n",
    "\n",
    "For brevity’s sake, I left out all the [and O] tags, but you can imagine that all the rest of the words in the sentence are assigned that null tag.\n",
    "\n",
    "\n",
    "### 1. Loading the CoNLL 2003 dataset\n",
    "\n",
    "Let’s take a look at an example from the CoNLL’03 dataset and see if they conform to the specification we laid down above:\n",
    "\n",
    "    Essex NNP I-NP I-ORG\n",
    "    , , O O\n",
    "    however RB I-ADVP O\n",
    "    , , O O\n",
    "    look VB I-VP O\n",
    "    certain JJ I-ADJP O\n",
    "    to TO I-VP O\n",
    "    regain VB I-VP O\n",
    "    their PRP$ I-NP O\n",
    "    .\n",
    "    .\n",
    "\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. Thus we only care about the first and last item.\n",
    "\n",
    "The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase.\n",
    "\n",
    "To download the dataset, run in your terminal:\n",
    "\n",
    "    curl -o train.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n",
    "    curl -o validation.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa\n",
    "    curl -o test.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n",
    "\n",
    "### 2. Dataset Reader\n",
    "\n",
    "The first thing we’re going to do is build a dataset reader, which can consume the CoNLL’03 dataset.  To get started, let’s create the directory structure for this project. Currently, you should have directories that look something like this:\n",
    "\n",
    "    mkdir conn_ner/readers\n",
    "    touch conn_ner/__init__.py\n",
    "    touch conn_ner/readers/__init__.py\n",
    "    touch conn_ner/readers/conll_reader.py\n",
    "\n",
    "Put the following code in a file in the `conll_reader.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import DatasetReader\n",
    "\n",
    "@DatasetReader.register(\"conll_03_reader\")\n",
    "class CoNLL03DatasetReader(DatasetReader):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@DatasetReader.register(...)` is one core feature of AllenNLP, `Registrables`.  When you call `.register()`, it allows us to confgure our experiments with JSON even though we write all the code in Python.\n",
    "\n",
    "When you write a new `Model`, a new `DatasetReader`, a new `Metric`, or pretty much anything else, you’ll want to register it so it’s visible to your configuration file. \n",
    "\n",
    "Every class that inherits from DatasetReader **should override these 3 functions**:\n",
    "\n",
    "    __init__(self, ...) -> None\n",
    "    _read(self, file_path: str) -> List[Instance]\n",
    "    text_to_instance(self, ...) -> Instance\n",
    "\n",
    "Any argument in `__init__()` will be visible to the JSON configuration later on, so if you have parameters in the dataset reader you want to change in between experiments, you’ll put them there. For our CoNLL’03 reader, our `__init__()` function will take in 2 parameters: `token_indexers`, and `lazy`.  The `token_indexers` will help AllenNLP map tokens to integers to keep track of them in the future.  If `lazy=True`, the AllenNLP won’t store the dataset in memory, but will load it from disk in batch-size chunks. This is desirable if your dataset is too large to fit in memory, but for our purposes we’ll stick with it being false.\n",
    "\n",
    "The next thing we need to define is the `_read()` function. The `_read()` function only takes in a `file_path: str` argument in pretty much every case. The purpose of this function is to take a single file which contains the dataset and convert it to a list of `Instances`.\n",
    "\n",
    "Last is to write `text_to_instance`.  Most code is very readable.  Note that AllenNLP has `SequenceLabelField` which supports sequential labels, common to some NLP tasks such as POS tagging, coreference resolution, and NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Dict, List, Iterator\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import Field, LabelField, TextField, SequenceLabelField\n",
    "\n",
    "@DatasetReader.register(\"conll_03_reader\")\n",
    "class CoNLL03DatasetReader(DatasetReader):    \n",
    "    def __init__(self,\n",
    "                token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                lazy: bool = False) -> None:\n",
    "            super().__init__(lazy)\n",
    "            self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        is_divider = lambda line: line.strip() == ''\n",
    "\n",
    "        with open(file_path, 'r') as conll_file:\n",
    "            for divider, lines in itertools.groupby(conll_file, is_divider):  #read each sentence groupby empty line\n",
    "                if not divider:\n",
    "                    fields = [l.strip().split() for l in lines] #e.g., [['EU', 'NNP', 'I-NP', 'I-ORG'], ['rejects', 'VBZ', 'I-VP', 'O'],...\n",
    "                    # switch it so that each field is a list of tokens/labels\n",
    "                    fields = [l for l in zip(*fields)]  #[('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'), ('NNP\n",
    "                    # only keep the tokens and NER labels\n",
    "                    tokens, _, _, ner_tags = fields  #('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.') ('I-ORG            \n",
    "\n",
    "                    yield self.text_to_instance(tokens, ner_tags)\n",
    "                    \n",
    "    def text_to_instance(self,\n",
    "                         words: List[str],\n",
    "                         ner_tags: List[str]) -> Instance:\n",
    "        fields: Dict[str, Field] = {}\n",
    "        # wrap each token in the file with a token object\n",
    "        tokens = TextField([Token(w) for w in words], self._token_indexers)\n",
    "\n",
    "        # Instances in AllenNLP are created using Python dictionaries,\n",
    "        # which map the token key to the Field type\n",
    "        fields[\"tokens\"] = tokens\n",
    "        fields[\"label\"] = SequenceLabelField(ner_tags, tokens)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Dataset Reader\n",
    "\n",
    "Now that we’ve written a dataset reader, we want to test that it can successfully load the CoNLL dataset, and perhaps see some corpus statistics.  Do:\n",
    "\n",
    "    touch test_reader.jsonnet\n",
    "\n",
    "Put the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   dataset_reader: {\n",
    "#     type: 'conll_03_reader',\n",
    "#     lazy: false\n",
    "#   },\n",
    "\n",
    "#   train_data_path: '../data/train.txt',\n",
    "#   validation_data_path: '../data/validation.txt',\n",
    "#   model: {},\n",
    "#   data_loader: {},\n",
    "#   trainer: {}\n",
    "# }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in the terminal:\n",
    "    \n",
    "    allennlp train --dry-run --include-package conn_ner -s /tmp/tagging/tests/0 test_reader.jsonnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
