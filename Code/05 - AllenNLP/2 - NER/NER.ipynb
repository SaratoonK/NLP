{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## NER using AllenNLP\n",
    "\n",
    "The problem statement of NER is as follows:\n",
    "\n",
    "Given \n",
    "\n",
    "    Bill Gates and Paul Allen, founders of Microsoft, started selling software in 1975 in New Mexico.\n",
    "\n",
    "We want to get\n",
    "\n",
    "    [Bill Gates PER] and [Paul Allen PER], founders of [Microsoft ORG], started selling software in [1975 DATE] in [New Mexico LOC].\n",
    "\n",
    "To do this, there are two steps:\n",
    "\n",
    "- First, as a segmentation task, where we attempt to find and classify segments that match entities, and assign some NULL or O label to the in-between stuff. Thus, our label space would be {PER, ORG, DATE, LOC, O}.\n",
    "- Second, as a token-level tagging task. This one requires a bit more thought — it’s not clear from the start how we associate entities with each other. But if you introduce a slightly modified label space, you can reconstruct the entities.\n",
    "\n",
    "To do this, each entity type (e.g. PER, LOC) gets split into two labels: B-PER, denoting “this is a new person entity” and I-PER, denoting, “I’m continuing the previous person entity”. On the above sentence, every token would be tagged like so:\n",
    "\n",
    "    [Bill B-PER] [Gates I-PER] and [Paul B-PER] [Allen I-PER], founders of [Microsoft B-ORG], started selling software in [1975 B-DATE] in [New B-LOC] [Mexico I-LOC].\n",
    "\n",
    "For brevity’s sake, I left out all the [and O] tags, but you can imagine that all the rest of the words in the sentence are assigned that null tag.\n",
    "\n",
    "\n",
    "### 1. Loading the CoNLL 2003 dataset\n",
    "\n",
    "Let’s take a look at an example from the CoNLL’03 dataset and see if they conform to the specification we laid down above:\n",
    "\n",
    "    Essex NNP I-NP I-ORG\n",
    "    , , O O\n",
    "    however RB I-ADVP O\n",
    "    , , O O\n",
    "    look VB I-VP O\n",
    "    certain JJ I-ADJP O\n",
    "    to TO I-VP O\n",
    "    regain VB I-VP O\n",
    "    their PRP$ I-NP O\n",
    "    .\n",
    "    .\n",
    "\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. Thus we only care about the first and last item.\n",
    "\n",
    "The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase.\n",
    "\n",
    "To download the dataset, run in your terminal:\n",
    "\n",
    "    curl -o train.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n",
    "    curl -o validation.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa\n",
    "    curl -o test.txt https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n",
    "\n",
    "### 2. Dataset Reader\n",
    "\n",
    "The first thing we’re going to do is build a dataset reader, which can consume the CoNLL’03 dataset.  To get started, let’s create the directory structure for this project. Currently, you should have directories that look something like this:\n",
    "\n",
    "    mkdir conn_ner/readers\n",
    "    touch conn_ner/__init__.py\n",
    "    touch conn_ner/readers/__init__.py\n",
    "    touch conn_ner/readers/conll_reader.py\n",
    "\n",
    "Put the following code in a file in the `conll_reader.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Dict, List, Iterator\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import Field, LabelField, TextField, SequenceLabelField\n",
    "\n",
    "@DatasetReader.register(\"conll_03_reader\")\n",
    "class CoNLL03DatasetReader(DatasetReader):    \n",
    "    def __init__(self,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 **kwargs,\n",
    "                 ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        is_divider = lambda line: line.strip() == ''\n",
    "\n",
    "        with open(file_path, 'r') as conll_file:\n",
    "            for divider, lines in itertools.groupby(conll_file, is_divider):  #read each sentence groupby empty line\n",
    "                if not divider:\n",
    "                    fields = [l.strip().split() for l in lines] #e.g., [['EU', 'NNP', 'I-NP', 'I-ORG'], ['rejects', 'VBZ', 'I-VP', 'O'],...\n",
    "                    # switch it so that each field is a list of tokens/labels\n",
    "                    fields = [l for l in zip(*fields)]  #[('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'), ('NNP\n",
    "                    # only keep the tokens and NER labels\n",
    "                    tokens, _, _, ner_tags = fields  #('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.') ('I-ORG            \n",
    "\n",
    "                    yield self.text_to_instance(tokens, ner_tags)\n",
    "                    \n",
    "    def text_to_instance(self,\n",
    "                         words: List[str],\n",
    "                         ner_tags: List[str]) -> Instance:\n",
    "        fields: Dict[str, Field] = {}\n",
    "        # wrap each token in the file with a token object\n",
    "        tokens = TextField([Token(w) for w in words], self._token_indexers)\n",
    "\n",
    "        # Instances in AllenNLP are created using Python dictionaries,\n",
    "        # which map the token key to the Field type\n",
    "        fields[\"tokens\"] = tokens\n",
    "        fields[\"label\"] = SequenceLabelField(ner_tags, tokens)\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@DatasetReader.register(...)` is one core feature of AllenNLP, `Registrables`.  When you call `.register()`, it allows us to confgure our experiments with JSON even though we write all the code in Python.\n",
    "\n",
    "When you write a new `Model`, a new `DatasetReader`, a new `Metric`, or pretty much anything else, you’ll want to register it so it’s visible to your configuration file. \n",
    "\n",
    "Every class that inherits from DatasetReader **should override these 3 functions**:\n",
    "\n",
    "    __init__(self, ...) -> None\n",
    "    _read(self, file_path: str) -> List[Instance]\n",
    "    text_to_instance(self, ...) -> Instance\n",
    "\n",
    "Any argument in `__init__()` will be visible to the JSON configuration later on, so if you have parameters in the dataset reader you want to change in between experiments, you’ll put them there. The `token_indexers` will help AllenNLP map tokens to integers to keep track of them in the future.\n",
    "\n",
    "The next thing we need to define is the `_read()` function. The `_read()` function only takes in a `file_path: str` argument in pretty much every case. The purpose of this function is to take a single file which contains the dataset and convert it to a list of `Instances`.\n",
    "\n",
    "Last is to write `text_to_instance`.  Most code is very readable.  Note that AllenNLP has `SequenceLabelField` which supports sequential labels, common to some NLP tasks such as POS tagging, coreference resolution, and NER."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Dataset Reader\n",
    "\n",
    "Let's test our dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.instance.Instance at 0x1503a3e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = CoNLL03DatasetReader()\n",
    "text = ['ACL', '2024', 'is', 'in', 'Thailand']\n",
    "label = ['I-ORG' , '0', '0', '0', 'I-LOC']\n",
    "instance = reader.text_to_instance(text, label)\n",
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-ORG', '0', '0', '0', 'I-LOC']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the label\n",
    "instance.fields['label'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ACL, 2024, is, in, Thailand]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the token\n",
    "instance.fields['tokens'][:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model\n",
    "\n",
    "LSTM, RNN, or GRU is a reasonable baseline. At every timestep, the LSTM takes in a token and outputs a prediction.\n",
    "Conveniently, building a sequence tagging LSTM in AllenNLP is reasonably straightforward. It’s also easy to swap out LSTM’s, GRU’s, RNN’s, BiLSTM’s, etc. without ever touching the model code. We’ll see how that’s done in this next section.\n",
    "\n",
    "To prep, do this:\n",
    "\n",
    "    mkdir conn_ner/my_models\n",
    "    touch conn_ner/my_models/__init__.py\n",
    "    touch conn_ner/my_models/lstm.py\n",
    "\n",
    "#### Making the model\n",
    "\n",
    "To complete the model, we’ll have to fill in 3 functions of our own:\n",
    "\n",
    "- `__init__(self, ...) -> None` - the initialization function, which takes all the configurable submodules as arguments\n",
    "- `forward(self, ...) -> Dict[str, torch.Tensor]` - the forward function, which defines a single forward-pass for our model. Note that the output is a Python dict. We’ll get to why this is later in the section.\n",
    "- `get_metrics(self, reset: bool = False) -> Dict[str, float]` - this method works with the way AllenNLP defines training metrics, and returns a dictionary where each key is the name of a metric you want to track. For instance, you could track recall, precision, f1 as different metrics.\n",
    "\n",
    "Our `__init__` function will always take a `Vocabulary` object, because we need to call `super()` with it. However, the rest of the arguments are defined by (a) the model architecture, and (b) what we want to expose to the configuration file.\n",
    "\n",
    "In this case, we’re going to want an `LSTM` or `GRU` to actually do the sequence tagging. AllenNLP has a module `Seq2SeqEncoder` which is a generalization over recurrent encoders like this, so we’ll want to take in a `Seq2SeqEncoder`. We’ll also want to use word embeddings, like GloVe or word2vec. To do this, we can make use of AllenNLP’s `TextFieldEmbedder`.  And to classify the LSTM outputs, we’ll need some kind of linear transformation. We have two options here, using a configurable `FeedForward` module or just defining an `nn.Linear` module ourselves. Because we don’t expect that this will ever change, let’s leave it as an `nn.Linear` for now.  Finally, we’ll want some metric to track our training progress, besides loss. Canonically, the best metric for this is macro-averaged F1 — it’s what CoNLL reports. In this case, we can use AllenNLP’s `SpanBasedF1Measure`, which will report the class-based and overall precision, recall, and F1.\n",
    "\n",
    "In `lstm.py`, put this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders.seq2seq_encoder import Seq2SeqEncoder\n",
    "from allennlp.training.metrics import SpanBasedF1Measure\n",
    "\n",
    "@Model.register('ner_lstm')\n",
    "class NerLSTM(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder) -> None:\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self._embedder = embedder\n",
    "        self._encoder = encoder\n",
    "        self._classifier = nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                     out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        self._f1 = SpanBasedF1Measure(vocab, 'labels')\n",
    "    \n",
    "    \n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        \n",
    "        #the tokens input isn’t a tensor of token indexes, it’s a dict. \n",
    "        # That dict contains all the namespaces defined by the  token_indexers.\n",
    "        \n",
    "        #`get_text_field_mask` - this function takes the tokens dict and returns a binary mask over the tokens. \n",
    "        # The mask is passed into the encoder, the metrics, and the sequence loss function so we can ignore missing text.\n",
    "\n",
    "        embedded = self._embedder(tokens) #embed the input tokens using our pretrained word embeddings\n",
    "        encoded = self._encoder(embedded, mask) #encode them using our LSTM or GRU encoder\n",
    "        classified = self._classifier(encoded) #classify each timestep to the target label space\n",
    "\n",
    "        self._f1(classified, label, mask) #compute some classification loss over the sequence of tokens\n",
    "\n",
    "        output: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        if label is not None:\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(classified, label, mask)\n",
    "            \n",
    "        #`sequence_cross_entropy_with_logits` - this is the cross-entropy loss applied to sequence classification/tagging tasks. \n",
    "\n",
    "        return output\n",
    "    \n",
    "    #note that this function is automatically called when we train\n",
    "    def get_metrics(self, reset: bool = True) -> Dict[str, float]:\n",
    "        return self._f1.get_metric(reset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configuring experiments\n",
    "\n",
    "Let's configure our experiment which provides a lot of flexibility.   \n",
    "\n",
    "    touch train_lstm.jsonnet\n",
    "\n",
    "To be able to train a model, we’re going to need to fill in the 3 empty configuration keys:\n",
    "- `data_loader`\n",
    "- `model`\n",
    "- `trainer`\n",
    "\n",
    "1. The `data_loader` describes how to batch the data and iterate over it. AllenNLP has a number of built-in iterators that we’ll be using, though for more advanced projects you might be inclined to write your own.\n",
    "\n",
    "There are two main types of iterators that you will likely be using: a **basic iterator** and a **bucket iterator**.  The basic iterator batches it into a fixed batch size, and then (by default) shuffles those batches every epoch. \n",
    "\n",
    "    data_loader: {\n",
    "        batch_size: 10,\n",
    "        shuffle: true\n",
    "    },\n",
    " \n",
    "AllenNLP also offers `batch_samplers`, which allow you to specify how to construct batches. For instance, you can use a **bucket sampling** strategy, which is slightly more advanced. It’s used to minimize the memory foot-print of all the batches. When batching a variable length sequence, AllenNLP will pad all the sequences to the length of the longest sequence in the batch. If you randomly sample from the data, you could end up with some long sequences and some short sequences in the same batch, leading to a lot of extra memory used for padding. Using a bucket iterator sorts all the examples by length and then batches them.\n",
    "\n",
    "\n",
    "2. The `model` configures each of those sub-modules that we defined in the last section. I.e. how big should our LSTM be, what embeddings do we want to use, etc.\n",
    "\n",
    "AllenNLP uses the concept of dependency injection which is a fancy term for specifying holders in the constructor, but specifying the concrete models in the configuration file.  For example, we can easily specify `lstm`, etc.   This allows quick and easy experimentation.\n",
    "\n",
    "3. The `trainer` configures the AllenNLP `Trainer` object, which handles all the training and optimizing for us. We pass it an optimizer, a number of epochs, maybe some early-stopping parameters, etc.\n",
    "\n",
    "In the trainer, `patience` and `validation_metric` are used for early stopping. `cuda_device` specifies whether or not you want to run on the GPU (I have left this set to -1 for CPU-only training).\n",
    "\n",
    "In `train_lstm.jsonnet`, put this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   dataset_reader: {\n",
    "#     type: 'conll_03_reader',\n",
    "#     token_indexers: {\n",
    "#       tokens: {\n",
    "#         type: 'single_id',\n",
    "#         namespace: 'tokens',\n",
    "#         lowercase_tokens: true\n",
    "#       }\n",
    "#     },\n",
    "#   },\n",
    "#   data_loader: {\n",
    "#     batch_sampler: {\n",
    "#       type: 'bucket',\n",
    "#       batch_size: 10\n",
    "#     }\n",
    "#   },\n",
    "#   train_data_path: 'data/train.txt',\n",
    "#   validation_data_path: 'data/validation.txt',\n",
    "#   model: {\n",
    "#     type: 'ner_lstm',\n",
    "#     embedder: {\n",
    "#       token_embedders: {\n",
    "#         tokens: {\n",
    "#         type: 'embedding',\n",
    "#           pretrained_file: \"https://allennlp.s3.amazonaws.com/datasets/glove/glove.6B.50d.txt.gz\",\n",
    "#           embedding_dim: 50,\n",
    "#           trainable: false\n",
    "#         }\n",
    "#       }\n",
    "#     },\n",
    "#     encoder: {\n",
    "#       type: 'lstm',\n",
    "#       input_size: 50,\n",
    "#       hidden_size: 25,\n",
    "#       bidirectional: true\n",
    "#     }\n",
    "#   },\n",
    "#   trainer: {\n",
    "#     num_epochs: 40,\n",
    "#     patience: 10,\n",
    "#     cuda_device: -1,\n",
    "#     grad_clipping: 5.0,\n",
    "#     validation_metric: '-loss',\n",
    "#     optimizer: {\n",
    "#       type: 'adam',\n",
    "#       lr: 0.003\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this:\n",
    "    \n",
    "    allennlp train -f --include-package conn_ner -s models train_lstm.jsonnet\n",
    "    \n",
    "`-f` forces to override serialization dir if exist.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
