{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## AllenNLP\n",
    "\n",
    "AllenNLP is an open source library for building deep learning models for natural language processing, developed by the Allen Institute for Artificial Intelligence. It is built on top of PyTorch and is designed to support researchers, engineers, students, etc., who wish to build high quality deep NLP models with ease. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments.\n",
    "\n",
    "In a nutshell, AllenNLP is\n",
    "\n",
    "- a library with well-thought-out abstractions encapsulating the common data and model operations that are done in NLP research\n",
    "- a commandline tool for training PyTorch models\n",
    "- a collection of pre-trained models that you can use to make predictions\n",
    "- a collection of readable reference implementations of common / recent NLP models\n",
    "- an experiment framework for doing replicable science\n",
    "- a way to demo your research\n",
    "- open source and community driven\n",
    "\n",
    "In part 1, geared towards someone who is brand new to the library, we give you a quick walk-through of main AllenNLP concepts and features. We'll build a complete, working NLP model (a text classifier) along the way.\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "\n",
    "### Fields\n",
    "\n",
    "The first step for building an NLP model is to define its input and output. In AllenNLP, each training example is represented by an `Instance` object. An `Instance` consists of one or more `Fields`, where each `Field` represents one piece of data used by your model, either as an input or an output. `Fields` will get converted to tensors and fed to your model.\n",
    "\n",
    "For text classification, the input and the output are very simple. The model takes a `TextField` that represents the input text and predicts its label, which is represented by a `LabelField`.\n",
    "\n",
    "Note that AllenNLP use the **type hint** features in Python 3, by specifying a colon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#a bit about type hint\n",
    "\n",
    "def some_func(text: str):\n",
    "    print(text)\n",
    "    \n",
    "some_func(\"hello world\")\n",
    "some_func(3)  #won't error, because this is type hinting.  Mostly used by editors to check errors before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import LabelField,  TextField\n",
    "\n",
    "# Inputs\n",
    "text: TextField\n",
    "\n",
    "# Outputs\n",
    "label: LabelField"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "\n",
    "The first step for building an NLP application is to read the dataset and represent it with some internal data structure.\n",
    "\n",
    "AllenNLP uses `DatasetReaders` to read the data, whose job it is to transform raw data files into `Instances` that match the input / output spec. \n",
    "\n",
    "AllenNLP assume the dataset has a simple data file format: `[text] [TAB] [label]`, for example:\n",
    "\n",
    "- I like this movie a lot! [TAB] positive\n",
    "\n",
    "- This was a monstrous waste of time [TAB] negative\n",
    "\n",
    "- AllenNLP is amazing [TAB] positive\n",
    "\n",
    "- Why does this have to be so complicated? [TAB] negative\n",
    "\n",
    "- This sentence expresses no sentiment [TAB] neutral\n",
    "\n",
    "You can implement your own `DatasetReader` by inheriting from the `DatasetReader` class. At minimum, you need to override the `_read()` method, which reads the input dataset and yields `Instances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import LabelField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"classification-tsv\")\n",
    "class ClassificationTsvReader(DatasetReader):\n",
    "    def __init__(self, max_tokens: int = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = SpacyTokenizer()\n",
    "        self.token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if self.max_tokens:\n",
    "            tokens = tokens[: self.max_tokens]\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": text_field}\n",
    "        if label:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(file_path, \"r\") as lines:\n",
    "            for line in lines:\n",
    "                text, sentiment = line.strip().split(\"\\t\")\n",
    "                yield self.text_to_instance(text, sentiment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimal DatasetReader that will return a list of classification Instances when you call `reader.read(file)`. This reader will take each line in the input file, split the text into words using a tokenizer (the SpacyTokenizer shown here relies on spaCy), and represent those words as tensors using a word id in a vocabulary we construct for you.\n",
    "\n",
    "Pay special attention to the text and label keys that are used in the fields dictionary passed to the `Instance` - these keys will be used as parameter names when passing tensors into your `Model` later.\n",
    "\n",
    "Ideally, the output label would be **optional** when we create the`Instances`, so that we can use the same code to make **predictions on unlabeled data (say, in a demo)**.\n",
    "\n",
    "There are lots of places where this could be made better for a more flexible and fully-featured reader but let's keep it simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
