{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Get to the Point \n",
    "\n",
    "This notebook implements model proposed in the paper: [Get to the Point](https://arxiv.org/abs/1704.04368) which is probably one of the most famous paper during that time.  Even today, people is using this pointer-generator architecture as one component in their model.    Btw, if you forget, you may want to read our lecture where we discuss this paper, or you can also encourage to read the paper once before doing this together.\n",
    "\n",
    "Source:\n",
    "- https://github.com/abisee/pointer-generator\n",
    "- https://github.com/atulkum/pointer_summarizer\n",
    "- https://github.com/laihuiyuan/pointer-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 09:33:53.953141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import csv, queue, random, time, glob, struct, time, os, math\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.core.example import example_pb2\n",
    "from threading import Thread\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('chaky_logger')\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_STA = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "UNK = 0\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "BOS_TOKEN = '[BOS]'\n",
    "EOS_TOKEN = '[EOS]'\n",
    "\n",
    "beam_size=4\n",
    "emb_dim= 128\n",
    "batch_size= 16\n",
    "hidden_dim= 256\n",
    "max_enc_steps=400\n",
    "max_dec_steps=100\n",
    "max_tes_steps=100\n",
    "min_dec_steps=35\n",
    "vocab_size=50000\n",
    "\n",
    "lr=0.15\n",
    "cov_loss_wt = 1.0\n",
    "pointer_gen = True\n",
    "is_coverage = False\n",
    "\n",
    "max_grad_norm=2.0\n",
    "adagrad_init_acc=0.1\n",
    "rand_unif_init_mag=0.02\n",
    "trunc_norm_init_std=1e-4\n",
    "\n",
    "eps = 1e-12\n",
    "use_gpu=True\n",
    "lr_coverage=0.15\n",
    "max_iterations = 5 #500000\n",
    "\n",
    "# transformer\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "n_head = 6\n",
    "tran = False\n",
    "dropout = 0.1\n",
    "n_layers = 6\n",
    "d_model = 128\n",
    "d_inner = 512\n",
    "n_warmup_steps = 4000\n",
    "\n",
    "log_root = \"data/log/\"\n",
    "\n",
    "train_data_path  = \"data/finished_files/chunked/train_*\"\n",
    "eval_data_path   = \"data/finished_files/val.bin\"\n",
    "decode_data_path = \"data/finished_files/test.bin\"\n",
    "vocab_path       = \"data/finished_files/vocab\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data\n",
    "\n",
    "Go to https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail and download `FINISHED_FILES`.  This is basically tokenized form of cnn news summaries and dailymail summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Since things are tokenized, we can skip!  Yay!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numericalize\n",
    "\n",
    "We gonna load our data, check for unique vocabs, and numericalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help our numericalization, we gonna made the `Vocab` object that gonna help us handle things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, file, max_size):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.count = 0     # keeps track of total number of words in the Vocab\n",
    "\n",
    "        # [UNK], [PAD], [BOS] and [EOS] get the ids 0,1,2,3.\n",
    "        for w in [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]:\n",
    "            self.word2idx[w] = self.count\n",
    "            self.idx2word[self.count] = w\n",
    "            self.count += 1\n",
    "\n",
    "        # Read the vocab file and add words up to max_size\n",
    "        with open(file, 'r') as fin:\n",
    "            for line in fin:\n",
    "                items = line.split()\n",
    "                if len(items) != 2:\n",
    "                    print('Warning: incorrectly formatted line in vocabulary file: %s' % line.strip())\n",
    "                    continue\n",
    "                w = items[0]\n",
    "                if w in [SENTENCE_STA, SENTENCE_END, UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]:\n",
    "                    raise Exception(\n",
    "                        '<s>, </s>, [UNK], [PAD], [BOS] and [EOS] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "                if w in self.word2idx:\n",
    "                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "                self.word2idx[w] = self.count\n",
    "                self.idx2word[self.count] = w\n",
    "                self.count += 1\n",
    "                if max_size != 0 and self.count >= max_size:\n",
    "                    break\n",
    "        print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (\n",
    "          self.count, self.idx2word[self.count - 1]))\n",
    "\n",
    "    def word2id(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[UNK_TOKEN]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def id2word(self, word_id):\n",
    "        if word_id not in self.idx2word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self.idx2word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def write_metadata(self, path):\n",
    "        print( \"Writing word embedding metadata file to %s...\" % (path))\n",
    "        with open(path, \"w\") as f:\n",
    "            fieldnames = ['word']\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "            for i in range(self.size()):\n",
    "                writer.writerow({\"word\": self.idx2word[i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write some utility function to convert some given article words and abstract words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oov = []\n",
    "    unk_id = vocab.word2id(UNK_TOKEN)\n",
    "    for w in article_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id:  # If w is OOV\n",
    "            if w not in oov:  # Add to list of OOVs\n",
    "                oov.append(w)\n",
    "            oov_num = oov.index(w)  # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            ids.append(vocab.size() + oov_num)  # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNK_TOKEN)\n",
    "    for w in abstract_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id:  # If w is an OOV word\n",
    "            if w in article_oovs:  # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w)  # Map to its temporary article OOV number\n",
    "                ids.append(vocab_idx)\n",
    "            else:  # If w is an out-of-article OOV\n",
    "                ids.append(unk_id)  # Map to the UNK token id\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "\n",
    "    def __init__(self, article, abstract_sentences, vocab):\n",
    "        # Get ids of special tokens\n",
    "        bos_decoding = vocab.word2id(BOS_TOKEN)\n",
    "        eos_decoding = vocab.word2id(EOS_TOKEN)\n",
    "\n",
    "        # Process the article\n",
    "        article_words = article.decode().split()\n",
    "        if len(article_words) > max_enc_steps:\n",
    "            article_words = article_words[:max_enc_steps]\n",
    "        self.enc_len = len(article_words)  # store the length after truncation but before padding\n",
    "        self.enc_inp = [vocab.word2id(w) for w in\n",
    "                          article_words]   # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Process the abstract\n",
    "        abstract = ' '.encode().join(abstract_sentences).decode()\n",
    "        abstract_words = abstract.split()  # list of strings\n",
    "        abs_ids = [vocab.word2id(w) for w in\n",
    "                   abstract_words]         # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Get the decoder input sequence and target sequence\n",
    "        self.dec_inp, self.tgt = self.get_dec_seq(abs_ids, max_dec_steps, bos_decoding, eos_decoding)\n",
    "        self.dec_len = len(self.dec_inp)\n",
    "\n",
    "        # If using pointer-generator mode, we need to store some extra info\n",
    "        if pointer_gen:\n",
    "            # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id;\n",
    "            # also store the in-article OOVs words themselves\n",
    "            self.enc_inp_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "            # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "            abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "            # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
    "            _, self.tgt = self.get_dec_seq(abs_ids_extend_vocab, max_dec_steps, bos_decoding, eos_decoding)\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article = article\n",
    "        self.original_abstract = abstract\n",
    "        self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "    def get_dec_seq(self, sequence, max_len, start_id, stop_id):\n",
    "        src = [start_id] + sequence[:]\n",
    "        tgt = sequence[:]\n",
    "        if len(src) > max_len:   # truncate\n",
    "            src = src[:max_len]\n",
    "            tgt = tgt[:max_len]  # no end_token\n",
    "        else:  # no truncation\n",
    "            tgt.append(stop_id)  # end token\n",
    "        assert len(src) == len(tgt)\n",
    "        return src, tgt\n",
    "\n",
    "    def pad_enc_seq(self, max_len, pad_id):\n",
    "        while len(self.enc_inp) < max_len:\n",
    "            self.enc_inp.append(pad_id)\n",
    "        if pointer_gen:\n",
    "            while len(self.enc_inp_extend_vocab) < max_len:\n",
    "                self.enc_inp_extend_vocab.append(pad_id)\n",
    "\n",
    "    def pad_dec_seq(self, max_len, pad_id):\n",
    "        while len(self.dec_inp) < max_len:\n",
    "            self.dec_inp.append(pad_id)\n",
    "        while len(self.tgt) < max_len:\n",
    "            self.tgt.append(pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self, example_list, vocab, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_id = vocab.word2id(PAD_TOKEN)  # id of the PAD token used to pad sequences\n",
    "        self.init_encoder_seq(example_list)  # initialize the input to the encoder\n",
    "        self.init_decoder_seq(example_list)  # initialize the input and targets for the decoder\n",
    "        self.store_orig_strings(example_list)  # store the original strings\n",
    "\n",
    "    def init_encoder_seq(self, example_list):\n",
    "        # Determine the maximum length of the encoder input sequence in this batch\n",
    "        max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "\n",
    "        # Pad the encoder input sequences up to the length of the longest sequence\n",
    "        for ex in example_list:\n",
    "            ex.pad_enc_seq(max_enc_seq_len, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays\n",
    "        # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
    "        self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "        self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "        self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.enc_batch[i, :] = ex.enc_inp[:]\n",
    "            self.enc_lens[i] = ex.enc_len\n",
    "            for j in range(ex.enc_len):\n",
    "                self.enc_padding_mask[i][j] = 1\n",
    "\n",
    "        # For pointer-generator mode, need to store some extra info\n",
    "        if pointer_gen:\n",
    "            # Determine the max number of in-article OOVs in this batch\n",
    "            self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "            # Store the in-article OOVs themselves\n",
    "            self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "            # Store the version of the enc_batch that uses the article OOV ids\n",
    "            self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "            for i, ex in enumerate(example_list):\n",
    "                self.enc_batch_extend_vocab[i, :] = ex.enc_inp_extend_vocab[:]\n",
    "\n",
    "    def init_decoder_seq(self, example_list):\n",
    "        # Pad the inputs and targets\n",
    "        for ex in example_list:\n",
    "            ex.pad_dec_seq(max_dec_steps, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays.\n",
    "        self.dec_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
    "        self.tgt_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
    "        self.dec_padding_mask = np.zeros((self.batch_size, max_dec_steps), dtype=np.float32)\n",
    "        self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.dec_batch[i, :] = ex.dec_inp[:]\n",
    "            self.tgt_batch[i, :] = ex.tgt[:]\n",
    "            self.dec_lens[i] = ex.dec_len\n",
    "            for j in range(ex.dec_len):\n",
    "                self.dec_padding_mask[i][j] = 1\n",
    "\n",
    "    def store_orig_strings(self, example_list):\n",
    "        self.original_articles = [ex.original_article for ex in example_list]  # list of lists\n",
    "        self.original_abstracts = [ex.original_abstract for ex in example_list]  # list of lists\n",
    "        self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list]  # list of list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract2sents(abstract):\n",
    "    cur_p = 0\n",
    "    sents = []\n",
    "    while True:\n",
    "        try:\n",
    "            sta_p = abstract.index(SENTENCE_STA.encode(), cur_p)\n",
    "            end_p = abstract.index(SENTENCE_END.encode(), sta_p + 1)\n",
    "            cur_p = end_p + len(SENTENCE_END.encode())\n",
    "            sents.append(abstract[sta_p + len(SENTENCE_STA.encode()):end_p])\n",
    "        except ValueError as e:  # no more sentences\n",
    "            return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Batcher(object):\n",
    "    BATCH_QUEUE_MAX = 100  # max number of batches the batch_queue can hold\n",
    "\n",
    "    def __init__(self, vocab, data_path, batch_size, single_pass, mode):\n",
    "        self._vocab = vocab\n",
    "        self._data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.single_pass = single_pass\n",
    "        self.mode = mode\n",
    "\n",
    "        # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
    "        self._batch_queue   = queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "        self._example_queue = queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n",
    "\n",
    "        # Different settings depending on whether we're in single_pass mode or not\n",
    "        if single_pass:\n",
    "            self._num_example_q_threads = 1  # just one thread, so we read through the dataset just once\n",
    "            self._num_batch_q_threads = 1    # just one thread to batch examples\n",
    "            self._bucketing_cache_size = 1   # only load one batch's worth of examples before bucketing\n",
    "            self._finished_reading = False   # this will tell us when we're finished reading the dataset\n",
    "        else:\n",
    "            self._num_example_q_threads = 1  # num threads to fill example queue\n",
    "            self._num_batch_q_threads = 1    # num threads to fill batch queue\n",
    "            self._bucketing_cache_size = 1   # how many batches-worth of examples to load into cache before bucketing\n",
    "\n",
    "        # Start the threads that load the queues\n",
    "        self._example_q_threads = []\n",
    "        for _ in range(self._num_example_q_threads):\n",
    "            self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "            self._example_q_threads[-1].daemon = True\n",
    "            self._example_q_threads[-1].start()\n",
    "        self._batch_q_threads = []\n",
    "        for _ in range(self._num_batch_q_threads):\n",
    "            self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "            self._batch_q_threads[-1].daemon = True\n",
    "            self._batch_q_threads[-1].start()\n",
    "\n",
    "        # Start a thread that watches the other threads and restarts them if they're dead\n",
    "        if not single_pass:                   # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
    "            self._watch_thread = Thread(target=self.watch_threads)\n",
    "            self._watch_thread.daemon = True\n",
    "            self._watch_thread.start()\n",
    "\n",
    "    def next_batch(self):\n",
    "        # If the batch queue is empty, print a warning\n",
    "        if self._batch_queue.qsize() == 0:\n",
    "            logger.warning(\n",
    "                'Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i',\n",
    "                self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "            if self.single_pass and self._finished_reading:\n",
    "                logger.info(\"Finished reading dataset in single_pass mode.\")\n",
    "                return None\n",
    "\n",
    "        batch = self._batch_queue.get()  # get the next Batch\n",
    "        return batch\n",
    "\n",
    "    def fill_example_queue(self):\n",
    "        example_generator = self.example_generator(self._data_path, self.single_pass)\n",
    "        input_gen = self.pair_generator(example_generator)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                (article,\n",
    "                 abstract) = input_gen.__next__()  # read the next example from file. article and abstract are both strings.\n",
    "            except StopIteration:  # if there are no more examples:\n",
    "                logger.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
    "                if self.single_pass:\n",
    "                    logger.info(\n",
    "                        \"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
    "                    self._finished_reading = True\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
    "\n",
    "            abstract_sentences = [sent.strip() for sent in abstract2sents(\n",
    "                abstract)]  # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
    "            example = Example(article, abstract_sentences, self._vocab)\n",
    "            self._example_queue.put(example)\n",
    "\n",
    "    def fill_batch_queue(self):\n",
    "        while True:\n",
    "            if self.mode == 'decode':\n",
    "                # beam search decode mode single example repeated in the batch\n",
    "                ex = self._example_queue.get()\n",
    "                b = [ex for _ in range(self.batch_size)]\n",
    "                self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "            else:\n",
    "                # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
    "                inputs = []\n",
    "                for _ in range(self.batch_size * self._bucketing_cache_size):\n",
    "                    inputs.append(self._example_queue.get())\n",
    "                inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True)  # sort by length of encoder sequence\n",
    "\n",
    "                # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n",
    "                batches = []\n",
    "                for i in range(0, len(inputs), self.batch_size):\n",
    "                    batches.append(inputs[i:i + self.batch_size])\n",
    "                if not self.single_pass:\n",
    "                    random.shuffle(batches)\n",
    "                for b in batches:  # each b is a list of Example objects\n",
    "                    self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "\n",
    "    def watch_threads(self):\n",
    "        while True:\n",
    "            logger.info(\n",
    "                'Bucket queue size: %i, Input queue size: %i',\n",
    "                self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "\n",
    "            time.sleep(60)\n",
    "            for idx, t in enumerate(self._example_q_threads):\n",
    "                if not t.is_alive():  # if the thread is dead\n",
    "                    logger.error('Found example queue thread dead. Restarting.')\n",
    "                    new_t = Thread(target=self.fill_example_queue)\n",
    "                    self._example_q_threads[idx] = new_t\n",
    "                    new_t.daemon = True\n",
    "                    new_t.start()\n",
    "            for idx, t in enumerate(self._batch_q_threads):\n",
    "                if not t.is_alive():  # if the thread is dead\n",
    "                    logger.error('Found batch queue thread dead. Restarting.')\n",
    "                    new_t = Thread(target=self.fill_batch_queue)\n",
    "                    self._batch_q_threads[idx] = new_t\n",
    "                    new_t.daemon = True\n",
    "                    new_t.start()\n",
    "\n",
    "    def pair_generator(self, example_generator):\n",
    "        while True:\n",
    "            e = example_generator.__next__()  # e is a tf.Example\n",
    "            try:\n",
    "                article_text = e.features.feature['article'].bytes_list.value[\n",
    "                    0]  # the article text was saved under the key 'article' in the data files\n",
    "                abstract_text = e.features.feature['abstract'].bytes_list.value[\n",
    "                    0]  # the abstract text was saved under the key 'abstract' in the data files\n",
    "            except ValueError:\n",
    "                logger.error('Failed to get article or abstract from example')\n",
    "                continue\n",
    "            if len(article_text) == 0:  # See https://github.com/abisee/pointer-generator/issues/1\n",
    "                # logger.warning('Found an example with empty article text. Skipping it.')\n",
    "                continue\n",
    "            else:\n",
    "                yield (article_text, abstract_text)\n",
    "\n",
    "    def example_generator(self, data_path, single_pass):\n",
    "        while True:\n",
    "            filelist = glob.glob(data_path)  # get the list of datafiles\n",
    "            assert filelist, ('Error: Empty filelist at %s' % data_path)  # check filelist isn't empty\n",
    "            if single_pass:\n",
    "                filelist = sorted(filelist)\n",
    "            else:\n",
    "                random.shuffle(filelist)\n",
    "            for f in filelist:\n",
    "                reader = open(f, 'rb')\n",
    "                while True:\n",
    "                    len_bytes = reader.read(8)\n",
    "                    if not len_bytes: break  # finished reading this file\n",
    "                    str_len = struct.unpack('q', len_bytes)[0]\n",
    "                    example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                    yield example_pb2.Example.FromString(example_str)\n",
    "            if single_pass:\n",
    "                print(\"example_generator completed reading all datafiles. No more data.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_running_avg_loss(loss, running_avg_loss, step, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    \n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_from_batch(batch, use_cuda):\n",
    "    extra_zeros = None\n",
    "    enc_lens = batch.enc_lens\n",
    "    max_enc_len = np.max(enc_lens)\n",
    "    enc_batch_extend_vocab = None\n",
    "    batch_size = len(batch.enc_lens)\n",
    "    enc_batch = Variable(torch.from_numpy(batch.enc_batch).long())\n",
    "    enc_padding_mask = Variable(torch.from_numpy(batch.enc_padding_mask)).float()\n",
    "\n",
    "    if pointer_gen:\n",
    "        enc_batch_extend_vocab = Variable(torch.from_numpy(batch.enc_batch_extend_vocab).long())\n",
    "        # max_art_oovs is the max over all the article oov list in the batch\n",
    "        if batch.max_art_oovs > 0:\n",
    "            extra_zeros = Variable(torch.zeros((batch_size, batch.max_art_oovs)))\n",
    "\n",
    "    c_t = Variable(torch.zeros((batch_size, 2 * hidden_dim)))\n",
    "\n",
    "    coverage = None\n",
    "    if is_coverage:\n",
    "        coverage = Variable(torch.zeros(enc_batch.size()))\n",
    "\n",
    "    enc_pos = np.zeros((batch_size, max_enc_len))\n",
    "    for i, inst in enumerate(batch.enc_batch):\n",
    "        for j, w_i in enumerate(inst):\n",
    "            if w_i != PAD:\n",
    "                enc_pos[i, j] = (j + 1)\n",
    "            else:\n",
    "                break\n",
    "    enc_pos = Variable(torch.from_numpy(enc_pos).long())\n",
    "\n",
    "    if use_cuda:\n",
    "        c_t = c_t.cuda()\n",
    "        enc_pos = enc_pos.cuda()\n",
    "        enc_batch = enc_batch.cuda()\n",
    "        enc_padding_mask = enc_padding_mask.cuda()\n",
    "\n",
    "        if coverage is not None:\n",
    "            coverage = coverage.cuda()\n",
    "\n",
    "        if extra_zeros is not None:\n",
    "            extra_zeros = extra_zeros.cuda()\n",
    "\n",
    "        if enc_batch_extend_vocab is not None:\n",
    "            enc_batch_extend_vocab = enc_batch_extend_vocab.cuda()\n",
    "\n",
    "\n",
    "    return enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, c_t, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_from_batch(batch, use_cuda):\n",
    "    dec_lens = batch.dec_lens\n",
    "    max_dec_len = np.max(dec_lens)\n",
    "    batch_size = len(batch.dec_lens)\n",
    "    dec_lens = Variable(torch.from_numpy(dec_lens)).float()\n",
    "    tgt_batch = Variable(torch.from_numpy(batch.tgt_batch)).long()\n",
    "    dec_batch = Variable(torch.from_numpy(batch.dec_batch).long())\n",
    "    dec_padding_mask = Variable(torch.from_numpy(batch.dec_padding_mask)).float()\n",
    "\n",
    "    dec_pos = np.zeros((batch_size, max_dec_steps))\n",
    "    for i, inst in enumerate(batch.dec_batch):\n",
    "        for j, w_i in enumerate(inst):\n",
    "            if w_i != PAD:\n",
    "                dec_pos[i, j] = (j + 1)\n",
    "            else:\n",
    "                break\n",
    "    dec_pos = Variable(torch.from_numpy(dec_pos).long())\n",
    "\n",
    "    if use_cuda:\n",
    "        dec_lens = dec_lens.cuda()\n",
    "        tgt_batch = tgt_batch.cuda()\n",
    "        dec_batch = dec_batch.cuda()\n",
    "        dec_padding_mask = dec_padding_mask.cuda()\n",
    "        dec_pos = dec_pos.cuda()\n",
    "\n",
    "    return dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        self.vocab = Vocab(vocab_path, vocab_size)\n",
    "        self.batcher = Batcher(self.vocab, train_data_path,\n",
    "                               batch_size, single_pass=False, mode='train')\n",
    "        time.sleep(10)\n",
    "\n",
    "        train_dir = os.path.join(log_root, 'train_%d' % (int(time.time())))\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.mkdir(train_dir)\n",
    "\n",
    "        self.model_dir = os.path.join(train_dir, 'models')\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "            \n",
    "    def save_model(self, running_avg_loss, iter):\n",
    "        state = {\n",
    "            'iter': iter,\n",
    "            'encoder_state_dict': self.model.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.model.decoder.state_dict(),\n",
    "            'reduce_state_dict': self.model.reduce_state.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'current_loss': running_avg_loss\n",
    "        }\n",
    "        model_save_path = os.path.join(self.model_dir, 'model_%d_%d' % (iter, int(time.time())))\n",
    "        torch.save(state, model_save_path)\n",
    "\n",
    "    def setup_train(self, model_path=None):\n",
    "        self.model = Model(model_path)\n",
    "        initial_lr = lr_coverage if is_coverage else lr\n",
    "\n",
    "        params = list(self.model.encoder.parameters()) + list(self.model.decoder.parameters()) + \\\n",
    "                 list(self.model.reduce_state.parameters())\n",
    "        total_params = sum([param[0].nelement() for param in params])\n",
    "        print('The Number of params of model: %.3f million' % (total_params / 1e6))  # million\n",
    "        self.optimizer = optim.Adagrad(params, lr=initial_lr, initial_accumulator_value=adagrad_init_acc)\n",
    "\n",
    "        start_iter, start_loss = 0, 0\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            start_iter = state['iter']\n",
    "            start_loss = state['current_loss']\n",
    "\n",
    "            if not is_coverage:\n",
    "                self.optimizer.load_state_dict(state['optimizer'])\n",
    "                if use_cuda:\n",
    "                    for state in self.optimizer.state.values():\n",
    "                        for k, v in state.items():\n",
    "                            if torch.is_tensor(v):\n",
    "                                state[k] = v.cuda()\n",
    "\n",
    "        return start_iter, start_loss\n",
    "\n",
    "    def train_one_batch(self, batch):\n",
    "        enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, \\\n",
    "        extra_zeros, c_t, coverage = get_input_from_batch(batch, use_cuda)\n",
    "        dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch = \\\n",
    "            get_output_from_batch(batch, use_cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        if not tran:\n",
    "            enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
    "        else:\n",
    "            enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_pos)\n",
    "\n",
    "        s_t = self.model.reduce_state(enc_h)\n",
    "\n",
    "        step_losses, cove_losses = [], []\n",
    "        for di in range(min(max_dec_len, max_dec_steps)):\n",
    "            y_t = dec_batch[:, di]  # Teacher forcing\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen, next_coverage = \\\n",
    "                self.model.decoder(y_t, s_t, enc_out, enc_fea, enc_padding_mask, c_t,\n",
    "                                   extra_zeros, enc_batch_extend_vocab, coverage, di)\n",
    "            tgt = tgt_batch[:, di]\n",
    "            step_mask = dec_padding_mask[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs + eps)\n",
    "            if is_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + cov_loss_wt * step_coverage_loss\n",
    "                cove_losses.append(step_coverage_loss * step_mask)\n",
    "                coverage = next_coverage\n",
    "\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / dec_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(self.model.encoder.parameters(), max_grad_norm)\n",
    "        clip_grad_norm_(self.model.decoder.parameters(), max_grad_norm)\n",
    "        clip_grad_norm_(self.model.reduce_state.parameters(), max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if is_coverage:\n",
    "            cove_losses = torch.sum(torch.stack(cove_losses, 1), 1)\n",
    "            batch_cove_loss = cove_losses / dec_lens\n",
    "            batch_cove_loss = torch.mean(batch_cove_loss)\n",
    "            return loss.item(), batch_cove_loss.item()\n",
    "\n",
    "        return loss.item(), 0.\n",
    "\n",
    "    def run(self, n_iters, model_path=None):\n",
    "        iter, running_avg_loss = self.setup_train(model_path)\n",
    "        start = time.time()\n",
    "        interval = 100\n",
    "\n",
    "        while iter < n_iters:\n",
    "            batch = self.batcher.next_batch()\n",
    "            loss, cove_loss = self.train_one_batch(batch)\n",
    "\n",
    "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, iter)\n",
    "            iter += 1\n",
    "\n",
    "            if iter % interval == 0:\n",
    "                print(\n",
    "                    'step: %d, second: %.2f , loss: %f, cover_loss: %f' % (iter, time.time() - start, loss, cove_loss))\n",
    "                start = time.time()\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(running_avg_loss, iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModule(nn.Module):\n",
    "    def __init__(self, init='uniform'):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.init = init\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if self.init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif self.init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif self.init == 'truncated_normal':\n",
    "                    self.truncated_normal_(param, mean=0,std=stddev)\n",
    "\n",
    "    def truncated_normal_(self, tensor, mean=0, std=1.):\n",
    "        \"\"\"\n",
    "        Implemented by @ruotianluo\n",
    "        See https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15\n",
    "        \"\"\"\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        self.dec_fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        if is_coverage:\n",
    "            self.con_fc = nn.Linear(1, hidden_dim * 2, bias=False)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, s_t, enc_out, enc_fea, enc_padding_mask, coverage):\n",
    "        b, l, n = list(enc_out.size())\n",
    "\n",
    "        dec_fea = self.dec_fc(s_t)  # B x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, l, n).contiguous()  # B x l x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)     # B*l x 2*hidden_dim\n",
    "\n",
    "        att_features = enc_fea + dec_fea_expanded           # B*l x 2*hidden_dim\n",
    "        if is_coverage:\n",
    "            coverage_inp = coverage.view(-1, 1)             # B*l x 1\n",
    "            coverage_fea = self.con_fc(coverage_inp)        # B*l x 2*hidden_dim\n",
    "            att_features = att_features + coverage_fea\n",
    "\n",
    "        e = torch.tanh(att_features)                        # B*l x 2*hidden_dim\n",
    "        scores = self.fc(e)                                 # B*l x 1\n",
    "        scores = scores.view(-1, l)                         # B x l\n",
    "\n",
    "        attn_dist_ = F.softmax(scores, dim=1) * enc_padding_mask  # B x l\n",
    "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist_ / normalization_factor\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)                        # B x 1 x l\n",
    "        c_t = torch.bmm(attn_dist, enc_out)                       # B x 1 x n\n",
    "        c_t = c_t.view(-1, hidden_dim * 2)                 # B x 2*hidden_dim\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, l)                         # B x l\n",
    "\n",
    "        if is_coverage:\n",
    "            coverage = coverage.view(-1, l)\n",
    "            coverage = coverage + attn_dist\n",
    "\n",
    "        return c_t, attn_dist, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    # seq_lens should be in descending order\n",
    "    def forward(self, input, seq_lens):\n",
    "        embedded = self.src_word_emb(input)  \n",
    "        #embedded: [16, 400, 128]\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, seq_lens, batch_first=True)\n",
    "        output, hidden = self.lstm(packed)\n",
    "\n",
    "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)  # h dim = B x l x n\n",
    "        encoder_outputs = encoder_outputs.contiguous()\n",
    "\n",
    "        encoder_feature = encoder_outputs.view(-1, 2 * hidden_dim)   # B*l x 2*hidden_dim\n",
    "        encoder_feature = self.fc(encoder_feature)\n",
    "\n",
    "        return encoder_outputs, encoder_feature, hidden\n",
    "\n",
    "\n",
    "class ReduceState(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(ReduceState, self).__init__()\n",
    "\n",
    "        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden  # h, c dim = 2 x b x hidden_dim\n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, hidden_dim * 2)\n",
    "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, hidden_dim * 2)\n",
    "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "\n",
    "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0))  # h, c dim = 1 x b x hidden_dim\n",
    "\n",
    "class Decoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention_network = Attention()\n",
    "        # decoder\n",
    "        self.tgt_word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.con_fc = nn.Linear(hidden_dim * 2 + emb_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        if pointer_gen:\n",
    "            self.p_gen_fc = nn.Linear(hidden_dim * 4 + emb_dim, 1)\n",
    "\n",
    "        # p_vocab\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, y_t, s_t, enc_out, enc_fea, enc_padding_mask,\n",
    "                c_t, extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
    "\n",
    "        if not self.training and step == 0:\n",
    "            dec_h, dec_c = s_t\n",
    "            s_t_hat = torch.cat((dec_h.view(-1, hidden_dim),\n",
    "                                 dec_c.view(-1, hidden_dim)), 1)  # B x 2*hidden_dim\n",
    "            c_t, _, coverage_next = self.attention_network(s_t_hat, enc_out, enc_fea,\n",
    "                                                           enc_padding_mask, coverage)\n",
    "            coverage = coverage_next\n",
    "\n",
    "        y_t_embd = self.tgt_word_emb(y_t)\n",
    "        x = self.con_fc(torch.cat((c_t, y_t_embd), 1))\n",
    "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t)\n",
    "\n",
    "        dec_h, dec_c = s_t\n",
    "        s_t_hat = torch.cat((dec_h.view(-1, hidden_dim),\n",
    "                             dec_c.view(-1, hidden_dim)), 1)     # B x 2*hidden_dim\n",
    "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, enc_out, enc_fea,\n",
    "                                                               enc_padding_mask, coverage)\n",
    "\n",
    "        if self.training or step > 0:\n",
    "            coverage = coverage_next\n",
    "\n",
    "        p_gen = None\n",
    "        if pointer_gen:\n",
    "            p_gen_inp = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
    "            p_gen = self.p_gen_fc(p_gen_inp)\n",
    "            p_gen = torch.sigmoid(p_gen)\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, hidden_dim), c_t), 1)  # B x hidden_dim * 3\n",
    "        output = self.fc1(output)  # B x hidden_dim\n",
    "        # output = F.relu(output)\n",
    "\n",
    "        output = self.fc2(output)  # B x vocab_size\n",
    "        vocab_dist = F.softmax(output, dim=1)\n",
    "\n",
    "        if pointer_gen:\n",
    "            vocab_dist_ = p_gen * vocab_dist\n",
    "            attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, model_path=None, is_eval=False):\n",
    "        encoder = Encoder()\n",
    "        decoder = Decoder()\n",
    "        reduce_state = ReduceState()\n",
    "\n",
    "        # shared the embedding between encoder and decoder\n",
    "        decoder.tgt_word_emb.weight = encoder.src_word_emb.weight\n",
    "\n",
    "        if is_eval:\n",
    "            encoder = encoder.eval()\n",
    "            decoder = decoder.eval()\n",
    "            reduce_state = reduce_state.eval()\n",
    "\n",
    "        if use_cuda:\n",
    "            encoder = encoder.cuda()\n",
    "            decoder = decoder.cuda()\n",
    "            reduce_state = reduce_state.cuda()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reduce_state = reduce_state\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            self.encoder.load_state_dict(state['encoder_state_dict'])\n",
    "            self.decoder.load_state_dict(state['decoder_state_dict'], strict=False)\n",
    "            self.reduce_state.load_state_dict(state['reduce_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
      "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n"
     ]
    }
   ],
   "source": [
    "trainer = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of params of model: 0.007 million\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/dsai/lib/python3.8/site-packages/torch/optim/adagrad.py:87: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adagrad, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "trainer.run(max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
