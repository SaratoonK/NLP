{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Technology NER\n",
    "\n",
    "Here we gonna teach our model to learn to label technology stuff.\n",
    "\n",
    "We gonna go through the whole process from labeling to training, so you understand how to do it.  The idea is:\n",
    "\n",
    "1. Grab some raw text containing technological stuffs\n",
    "2. Grab another text containing terms about technology\n",
    "3. Use 2 to annotate 1\n",
    "4. Then train the NER model with the annotated 1\n",
    "\n",
    "First, let us grab the raw text containing technology related stuffs.  We grab these raw texts from patent\n",
    "\n",
    "This is edited from https://github.com/kinivi/patent_ner_linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMMUNICATION DEVICE, COMMUNICATION METHOD AND PROGRAM\\n_____2019_____3500050_____490084061_____EP3500000.txt_____G06K_____G06K7/10722:G06K7/1417:H04L67/104:H04M1/00:H04M11/00:H04W12/001:H04W12/04:H04W12/04033:H04W12/04071:H04W12/06:H04W76/14:H04W84/12:H04W84/20\\nA communication device obtains identification information and a public key of a first other communication device by a particular obtaining method that does not use a wireless LAN and notifies the first other communication device of a role'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you've already unzipped the file\n",
    "# this is a text I grab from \n",
    "#https://www.google.com/patents/sitemap/en/Sitemap/G06/G06K.html\n",
    "patent_data = open('../data/G06K.txt').read().strip()\n",
    "patent_data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since when we train NER, we need to give many samples, each sample as a `Doc`, we gonna split our `patent_data` into many samples.  One doc per one patent.  Looking closely, they are splitted by `\\n\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  2003\n",
      "First patent:  COMMUNICATION DEVICE, COMMUNICATION METHOD AND PRO\n",
      "Second patent:  \n",
      "OPERATIONAL STATUS CLASSIFICATION DEVICE\n",
      "_____201\n"
     ]
    }
   ],
   "source": [
    "# split into patents texts | 1 entry = 1 patent\n",
    "patent_texts = patent_data.split('\\n\\n')\n",
    "print(\"Length: \", len(patent_texts))\n",
    "print(\"First patent: \",  patent_texts[0][:50])\n",
    "print(\"Second patent: \", patent_texts[1][:50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab some technological terms from another text file.  To extract relevant terms from the text, we can use `CountVectorizer` from scikit-learn. In such way, we can remove less frequent terms than some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the potential terms\n",
    "terms = open('../data/manyterms.lower.txt').read().lower().strip().split('\\n')\n",
    "print(terms[44444:44456])\n",
    "print(len(terms), 'terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = open('../data/manyterms.lower.txt').read().lower().strip().split('\\n')\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got a lot of irrelevant terms.  Let's filter only the top 25 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Here lowercase=False option is used to keep the original case of the terms, since we possibly could have term abbreviations. Like API, CAT, etc.\n",
    "cvectorizer = CountVectorizer(ngram_range=(\n",
    "    1, 4), stop_words=\"english\", vocabulary=terms, lowercase=True)\n",
    "X = cvectorizer.fit_transform(patent_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results of the counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row = patents\n",
    "#columns = terms\n",
    "#value  = counts\n",
    "X.toarray().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sum the row for each column (to get each term frequency), sort them, and map to actual vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sum them across all documents\n",
    "counts = np.sum(X, axis=0)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get the actual vocab name\n",
    "vocabs = cvectorizer.get_feature_names_out()\n",
    "cvectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#put in the dataframe nicely for viewing\n",
    "#.T to transpose columns to rows\n",
    "df = pd.DataFrame(counts, columns = vocabs).T.sort_values(by=0, ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SpaCy NER\n",
    "\n",
    "Let's start from the original model, and try to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(patent_texts[0][18000:20000])\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great!  But what we want is to further enhance the model so it can tag some technological stuffs\n",
    "\n",
    "First thing is the create a proper dataset that is compatible with spaCy 3.0 to train a NER model\n",
    "\n",
    "### 2.1 Create Dataset\n",
    "\n",
    "Here we used the libraryâ€™s PhraseMatcher class to find the entities from the pre-defined Wiki list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Creating matcher to label enitites in text\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "# Create an efficient stream of data\n",
    "# nlp.pipe gives you docs\n",
    "patterns = list(nlp.pipe(list(df.index[:25]))) #top 25\n",
    "print(\"patterns:\", patterns)\n",
    "print(\"type:    \", type(patterns[0]))\n",
    "matcher.add(\"TECH\", patterns) #expect list of docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create training and dev dataset, where each sample is simply each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin, Span\n",
    "\n",
    "def create_dataset(text):\n",
    "    #text is each sentence.\n",
    "    docs = []\n",
    "    for doc in nlp.pipe(text):\n",
    "        matches = matcher(doc)\n",
    "        spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "        doc.ents = spans\n",
    "        docs.append(doc)\n",
    "        \n",
    "    train_size = int(len(docs) * 0.8)\n",
    "        \n",
    "    train_docs = docs[:train_size]\n",
    "    dev_docs   = docs[train_size:]\n",
    "\n",
    "    train_doc_bin = DocBin(docs=train_docs)\n",
    "    train_doc_bin.to_disk(\"docs/train.spacy\")\n",
    "\n",
    "    dev_doc_bin = DocBin(docs=dev_docs)\n",
    "    dev_doc_bin.to_disk(\"docs/dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `patent_texts` into sentences, and create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each patent into chunks based on end line\n",
    "patent_lines = patent_data.split('\\n')\n",
    "print(len(patent_lines))\n",
    "patent_lines[2], patent_lines[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 280k+ chunks, it will take too much time, let's just grab 10000 chunks for now for training and dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(patent_lines[:10000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config --force configs/tech-config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train configs/tech-config.cfg --output ./output --paths.train docs/train.spacy --paths.dev docs/dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"output/model-best\")\n",
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Loading and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"output/model-best\")\n",
    "doc = nlp(\"Wi-Fi Direct (registered trademark, which will be hereinafter referred to as WFD) \\\n",
    "           corresponding to a technology for directly performing a communication based on a \\\n",
    "           wireless LAN between communication devices without intermediation of an access \\\n",
    "           point (hereinafter referred to as AP) is standardized in Wi-Fi Alliance serving \\\n",
    "           as a wireless LAN industry group.\")\n",
    "\n",
    "colors = {\"TECH\": \"#F67DE3\"}\n",
    "options = {\"colors\": colors}\n",
    "\n",
    "print(doc.ents)\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
