{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using SpaCy\n",
    "\n",
    "## 0. Text processing using SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Lemmatization\n",
    "\n",
    "It turns your word to its original form.  Very common thing you wanna to do, because YouTubeVideo\n",
    "do not want to confuse your model that run and running are different.\n",
    "\n",
    "Note:  But if you use very powerful neural network like transformer, NO NEED lemmatization...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "ran run\n",
      "running run\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"run ran running\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "    \n",
    "#to NOT confuse the model, you want to convert words to their lemma\n",
    "#for very powerful neural network like Transformer (huggingface), NO NEED TO LEMMATIZATION, bc they understand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Stop words\n",
    "\n",
    "Common preprocessing is to remove stopwords, e.g., at, in, on, etc.  Removing them helps model memorize only the keywords.\n",
    "\n",
    "Note: In powerful network, we DON'T remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serious', 'will', 'on', 'once', 'already']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's demonstrate how to remove stopword\n",
    "doc = nlp(\"Chaky is going to eat at Thammasat with his best friend Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chaky', 'going', 'eat', 'Thammasat', 'best', 'friend', 'Peter', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text not in stopwords:\n",
    "        clean_tokens.append(token.text)\n",
    "        \n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'movie', 'good', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"The movie should have been good.\")\n",
    "\n",
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text not in stopwords:\n",
    "        clean_tokens.append(token.text)\n",
    "        \n",
    "clean_tokens  #not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Removing punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "doc = nlp(\"Chaky, the teacher $  /   @ # at AIT,!!!???? likes to eat naan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #leverage pos tag\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_no_punct = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and token.pos_ != 'SYM':\n",
    "        token_no_punct.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chaky',\n",
       " 'the',\n",
       " 'teacher',\n",
       " '@',\n",
       " '#',\n",
       " 'at',\n",
       " 'AIT',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'naan']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_no_punct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Lowercasing and unnecessary spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chaky',\n",
       " ',',\n",
       " 'the',\n",
       " 'teacher',\n",
       " '$',\n",
       " '',\n",
       " '/',\n",
       " '',\n",
       " '@',\n",
       " '#',\n",
       " 'at',\n",
       " 'ait',\n",
       " ',',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'naan',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_lowercase_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    stripped_lowercase_tokens.append(token.text.lower().strip())\n",
    "    \n",
    "stripped_lowercase_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Combine everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nowadays, we don't preprocess anymore, especially for big models, because you lose a lot of information\n",
    "#if there is something you can clean, is extra spaces or like duplicate symbols.....\n",
    "\n",
    "#if you use ML, e.g., SVM, KNN, RF, you need to preprocess\n",
    "def preprocessing(sentence):\n",
    "    \n",
    "    stopwords = list(STOP_WORDS)\n",
    "    doc = nlp(sentence)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM':\n",
    "                clean_tokens.append(token.text)\n",
    "                \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's do sentiment analysis with the help sklearn and spacy!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp = pd.read_csv('../data/yelp_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_amazon = pd.read_csv('../data/amazon_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_imdb = pd.read_csv('../data/imdb_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
