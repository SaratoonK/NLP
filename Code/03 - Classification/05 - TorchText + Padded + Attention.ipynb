{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 5: TorchText + Padded + BiLSTM + Attention\n",
    "\n",
    "\n",
    "Here we shall explore three kinds of attention in this tutorial - (1) general attention, (2) self attention, and (3) multi-headed self-attention.  \n",
    "\n",
    "Feel free to just skip to the model part because it's the only part changing from Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train, test = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's change the unique label\n",
    "set([y for y, x in list(iter(train))])\n",
    "#{“World”, “Sports”, “Business”, “Sci/Tech”}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 120000 gonna just take up too much of our GPU, and also for the sake of learning, we gonna resize it.....  All `DataPipe` instance has a handy function called `random_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_much, train, valid = train.random_split(total_length=train_size, weights = {\"too_much\": 0.7, \"smaller_train\": 0.2, \"valid\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(valid)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "The first step is to decide which tokenizer we want to use, which depicts how we split our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'torchtext', 'in', 'AIT', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install spacy\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = tokenizer(\"We are learning torchtext in AIT!\")  #some test\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[509, 27, 9, 0, 9]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab(['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab.get_itos()\n",
    "\n",
    "#print 159, for example\n",
    "mapping[509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "vocab(['dddd', 'aaaa'])\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "vocab(['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52828"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we can 50k+ unique vocabularies!\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple') #small for easy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "# vocab.get_itos() returns a list of strings (tokens), where the token at the i'th position is what you get from doing vocab[token]\n",
    "# get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings\n",
    "# therefore pretrained_embedding is a fully \"aligned\" embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52828, 300])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape   #we have X vocabs, each with a 300 fasttext embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torchtext, first thing before the batch iterator is to define how you want to process your text and label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1 #turn {1, 2, 3, 4} to {0, 1, 2, 3} for pytorch training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at example how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275, 4021, 8, 389, 574]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I love to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make the batch iterator.  Here we create a function <code>collate_fn</code> that define how we want to create our batch.  **We gonna add length of the sequence since packed padded sequences require this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] #++<----making sure our embedding layer ignores pad\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require \n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list,  padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader.  Note that {“World”, “Sports”, “Business”, “Sci/Tech”} maps to {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text, length in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape:  torch.Size([64])\n",
      "Text shape:  torch.Size([64, 138])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label shape: \", label.shape) # (batch_size, )\n",
    "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Design the model\n",
    "\n",
    "Here we shall implement three types of attention:\n",
    "\n",
    "1. **General-attention** refers to the attention between output from LSTM (all hidden states) and the last hidden state.\n",
    "2. **Self-attention** refersto the self-attention between output from LSTM (all hidden states) and themselves.  That is, pairwise.\n",
    "3. **Multi-head attention** divides the input into several smaller parts and feed into the attention net parallely, speedying up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "hidden_dim = 256\n",
    "embed_dim  = 300       \n",
    "output_dim = 4 #four classes\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "#training hyperparameters\n",
    "num_epochs=1\n",
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class generalAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    #https://arxiv.org/pdf/1409.0473.pdf\n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.unsqueeze(2)  # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, seq_len, 1]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), seq_len] * [batch_size, seq_len, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.cpu().data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.attention_net(output, hn)\n",
    "        return self.fc(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#this attention mask will be apply after Q @ K^T thus the shape will be batch, seq_len, seq_len\n",
    "def get_pad_mask(text):  #[batch, seq_len]\n",
    "    batch_size, seq_len = text.size()\n",
    "    # eq(zero) is lstm output over PAD token\n",
    "    pad_mask = text.data.eq(0).unsqueeze(1)  # batch_size x 1 x seq_len; we unsqueeze so we can make expansion below\n",
    "    return pad_mask.expand(batch_size, seq_len, seq_len)  # batch_size x seq_len x seq_len\n",
    "\n",
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, len_reduction='mean'):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "        self.len_reduction = len_reduction\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def self_attention_net(self, lstm_output, pad_mask):\n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "                \n",
    "        attn_w = torch.matmul(q, k.transpose(1, 2))\n",
    "                \n",
    "        #apply padding mask \n",
    "        if self.mask:\n",
    "            attn_w.masked_fill_(pad_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "\n",
    "        sfmx_attn_w = self.softmax(attn_w)\n",
    "        # context = [batch_size, seq_len, hidden_dim * num_directions(=2)]\n",
    "        context = torch.matmul(sfmx_attn_w, v)\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()\n",
    "        \n",
    "    def forward(self, text, text_lengths, mask=True):\n",
    "        \n",
    "        self.mask = mask\n",
    "        pad_mask = get_pad_mask(text)\n",
    "        \n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "                \n",
    "        attn_output, attention  = self.self_attention_net(output, pad_mask)\n",
    "                \n",
    "        #attn_output = [batch_size, hidden_dim * num_direction(=2)]\n",
    "        return self.fc(attn_output), attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_heads = 8\n",
    "d_k = (hidden_dim * 2) // n_heads # (256 * 2) // 8\n",
    "\n",
    "#this attention mask will be apply after Q @ K^T thus the shape will be batch, seq_len, seq_len\n",
    "def get_pad_mask(text):  #[batch, seq_len]\n",
    "    batch_size, seq_len = text.size()\n",
    "    # eq(zero) is lstm output over PAD token\n",
    "    pad_mask = text.data.eq(0).unsqueeze(1)  # batch_size x 1 x seq_len; we unsqueeze so we can make expansion below\n",
    "    return pad_mask.expand(batch_size, seq_len, seq_len)  # batch_size x seq_len x seq_len\n",
    "\n",
    "class multiheadAttention(nn.Module):\n",
    "    def __init__(self, len_reduction='mean'):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "        self.len_reduction = len_reduction\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def multi_headed_self_attention_net(self, lstm_output, pad_mask):           \n",
    "        residual, batch_size = lstm_output, lstm_output.size(0) #<---residual added to the last output; batch_size may not be even for the last unit\n",
    "        \n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        #split into heads\n",
    "        q = q.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q: [batch_size x n_heads x seq_len x d_k]\n",
    "        k = k.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k: [batch_size x n_heads x seq_len x d_k]\n",
    "        v = v.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # v: [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # repeat the attention mask n_heads time\n",
    "        pad_mask = pad_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # pad_mask : [batch_size x n_heads x seq_len x seq_len]\n",
    "        \n",
    "        # dot production attention\n",
    "        attn_w = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(d_k) # [batch_size x n_heads x seq_len x seq_len]\n",
    "                \n",
    "        if self.mask:\n",
    "            attn_w.masked_fill_(pad_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        sfmx_attn_w = self.softmax(attn_w)\n",
    "        context = torch.matmul(sfmx_attn_w, v) # [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k) # context: [batch_size x seq_len x n_heads * d_k]\n",
    "        # now context: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # doing skip connection\n",
    "        # https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm\n",
    "        context = self.layer_norm(residual + context)\n",
    "\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()\n",
    "        \n",
    "    def forward(self, text, text_lengths, mask=True):\n",
    "        self.mask = mask\n",
    "        pad_mask = get_pad_mask(text)  # batch_size x seq_len x seq_len\n",
    "        \n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "                \n",
    "        attn_output, attention  = self.multi_headed_self_attention_net(output, pad_mask)        \n",
    "        #attn_output = [batch_size, hidden_dim * num_direction(=2)]\n",
    "        \n",
    "        return self.fc(attn_output), attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = generalAttention().to(device)\n",
    "model1.apply(initialize_weights)\n",
    "model1.embedding.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = selfAttention().to(device)\n",
    "model2.apply(initialize_weights)\n",
    "model2.embedding.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = multiheadAttention().to(device)\n",
    "model3.apply(initialize_weights)\n",
    "model3.embedding.weight.data = fast_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15848400\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "  2048\n",
      "     4\n",
      "______\n",
      "18570196\n",
      "15848400\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "  2048\n",
      "     4\n",
      "262144\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "______\n",
      "19358164\n",
      "15848400\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "307200\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "524288\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "  2048\n",
      "     4\n",
      "262144\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "   512\n",
      "   512\n",
      "______\n",
      "19359188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model1), count_parameters(model2), count_parameters(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify here\n",
    "model = model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions, attention = model(text, text_length)\n",
    "        predictions = predictions.squeeze(1)  #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions, attention = model(text, text_length)\n",
    "            predictions = predictions.squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.710 | Train Acc: 76.17%\n",
      "\t Val. Loss: 0.294 |  Val. Acc: 91.09%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'acc')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAINCAYAAADSoIXVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAMUlEQVR4nO3dfVhVVf7//9cB5UYFvENIQ7E7U/OGAPlIljUxYSapNaViiVSak1YT2ndQwdtRrByHMq2p0aksR7Ks7NKcCrPUSBLCNBXNmzAVkBohMAE5+/dHP8/MSSTBhUfk+biufc2wzlprvxdnD+PrWvvsY7MsyxIAAAAA4Ly4uboAAAAAALgUEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCgiasLuBjZ7XYdOXJEPj4+stlsri4HAAAAgItYlqWffvpJ7du3l5tbzXtThKtqHDlyREFBQa4uAwAAAMBF4tChQ7r88str7EO4qoaPj4+kX36Bvr6+Lq4GAAAAgKuUlJQoKCjIkRFqQriqxulbAX19fQlXAAAAAM7p40I80AIAAAAADCBcAQAAAIABhCsAAAAAMIDPXAEAAACGWJalU6dOqaqqytWl4By5u7urSZMmRr6CiXAFAAAAGFBRUaGjR4/qxIkTri4FtdSsWTNddtll8vDwOK95CFcAAADAebLb7Tpw4IDc3d3Vvn17eXh4GNkJQf2yLEsVFRU6duyYDhw4oKuvvvo3vyi4JoQrAAAA4DxVVFTIbrcrKChIzZo1c3U5qAVvb281bdpU3333nSoqKuTl5VXnuXigBQAAAGDI+ex6wHVMvW+8+wAAAABgAOEKAAAAAAwgXAEAAAAwJjg4WKmpqa4uwyV4oAUAAADQiN18883q3bu3sUD05Zdfqnnz5kbmamgIVwAAAABqZFmWqqqq1KTJb8cHf3//C1DRxYnbAgEAAIB6YFmWTlSccslhWdY51Th69Gh9+umnevbZZ2Wz2WSz2XTw4EFt2LBBNptNH3zwgUJDQ+Xp6alNmzZp3759Gjx4sAICAtSiRQuFh4fr448/dprz17cF2mw2/eMf/9DQoUPVrFkzXX311Vq9enWNdS1btkxhYWHy8fFRYGCgYmNjVVhY6NTnm2++0aBBg+Tr6ysfHx/deOON2rdvn+P1pUuXqnv37vL09NRll12mCRMmnNPv5HywcwUAAADUg58rq9Rt2r9dcu6ds6LVzOO3/6n/7LPPas+ePbruuus0a9YsSb/sPB08eFCSlJiYqPnz5+uKK65Qq1atdOjQIQ0cOFBz5syRp6enXnvtNcXExCg3N1cdO3Y863lmzpypp59+Ws8884wWLlyokSNH6rvvvlPr1q2r7V9ZWanZs2erS5cuKiwsVEJCgkaPHq21a9dKkg4fPqybbrpJN998s9avXy9fX19t3rxZp06dkiS98MILSkhI0Lx583T77beruLhYmzdvrs2vsE4IVwAAAEAj5efnJw8PDzVr1kyBgYFnvD5r1iz9/ve/d/zcunVr9erVy/Hz7Nmz9c4772j16tU17gyNHj1aI0aMkCTNnTtXzz33nDIzMzVgwIBq+z/wwAOO/37FFVfoueeeU3h4uEpLS9WiRQstWrRIfn5+WrFihZo2bSpJuuaaaxxj/vKXv2jixIl6/PHHHW3h4eG/9es4b4QrAAAAoB54N3XXzlnRLju3CWFhYU4/l5aWasaMGVqzZo2OHj2qU6dO6eeff1ZeXl6N8/Ts2dPx35s3by5fX98zbvP7X1lZWZoxY4a2bdum//znP7Lb7ZKkvLw8devWTTk5Obrxxhsdwep/FRYW6siRI7r11ltrs1QjCFcAAABAPbDZbOd0a97F7NdP/Zs0aZI++ugjzZ8/X1dddZW8vb31hz/8QRUVFTXO8+sQZLPZHIHp18rKyhQdHa3o6Gi98cYb8vf3V15enqKjox3n8fb2Puu5anqtvvFACwAAAKAR8/DwUFVV1Tn13bx5s0aPHq2hQ4eqR48eCgwMdHw+y5Tdu3frhx9+0Lx583TjjTfq2muvPWOXq2fPntq4caMqKyvPGO/j46Pg4GClp6cbretcEK4AAACARiw4OFhbtmzRwYMHVVRUdNYdJUm6+uqrtWrVKuXk5Gjbtm2KjY2tsX9ddOzYUR4eHlq4cKH279+v1atXa/bs2U59JkyYoJKSEg0fPlxbt27V3r17tWzZMuXm5kqSZsyYob/+9a967rnntHfvXmVnZ2vhwoVG66wO4QoAAABoxCZNmiR3d3d169bNcQve2SxYsECtWrVSZGSkYmJiFB0dreuvv95oPf7+/nrllVe0cuVKdevWTfPmzdP8+fOd+rRp00br169XaWmp+vfvr9DQUL388suO2w/j4uKUmpqqxYsXq3v37ho0aJD27t1rtM7q2KxzfQh+I1JSUiI/Pz8VFxfL19fX1eUAAADgInfy5EkdOHBAnTt3lpeXl6vLQS3V9P7VJhuwcwUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAA4LwEBwcrNTXV8bPNZtO777571v4HDx6UzWZTTk5Ovdd2ITVxdQEAAAAALi1Hjx5Vq1atXF3GBUe4AgAAAGBUYGCgq0twCW4LBAAAABqpl156Se3bt5fdbndqHzx4sB544AFJ0r59+zR48GAFBASoRYsWCg8P18cff1zjvL++LTAzM1MhISHy8vJSWFiYvvrqq9+sbdmyZQoLC5OPj48CAwMVGxurwsJCpz7ffPONBg0aJF9fX/n4+OjGG2/Uvn37HK8vXbpU3bt3l6enpy677DJNmDDhN897PghXAAAAQH2wLKmizDWHZZ1Tiffcc49++OEHffLJJ462H3/8UevWrdPIkSMlSaWlpRo4cKDS09P11VdfacCAAYqJiVFeXt45naO0tFSDBg1St27dlJWVpRkzZmjSpEm/Oa6yslKzZ8/Wtm3b9O677+rgwYMaPXq04/XDhw/rpptukqenp9avX6+srCw98MADOnXqlCTphRde0Pjx4zV27Fht375dq1ev1lVXXXVONdcVtwUCAAAA9aHyhDS3vWvOPeWI5NH8N7u1atVKt99+u5YvX65bb71VkvTWW2+pbdu2uuWWWyRJvXr1Uq9evRxjZs+erXfeeUerV68+p52g5cuXy263a8mSJfLy8lL37t31/fff649//GON407vnEnSFVdcoeeee07h4eEqLS1VixYttGjRIvn5+WnFihVq2rSpJOmaa65xjPnLX/6iiRMn6vHHH3e0hYeH/2a954OdKwAAAKARGzlypN5++22Vl5dLkt544w0NHz5cbm6/RIXS0lJNmjRJXbt2VcuWLdWiRQvt2rXrnHeudu3apZ49e8rLy8vR1rdv398cl5WVpZiYGHXs2FE+Pj7q37+/JDnOm5OToxtvvNERrP5XYWGhjhw54giMFwo7VwAAAEB9aNrslx0kV537HMXExMiyLK1Zs0bh4eHauHGj/va3vzlenzRpkj766CPNnz9fV111lby9vfWHP/xBFRUV9VG5JKmsrEzR0dGKjo7WG2+8IX9/f+Xl5Sk6OtpxXm9v77OOr+m1+kS4AgAAAOqDzXZOt+a5mpeXl+666y698cYb+vbbb9WlSxddf/31jtc3b96s0aNHa+jQoZJ+2ck6ePDgOc/ftWtXLVu2TCdPnnTsXn3xxRc1jtm9e7d++OEHzZs3T0FBQZKkrVu3OvXp2bOnXn31VVVWVp6xe+Xj46Pg4GClp6c7bm+8ELgtEAAAAGjkRo4cqTVr1mjp0qWOB1mcdvXVV2vVqlXKycnRtm3bFBsbe8bTBWsSGxsrm82mMWPGaOfOnVq7dq3mz59f45iOHTvKw8NDCxcu1P79+7V69WrNnj3bqc+ECRNUUlKi4cOHa+vWrdq7d6+WLVum3NxcSdKMGTP017/+Vc8995z27t2r7OxsLVy48JzrrgvCFQAAANDI/e53v1Pr1q2Vm5ur2NhYp9cWLFigVq1aKTIyUjExMYqOjnba2fotLVq00Pvvv6/t27crJCREU6dO1VNPPVXjGH9/f73yyitauXKlunXrpnnz5p0RyNq0aaP169ertLRU/fv3V2hoqF5++WXHLlZcXJxSU1O1ePFide/eXYMGDdLevXvPue66sFnWOT6nsREpKSmRn5+fiouL5evr6+pyAAAAcJE7efKkDhw4oM6dOzs9uAENQ03vX22yATtXAAAAAGAA4QoAAAAADLgowtWiRYsUHBwsLy8vRUREKDMz86x9KysrNWvWLF155ZXy8vJSr169tG7duvOaEwAAAADOl8vDVVpamhISEjR9+nRlZ2erV69eio6OVmFhYbX9k5KS9Pe//10LFy7Uzp07NW7cOA0dOlRfffVVnecEAAAAgPPl8gdaREREKDw8XM8//7wkyW63KygoSI8++qgSExPP6N++fXtNnTpV48ePd7Tdfffd8vb21uuvv16nOX+NB1oAAACgNnigRcN2STzQoqKiQllZWYqKinK0ubm5KSoqShkZGdWOKS8vP2PB3t7e2rRp03nNWVJS4nQAAAAAtcWDuBsmU++bS8NVUVGRqqqqFBAQ4NQeEBCg/Pz8asdER0drwYIF2rt3r+x2uz766COtWrVKR48erfOcKSkp8vPzcxynvwUaAAAAOBenv1vpxIkTLq4EdXH6fTv9PtZVExPFXEjPPvusxowZo2uvvVY2m01XXnml4uPjtXTp0jrPOXnyZCUkJDh+LikpIWABAADgnLm7u6tly5aOz/g3a9ZMNpvNxVXht1iWpRMnTqiwsFAtW7aUu7v7ec3n0nDVtm1bubu7q6CgwKm9oKBAgYGB1Y7x9/fXu+++q5MnT+qHH35Q+/btlZiYqCuuuKLOc3p6esrT09PAigAAANBYnf63Jg9Ra3hatmx51qxQGy4NVx4eHgoNDVV6erqGDBki6ZeHT6Snp2vChAk1jvXy8lKHDh1UWVmpt99+W/fee+95zwkAAADUlc1m02WXXaZ27dqpsrLS1eXgHDVt2vS8d6xOc/ltgQkJCYqLi1NYWJj69Omj1NRUlZWVKT4+XpI0atQodejQQSkpKZKkLVu26PDhw+rdu7cOHz6sGTNmyG636//9v/93znMCAAAA9cXd3d3YP9bRsLg8XA0bNkzHjh3TtGnTlJ+fr969e2vdunWOB1Lk5eXJze2/z904efKkkpKStH//frVo0UIDBw7UsmXL1LJly3OeEwAAAABMc/n3XF2M+J4rAAAAAFID+p4rAAAAALhUEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADHB5uFq0aJGCg4Pl5eWliIgIZWZm1tg/NTVVXbp0kbe3t4KCgvTEE0/o5MmTjterqqqUnJyszp07y9vbW1deeaVmz54ty7LqeykAAAAAGrEmrjx5WlqaEhIS9OKLLyoiIkKpqamKjo5Wbm6u2rVrd0b/5cuXKzExUUuXLlVkZKT27Nmj0aNHy2azacGCBZKkp556Si+88IJeffVVde/eXVu3blV8fLz8/Pz02GOPXeglAgAAAGgkbJYLt3QiIiIUHh6u559/XpJkt9sVFBSkRx99VImJiWf0nzBhgnbt2qX09HRH28SJE7VlyxZt2rRJkjRo0CAFBARoyZIljj533323vL299frrr59TXSUlJfLz81NxcbF8fX3PZ4kAAAAAGrDaZAOX3RZYUVGhrKwsRUVF/bcYNzdFRUUpIyOj2jGRkZHKyspy3Dq4f/9+rV27VgMHDnTqk56erj179kiStm3bpk2bNun2228/ay3l5eUqKSlxOgAAAACgNlx2W2BRUZGqqqoUEBDg1B4QEKDdu3dXOyY2NlZFRUXq16+fLMvSqVOnNG7cOE2ZMsXRJzExUSUlJbr22mvl7u6uqqoqzZkzRyNHjjxrLSkpKZo5c6aZhQEAAABolFz+QIva2LBhg+bOnavFixcrOztbq1at0po1azR79mxHnzfffFNvvPGGli9fruzsbL366quaP3++Xn311bPOO3nyZBUXFzuOQ4cOXYjlAAAAALiEuGznqm3btnJ3d1dBQYFTe0FBgQIDA6sdk5ycrPvvv18PPfSQJKlHjx4qKyvT2LFjNXXqVLm5uenJJ59UYmKihg8f7ujz3XffKSUlRXFxcdXO6+npKU9PT4OrAwAAANDYuGznysPDQ6GhoU4Pp7Db7UpPT1ffvn2rHXPixAm5uTmX7O7uLkmOR62frY/dbjdZPgAAAAA4cemj2BMSEhQXF6ewsDD16dNHqampKisrU3x8vCRp1KhR6tChg1JSUiRJMTExWrBggUJCQhQREaFvv/1WycnJiomJcYSsmJgYzZkzRx07dlT37t311VdfacGCBXrggQdctk4AAAAAlz6Xhqthw4bp2LFjmjZtmvLz89W7d2+tW7fO8ZCLvLw8p12opKQk2Ww2JSUl6fDhw/L393eEqdMWLlyo5ORkPfLIIyosLFT79u318MMPa9q0aRd8fQAAAAAaD5d+z9XFiu+5AgAAACA1kO+5AgAAAIBLCeEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGuDxcLVq0SMHBwfLy8lJERIQyMzNr7J+amqouXbrI29tbQUFBeuKJJ3Ty5EmnPocPH9Z9992nNm3ayNvbWz169NDWrVvrcxkAAAAAGrkmrjx5WlqaEhIS9OKLLyoiIkKpqamKjo5Wbm6u2rVrd0b/5cuXKzExUUuXLlVkZKT27Nmj0aNHy2azacGCBZKk//znP7rhhht0yy236IMPPpC/v7/27t2rVq1aXejlAQAAAGhEbJZlWa46eUREhMLDw/X8889Lkux2u4KCgvToo48qMTHxjP4TJkzQrl27lJ6e7mibOHGitmzZok2bNkmSEhMTtXnzZm3cuLHOdZWUlMjPz0/FxcXy9fWt8zwAAAAAGrbaZAOX3RZYUVGhrKwsRUVF/bcYNzdFRUUpIyOj2jGRkZHKyspy3Dq4f/9+rV27VgMHDnT0Wb16tcLCwnTPPfeoXbt2CgkJ0csvv1xjLeXl5SopKXE6AAAAAKA2XBauioqKVFVVpYCAAKf2gIAA5efnVzsmNjZWs2bNUr9+/dS0aVNdeeWVuvnmmzVlyhRHn/379+uFF17Q1VdfrX//+9/64x//qMcee0yvvvrqWWtJSUmRn5+f4wgKCjKzSAAAAACNhssfaFEbGzZs0Ny5c7V48WJlZ2dr1apVWrNmjWbPnu3oY7fbdf3112vu3LkKCQnR2LFjNWbMGL344otnnXfy5MkqLi52HIcOHboQywEAAABwCXHZAy3atm0rd3d3FRQUOLUXFBQoMDCw2jHJycm6//779dBDD0mSevToobKyMo0dO1ZTp06Vm5ubLrvsMnXr1s1pXNeuXfX222+ftRZPT095enqe54oAAAAANGYu27ny8PBQaGio08Mp7Ha70tPT1bdv32rHnDhxQm5uziW7u7tLkk4/l+OGG25Qbm6uU589e/aoU6dOJssHAAAAACcufRR7QkKC4uLiFBYWpj59+ig1NVVlZWWKj4+XJI0aNUodOnRQSkqKJCkmJkYLFixQSEiIIiIi9O233yo5OVkxMTGOkPXEE08oMjJSc+fO1b333qvMzEy99NJLeumll1y2TgAAAACXPpeGq2HDhunYsWOaNm2a8vPz1bt3b61bt87xkIu8vDynnaqkpCTZbDYlJSXp8OHD8vf3V0xMjObMmePoEx4ernfeeUeTJ0/WrFmz1LlzZ6WmpmrkyJEXfH0AAAAAGg+Xfs/VxYrvuQIAAAAgNZDvuQIAAACASwnhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABlwU4WrRokUKDg6Wl5eXIiIilJmZWWP/1NRUdenSRd7e3goKCtITTzyhkydPVtt33rx5stls+tOf/lQPlQMAAADAL1wertLS0pSQkKDp06crOztbvXr1UnR0tAoLC6vtv3z5ciUmJmr69OnatWuXlixZorS0NE2ZMuWMvl9++aX+/ve/q2fPnvW9DAAAAACNnMvD1YIFCzRmzBjFx8erW7duevHFF9WsWTMtXbq02v6ff/65brjhBsXGxio4OFi33XabRowYccZuV2lpqUaOHKmXX35ZrVq1uhBLAQAAANCIuTRcVVRUKCsrS1FRUY42Nzc3RUVFKSMjo9oxkZGRysrKcoSp/fv3a+3atRo4cKBTv/Hjx+uOO+5wmvtsysvLVVJS4nQAAAAAQG00ceXJi4qKVFVVpYCAAKf2gIAA7d69u9oxsbGxKioqUr9+/WRZlk6dOqVx48Y53Ra4YsUKZWdn68svvzynOlJSUjRz5sy6LwQAAABAo+fy2wJra8OGDZo7d64WL16s7OxsrVq1SmvWrNHs2bMlSYcOHdLjjz+uN954Q15eXuc05+TJk1VcXOw4Dh06VJ9LAAAAAHAJqtPO1d13360+ffroz3/+s1P7008/rS+//FIrV648p3natm0rd3d3FRQUOLUXFBQoMDCw2jHJycm6//779dBDD0mSevToobKyMo0dO1ZTp05VVlaWCgsLdf311zvGVFVV6bPPPtPzzz+v8vJyubu7O83p6ekpT0/Pc6oZAAAAAKpTp52rzz777IzPOEnS7bffrs8+++yc5/Hw8FBoaKjS09MdbXa7Xenp6erbt2+1Y06cOCE3N+eyT4cly7J06623avv27crJyXEcYWFhGjlypHJycs4IVgAAAABgQp12rkpLS+Xh4XFGe9OmTWv9MIiEhATFxcUpLCxMffr0UWpqqsrKyhQfHy9JGjVqlDp06KCUlBRJUkxMjBYsWKCQkBBFRETo22+/VXJysmJiYuTu7i4fHx9dd911Tudo3ry52rRpc0Y7AAAAAJhSp3DVo0cPpaWladq0aU7tK1asULdu3Wo117Bhw3Ts2DFNmzZN+fn56t27t9atW+d4yEVeXp7TTlVSUpJsNpuSkpJ0+PBh+fv7KyYmRnPmzKnLUgAAAADACJtlWVZtB73//vu66667FBsbq9/97neSpPT0dP3rX//SypUrNWTIENN1XlAlJSXy8/NTcXGxfH19XV0OAAAAABepTTao085VTEyM3n33Xc2dO1dvvfWWvL291bNnT3388cfq379/nYoGAAAAgIasTjtXlzp2rgAAAABItcsGdXpa4JdffqktW7ac0b5lyxZt3bq1LlMCAAAAQINWp3A1fvz4ar9o9/Dhwxo/fvx5FwUAAAAADU2dwtXOnTudvqT3tJCQEO3cufO8iwIAAACAhqZO4crT01MFBQVntB89elRNmtTpGRkAAAAA0KDVKVzddtttmjx5soqLix1tx48f15QpU/T73//eWHEAAAAA0FDUaZtp/vz5uummm9SpUyeFhIRIknJychQQEKBly5YZLRAAAAAAGoI6hasOHTro66+/1htvvKFt27bJ29tb8fHxGjFihJo2bWq6RgAAAAC46NX5A1LNmzdXv3791LFjR1VUVEiSPvjgA0nSnXfeaaY6AAAAAGgg6hSu9u/fr6FDh2r79u2y2WyyLEs2m83xelVVlbECAQAAAKAhqNMDLR5//HF17txZhYWFatasmXbs2KFPP/1UYWFh2rBhg+ESAQAAAODiV6edq4yMDK1fv15t27aVm5ub3N3d1a9fP6WkpOixxx7TV199ZbpOAAAAALio1WnnqqqqSj4+PpKktm3b6siRI5KkTp06KTc311x1AAAAANBA1Gnn6rrrrtO2bdvUuXNnRURE6Omnn5aHh4deeuklXXHFFaZrBAAAAICLXp3CVVJSksrKyiRJs2bN0qBBg3TjjTeqTZs2SktLM1ogAAAAADQENsuyLBMT/fjjj2rVqpXTUwMbqpKSEvn5+am4uFi+vr6uLgcAAACAi9QmG9T5e65+rXXr1qamAgAAAIAGp04PtAAAAAAAOCNcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMOCiCFeLFi1ScHCwvLy8FBERoczMzBr7p6amqkuXLvL29lZQUJCeeOIJnTx50vF6SkqKwsPD5ePjo3bt2mnIkCHKzc2t72UAAAAAaMRcHq7S0tKUkJCg6dOnKzs7W7169VJ0dLQKCwur7b98+XIlJiZq+vTp2rVrl5YsWaK0tDRNmTLF0efTTz/V+PHj9cUXX+ijjz5SZWWlbrvtNpWVlV2oZQEAAABoZGyWZVmuLCAiIkLh4eF6/vnnJUl2u11BQUF69NFHlZiYeEb/CRMmaNeuXUpPT3e0TZw4UVu2bNGmTZuqPcexY8fUrl07ffrpp7rpppt+s6aSkhL5+fmpuLhYvr6+dVwZAAAAgIauNtnApTtXFRUVysrKUlRUlKPNzc1NUVFRysjIqHZMZGSksrKyHLcO7t+/X2vXrtXAgQPPep7i4mJJUuvWrQ1WDwAAAAD/1cSVJy8qKlJVVZUCAgKc2gMCArR79+5qx8TGxqqoqEj9+vWTZVk6deqUxo0b53Rb4P+y2+3605/+pBtuuEHXXXddtX3Ky8tVXl7u+LmkpKSOKwIAAADQWLn8M1e1tWHDBs2dO1eLFy9Wdna2Vq1apTVr1mj27NnV9h8/frx27NihFStWnHXOlJQU+fn5OY6goKD6Kh8AAADAJcqln7mqqKhQs2bN9NZbb2nIkCGO9ri4OB0/flzvvffeGWNuvPFG/d///Z+eeeYZR9vrr7+usWPHqrS0VG5u/82LEyZM0HvvvafPPvtMnTt3Pmsd1e1cBQUF8ZkrAAAAoJFrMJ+58vDwUGhoqNPDKex2u9LT09W3b99qx5w4ccIpQEmSu7u7JOl0TrQsSxMmTNA777yj9evX1xisJMnT01O+vr5OBwAAAADUhks/cyVJCQkJiouLU1hYmPr06aPU1FSVlZUpPj5ekjRq1Ch16NBBKSkpkqSYmBgtWLBAISEhioiI0Lfffqvk5GTFxMQ4Qtb48eO1fPlyvffee/Lx8VF+fr4kyc/PT97e3q5ZKAAAAIBLmsvD1bBhw3Ts2DFNmzZN+fn56t27t9atW+d4yEVeXp7TTlVSUpJsNpuSkpJ0+PBh+fv7KyYmRnPmzHH0eeGFFyRJN998s9O5/vnPf2r06NH1viYAAAAAjY/Lv+fqYsT3XAEAAACQGtBnrgAAAADgUkG4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADDgoghXixYtUnBwsLy8vBQREaHMzMwa+6empqpLly7y9vZWUFCQnnjiCZ08efK85gQAAACA8+HycJWWlqaEhARNnz5d2dnZ6tWrl6Kjo1VYWFht/+XLlysxMVHTp0/Xrl27tGTJEqWlpWnKlCl1nhMAAAAAzpfNsizLlQVEREQoPDxczz//vCTJbrcrKChIjz76qBITE8/oP2HCBO3atUvp6emOtokTJ2rLli3atGlTneb8tZKSEvn5+am4uFi+vr4mlgkAAACgAapNNnDpzlVFRYWysrIUFRXlaHNzc1NUVJQyMjKqHRMZGamsrCzHbX779+/X2rVrNXDgwDrPWV5erpKSEqcDAAAAAGqjiStPXlRUpKqqKgUEBDi1BwQEaPfu3dWOiY2NVVFRkfr16yfLsnTq1CmNGzfOcVtgXeZMSUnRzJkzDawIAAAAQGPl8s9c1daGDRs0d+5cLV68WNnZ2Vq1apXWrFmj2bNn13nOyZMnq7i42HEcOnTIYMUAAAAAGgOX7ly1bdtW7u7uKigocGovKChQYGBgtWOSk5N1//3366GHHpIk9ejRQ2VlZRo7dqymTp1apzk9PT3l6elpYEUAAAAAGiuX7lx5eHgoNDTU6eEUdrtd6enp6tu3b7VjTpw4ITc357Ld3d0lSZZl1WlOAAAAADhfLt25kqSEhATFxcUpLCxMffr0UWpqqsrKyhQfHy9JGjVqlDp06KCUlBRJUkxMjBYsWKCQkBBFRETo22+/VXJysmJiYhwh67fmBAAAAADTXB6uhg0bpmPHjmnatGnKz89X7969tW7dOscDKfLy8px2qpKSkmSz2ZSUlKTDhw/L399fMTExmjNnzjnPCQAAAACmufx7ri5GfM8VAAAAAKkBfc8VAAAAAFwqCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADDgoghXixYtUnBwsLy8vBQREaHMzMyz9r355ptls9nOOO644w5Hn9LSUk2YMEGXX365vL291a1bN7344osXYikAAAAAGimXh6u0tDQlJCRo+vTpys7OVq9evRQdHa3CwsJq+69atUpHjx51HDt27JC7u7vuueceR5+EhAStW7dOr7/+unbt2qU//elPmjBhglavXn2hlgUAAACgkXF5uFqwYIHGjBmj+Ph4xw5Ts2bNtHTp0mr7t27dWoGBgY7jo48+UrNmzZzC1eeff664uDjdfPPNCg4O1tixY9WrV68ad8QAAAAA4Hy4NFxVVFQoKytLUVFRjjY3NzdFRUUpIyPjnOZYsmSJhg8frubNmzvaIiMjtXr1ah0+fFiWZemTTz7Rnj17dNttt1U7R3l5uUpKSpwOAAAAAKgNl4aroqIiVVVVKSAgwKk9ICBA+fn5vzk+MzNTO3bs0EMPPeTUvnDhQnXr1k2XX365PDw8NGDAAC1atEg33XRTtfOkpKTIz8/PcQQFBdV9UQAAAAAaJZffFng+lixZoh49eqhPnz5O7QsXLtQXX3yh1atXKysrS3/96181fvx4ffzxx9XOM3nyZBUXFzuOQ4cOXYjyAQAAAFxCmrjy5G3btpW7u7sKCgqc2gsKChQYGFjj2LKyMq1YsUKzZs1yav/55581ZcoUvfPOO44nCPbs2VM5OTmaP3++0y2Ip3l6esrT0/M8VwMAAACgMXPpzpWHh4dCQ0OVnp7uaLPb7UpPT1ffvn1rHLty5UqVl5frvvvuc2qvrKxUZWWl3Nycl+bu7i673W6ueAAAAAD4Hy7duZJ+eWx6XFycwsLC1KdPH6WmpqqsrEzx8fGSpFGjRqlDhw5KSUlxGrdkyRINGTJEbdq0cWr39fVV//799eSTT8rb21udOnXSp59+qtdee00LFiy4YOsCAAAA0Li4PFwNGzZMx44d07Rp05Sfn6/evXtr3bp1jodc5OXlnbELlZubq02bNunDDz+sds4VK1Zo8uTJGjlypH788Ud16tRJc+bM0bhx4+p9PQAAAAAaJ5tlWZari7jYlJSUyM/PT8XFxfL19XV1OQAAAABcpDbZoEE/LRAAAAAALhaEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGHBRhKtFixYpODhYXl5eioiIUGZm5ln73nzzzbLZbGccd9xxh1O/Xbt26c4775Sfn5+aN2+u8PBw5eXl1fdSAAAAADRSLg9XaWlpSkhI0PTp05Wdna1evXopOjpahYWF1fZftWqVjh496jh27Nghd3d33XPPPY4++/btU79+/XTttddqw4YN+vrrr5WcnCwvL68LtSwAAAAAjYzNsizLlQVEREQoPDxczz//vCTJbrcrKChIjz76qBITE39zfGpqqqZNm6ajR4+qefPmkqThw4eradOmWrZsWZ1qKikpkZ+fn4qLi+Xr61unOQAAAAA0fLXJBi7duaqoqFBWVpaioqIcbW5uboqKilJGRsY5zbFkyRINHz7cEazsdrvWrFmja665RtHR0WrXrp0iIiL07rvv1scSAAAAAECSi8NVUVGRqqqqFBAQ4NQeEBCg/Pz83xyfmZmpHTt26KGHHnK0FRYWqrS0VPPmzdOAAQP04YcfaujQobrrrrv06aefVjtPeXm5SkpKnA4AAAAAqI0mri7gfCxZskQ9evRQnz59HG12u12SNHjwYD3xxBOSpN69e+vzzz/Xiy++qP79+58xT0pKimbOnHlhigYAAABwSXLpzlXbtm3l7u6ugoICp/aCggIFBgbWOLasrEwrVqzQgw8+eMacTZo0Ubdu3Zzau3btetanBU6ePFnFxcWO49ChQ3VYDQAAAIDGzKXhysPDQ6GhoUpPT3e02e12paenq2/fvjWOXblypcrLy3XfffedMWd4eLhyc3Od2vfs2aNOnTpVO5enp6d8fX2dDgAAAACoDZffFpiQkKC4uDiFhYWpT58+Sk1NVVlZmeLj4yVJo0aNUocOHZSSkuI0bsmSJRoyZIjatGlzxpxPPvmkhg0bpptuukm33HKL1q1bp/fff18bNmy4EEsCAAAA0Ai5PFwNGzZMx44d07Rp05Sfn6/evXtr3bp1jodc5OXlyc3NeYMtNzdXmzZt0ocffljtnEOHDtWLL76olJQUPfbYY+rSpYvefvtt9evXr97XAwAAAKBxcvn3XF2M+J4rAAAAAFID+p4rAAAAALhUEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABjRxdQEXI8uyJP3ybcwAAAAAGq/TmeB0RqgJ4aoaP/30kyQpKCjIxZUAAAAAuBj89NNP8vPzq7GPzTqXCNbI2O12HTlyRD4+PrLZbK4uB2dRUlKioKAgHTp0SL6+vq4uBw0A1wxqi2sGtcU1g9rgemkYLMvSTz/9pPbt28vNreZPVbFzVQ03Nzddfvnlri4D58jX15c/SKgVrhnUFtcMaotrBrXB9XLx+60dq9N4oAUAAAAAGEC4AgAAAAADCFdosDw9PTV9+nR5enq6uhQ0EFwzqC2uGdQW1wxqg+vl0sMDLQAAAADAAHauAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhChetH3/8USNHjpSvr69atmypBx98UKWlpTWOOXnypMaPH682bdqoRYsWuvvuu1VQUFBt3x9++EGXX365bDabjh8/Xg8rwIVWH9fMtm3bNGLECAUFBcnb21tdu3bVs88+W99LQT1ZtGiRgoOD5eXlpYiICGVmZtbYf+XKlbr22mvl5eWlHj16aO3atU6vW5aladOm6bLLLpO3t7eioqK0d+/e+lwCLjCT10xlZaX+/Oc/q0ePHmrevLnat2+vUaNG6ciRI/W9DFxApv/O/K9x48bJZrMpNTXVcNUwxgIuUgMGDLB69eplffHFF9bGjRutq666yhoxYkSNY8aNG2cFBQVZ6enp1tatW63/+7//syIjI6vtO3jwYOv222+3JFn/+c9/6mEFuNDq45pZsmSJ9dhjj1kbNmyw9u3bZy1btszy9va2Fi5cWN/LgWErVqywPDw8rKVLl1rffPONNWbMGKtly5ZWQUFBtf03b95subu7W08//bS1c+dOKykpyWratKm1fft2R5958+ZZfn5+1rvvvmtt27bNuvPOO63OnTtbP//884VaFuqR6Wvm+PHjVlRUlJWWlmbt3r3bysjIsPr06WOFhoZeyGWhHtXH35nTVq1aZfXq1ctq37699be//a2eV4K6IlzhorRz505LkvXll1862j744APLZrNZhw8frnbM8ePHraZNm1orV650tO3atcuSZGVkZDj1Xbx4sdW/f38rPT2dcHWJqO9r5n898sgj1i233GKueFwQffr0scaPH+/4uaqqymrfvr2VkpJSbf97773XuuOOO5zaIiIirIcfftiyLMuy2+1WYGCg9cwzzzheP378uOXp6Wn961//qocV4EIzfc1UJzMz05Jkfffdd2aKhkvV1zXz/fffWx06dLB27NhhderUiXB1EeO2QFyUMjIy1LJlS4WFhTnaoqKi5Obmpi1btlQ7JisrS5WVlYqKinK0XXvtterYsaMyMjIcbTt37tSsWbP02muvyc2N/wlcKurzmvm14uJitW7d2lzxqHcVFRXKyspyeq/d3NwUFRV11vc6IyPDqb8kRUdHO/ofOHBA+fn5Tn38/PwUERFR4/WDhqE+rpnqFBcXy2azqWXLlkbqhuvU1zVjt9t1//3368knn1T37t3rp3gYw78scVHKz89Xu3btnNqaNGmi1q1bKz8//6xjPDw8zvg/qICAAMeY8vJyjRgxQs8884w6duxYL7XDNerrmvm1zz//XGlpaRo7dqyRunFhFBUVqaqqSgEBAU7tNb3X+fn5NfY//Z+1mRMNR31cM7928uRJ/fnPf9aIESPk6+trpnC4TH1dM0899ZSaNGmixx57zHzRMI5whQsqMTFRNputxmP37t31dv7Jkyera9euuu++++rtHDDL1dfM/9qxY4cGDx6s6dOn67bbbrsg5wRwaaqsrNS9994ry7L0wgsvuLocXKSysrL07LPP6pVXXpHNZnN1OTgHTVxdABqXiRMnavTo0TX2ueKKKxQYGKjCwkKn9lOnTunHH39UYGBgteMCAwNVUVGh48ePO+1EFBQUOMasX79e27dv11tvvSXplyd9SVLbtm01depUzZw5s44rQ31x9TVz2s6dO3Xrrbdq7NixSkpKqtNa4Dpt27aVu7v7GU8Pre69Pi0wMLDG/qf/s6CgQJdddplTn969exusHq5QH9fMaaeD1Xfffaf169eza3WJqI9rZuPGjSosLHS626aqqkoTJ05UamqqDh48aHYROG/sXOGC8vf317XXXlvj4eHhob59++r48ePKyspyjF2/fr3sdrsiIiKqnTs0NFRNmzZVenq6oy03N1d5eXnq27evJOntt9/Wtm3blJOTo5ycHP3jH/+Q9Msfr/Hjx9fjylFXrr5mJOmbb77RLbfcori4OM2ZM6f+Fot64+HhodDQUKf32m63Kz093em9/l99+/Z16i9JH330kaN/586dFRgY6NSnpKREW7ZsOeucaDjq45qR/hus9u7dq48//lht2rSpnwXggquPa+b+++/X119/7fh3S05Ojtq3b68nn3xS//73v+tvMag7Vz9RAzibAQMGWCEhIdaWLVusTZs2WVdffbXTY7W///57q0uXLtaWLVscbePGjbM6duxorV+/3tq6davVt29fq2/fvmc9xyeffMLTAi8h9XHNbN++3fL397fuu+8+6+jRo46jsLDwgq4N52/FihWWp6en9corr1g7d+60xo4da7Vs2dLKz8+3LMuy7r//fisxMdHRf/PmzVaTJk2s+fPnW7t27bKmT59e7aPYW7Zsab333nvW119/bQ0ePJhHsV9CTF8zFRUV1p133mldfvnlVk5OjtPflPLycpesEWbVx9+ZX+NpgRc3whUuWj/88IM1YsQIq0WLFpavr68VHx9v/fTTT47XDxw4YEmyPvnkE0fbzz//bD3yyCNWq1atrGbNmllDhw61jh49etZzEK4uLfVxzUyfPt2SdMbRqVOnC7gymLJw4UKrY8eOloeHh9WnTx/riy++cLzWv39/Ky4uzqn/m2++aV1zzTWWh4eH1b17d2vNmjVOr9vtdis5OdkKCAiwPD09rVtvvdXKzc29EEvBBWLymjn9N6i643//LqFhM/135tcIVxc3m2X9/x86AQAAAADUGZ+5AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAACSDh48KJvNppycHFeXAgBooAhXAADU0ejRozVkyBBXlwEAuEgQrgAAAADAAMIVAKDBCQ4OVmpqqlNb7969NWPGDEmSzWbTCy+8oNtvv13e3t664oor9NZbbzn1z8zMVEhIiLy8vBQWFqavvvrK6fWqqio9+OCD6ty5s7y9vdWlSxc9++yzjtdnzJihV199Ve+9955sNptsNps2bNggSTp06JDuvfdetWzZUq1bt9bgwYN18OBBx9gNGzaoT58+at68uVq2bKkbbrhB3333nbHfDwDANQhXAIBLUnJysu6++25t27ZNI0eO1PDhw7Vr1y5JUmlpqQYNGqRu3bopKytLM2bM0KRJk5zG2+12XX755Vq5cqV27typadOmacqUKXrzzTclSZMmTdK9996rAQMG6OjRozp69KgiIyNVWVmp6Oho+fj4aOPGjdq8ebNatGihAQMGqKKiQqdOndKQIUPUv39/ff3118rIyNDYsWNls9ku+O8IAGBWE1cXAABAfbjnnnv00EMPSZJmz56tjz76SAsXLtTixYu1fPly2e12LVmyRF5eXurevbu+//57/fGPf3SMb9q0qWbOnOn4uXPnzsrIyNCbb76pe++9Vy1atJC3t7fKy8sVGBjo6Pf666/LbrfrH//4hyMw/fOf/1TLli21YcMGhYWFqbi4WIMGDdKVV14pSerateuF+JUAAOoZO1cAgEtS3759z/j59M7Vrl271LNnT3l5eZ21vyQtWrRIoaGh8vf3V4sWLfTSSy8pLy+vxvNu27ZN3377rXx8fNSiRQu1aNFCrVu31smTJ7Vv3z61bt1ao0ePVnR0tGJiYvTss8/q6NGjBlYMAHA1whUAoMFxc3OTZVlObZWVlUbPsWLFCk2aNEkPPvigPvzwQ+Xk5Cg+Pl4VFRU1jistLVVoaKhycnKcjj179ig2NlbSLztZGRkZioyMVFpamq655hp98cUXRusHAFx4hCsAQIPj7+/vtNtTUlKiAwcOOPX5dVj54osvHLffde3aVV9//bVOnjx51v6bN29WZGSkHnnkEYWEhOiqq67Svn37nPp4eHioqqrKqe3666/X3r171a5dO1111VVOh5+fn6NfSEiIJk+erM8//1zXXXedli9fXoffBADgYkK4AgA0OL/73e+0bNkybdy4Udu3b1dcXJzc3d2d+qxcuVJLly7Vnj17NH36dGVmZmrChAmSpNjYWNlsNo0ZM0Y7d+7U2rVrNX/+fKfxV199tbZu3ap///vf2rNnj5KTk/Xll1869QkODtbXX3+t3NxcFRUVqbKyUiNHjlTbtm01ePBgbdy4UQcOHNCGDRv02GOP6fvvv9eBAwc0efJkZWRk6LvvvtOHH36ovXv38rkrALgEEK4AAA3O5MmT1b9/fw0aNEh33HGHhgwZ4ng4xGkzZ87UihUr1LNnT7322mv617/+pW7dukmSWrRooffff1/bt29XSEiIpk6dqqeeespp/MMPP6y77rpLw4YNU0REhH744Qc98sgjTn3GjBmjLl26KCwsTP7+/tq8ebOaNWumzz77TB07dtRdd92lrl276sEHH9TJkyfl6+urZs2aaffu3br77rt1zTXXaOzYsRo/frwefvjh+v2lAQDqnc369U3rAAA0cDabTe+8846GDBni6lIAAI0IO1cAAAAAYADhCgAAAAAM4EuEAQCXHO54BwC4AjtXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgwP8HtysYI2uXZEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_accs, label = 'train acc')\n",
    "ax.plot(valid_accs, label = 'valid acc')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.323 | Test Acc: 90.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  190,    27,   231,  1742, 14918,     6,    16,    21,   358,    27,\n",
       "         2561,  1316,   231,     6], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = \"Google is now falling nonstop.  The price is really bad now.\"\n",
    "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[190, 27, 231, 1742, 14918, 6, 16, 21, 358, 27, 2561, 1316, 231, 6]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [x.item() for x in text]\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google',\n",
       " 'is',\n",
       " 'now',\n",
       " 'falling',\n",
       " 'nonstop',\n",
       " '.',\n",
       " ' ',\n",
       " 'The',\n",
       " 'price',\n",
       " 'is',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'now',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mapping[num] for num in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_length):\n",
    "    with torch.no_grad():\n",
    "        output, attention = model(text, text_length)\n",
    "        output = output.squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(text, text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In general, LSTM with attention provides strong flexibility for the network to adjust the weights more creatively, especially among the hidden states, thus generally better than vanilla LSTM.  As for which variant, there are no clear conclusion yet, since different models simply allows more flexibility for the model to learn in different ways, thus researchers are not so interested to compare different variants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
