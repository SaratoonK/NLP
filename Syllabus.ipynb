{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* My name is **Chaklam**.  Welcome you all!\n",
    "\n",
    "* My email - **chaklam@ait.asia**\n",
    "\n",
    "* All materials can be found on my **Github - https://github.com/chaklam-silpasuwanchai/**.  If you forget, you can also get the link from http://chaklam.com\n",
    "\n",
    "* Our TAs, **Mr. Amanda Raj Shrestha** - st122245@ait.asia \n",
    "\n",
    "* Google classroom code - announce in class\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "This course is best taken after the following courses offered in the August semester:\n",
    "\n",
    "* Python for DSAI by myself - https://github.com/chaklam-silpasuwanchai/Python-for-DS-AI\n",
    "  \n",
    "* Machine Learning by Matthew Dailey - https://github.com/dsai-asia/ML\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- Understand Word Embeddings/Vectors, e.g., Word2Vec, GloVe, etc.\n",
    "\n",
    "- Learn some classic NLP knowledge including constituency and dependency parsing\n",
    "\n",
    "- Understand modern NLP models, e.g., LSTM, LSTM with attention, transformers, etc.\n",
    "\n",
    "- Learn typical NLP tasks, e.g., question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Outline\n",
    "\n",
    "The course will be delivered in 13 weeks, 2 lectures per week, 1 lab per week. Each lecture spans 1.5h; lab spans 3h.\n",
    "\n",
    "Part I: Fundamentals\n",
    "1. Word Vectors - Word2vec (*A1 starts*)\n",
    "2. Word Vectors - GloVe\n",
    "3. Neural Networks and Backprops Review (*A2 starts, A1 due*)\n",
    "4. Dependency Parsing \n",
    "5. Constituency Parsing (*A3 starts, A2 due*)\n",
    "\n",
    "Part II: Model Architectures\n",
    "1. Language Models and Recurrent Neural Network \n",
    "2. LSTM and GRU \n",
    "3. Machine Translation, Attention (*A4 starts, A3 due*)\n",
    "4. Transformer   \n",
    "5. Pretrained Models - BERT, GPT, T5\n",
    "6. Word Vectors - FastText, ElMo   (*A5 starts, A4 due*)\n",
    "\n",
    " Part III: NLP Tasks and Evaluations\n",
    "1. Natural Language Generation\n",
    "2. Question-Answering (*A6 starts, A5 due*)\n",
    "3. Analysis of Model's Inner Workings\n",
    "4. **Project Tips and Ideas** (by TAs) (*A6 due*; project starts)\n",
    "5. **Project Proposal**\n",
    "\n",
    "Part IV: Future of NLP\n",
    "1. Knowledge Integration\n",
    "2. Coreference Resolution\n",
    "3. Recent NLP Trend (by TAs)\n",
    "4. Multi-Task Learning\n",
    "5. Meta Learning\n",
    "\n",
    "Part V: Project\n",
    "1. **Project Progress Presentation**\n",
    "2. Project Day\n",
    "3. Project Day\n",
    "4. **Final Project Presentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* Reference books:\n",
    "    - Jurafsky, D. **Speech and Language Processing**, Drafting New Edition (https://web.stanford.edu/~jurafsky/slp3/)\n",
    "    - Tunstall, L. **Natural Language Processing with Transformers**, 2022 (1st edition) (https://www.amazon.com/Natural-Language-Processing-Transformers-Applications)\n",
    "    \n",
    "* Online resources:\n",
    "    - https://web.stanford.edu/class/cs224n/ (a lot of our assignments and lectures come from Manning's awesome course!)\n",
    "    - https://github.com/bentrevett (created an awesome github for learning NLP!)\n",
    "    - https://github.com/graykode/nlp-tutorial (a simple github with very short code; good for learning)\n",
    "    - https://github.com/mhagiwara/100-nlp-papers (listed first 100 influential NLP papers)\n",
    "    - https://github.com/keon/awesome-nlp (collection of all NLP learning resources)\n",
    "    - https://github.com/sebastianruder/NLP-progress (omg...this is like a mini wikipedia for NLP!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade Criteria\n",
    "\n",
    "The course has the following grade criteria:\n",
    "1. Assignment (30%) --> \n",
    "    - There will be a total of 6 coding assignments\n",
    "    - Any late work (indicated by Google Classroom) will be deducted 50%.  **NO excuses** will be accepted.\n",
    "    - We are extremely serious about copying and plagiarism.  This assignment is intended for you to learn.  TA has the privilege to give zeros or partial score to any sort of plagiarism or alike.  Their call IS FINAL.\n",
    "      -  A1: Getting Started (5%)\n",
    "      -  A2: Word2Vec (5%)\n",
    "      -  A3: Dependency Parsing (5%)\n",
    "      -  A4: Bidirectional LSTM with Attention for Classification from Scratch (5%)\n",
    "      -  A5: Transformers for Seq2Seq from Scratch (5%)\n",
    "      -  A6: Pretraining BERT + finetuning (5%)\n",
    "2. Final Project (30%)\n",
    "    - The default project topic are as follows:\n",
    "      - (1) Research Writing Assistant\n",
    "      - (2) Personality-Based Multi-Chatbots\n",
    "      - (3) Stock Trading using News\n",
    "    - You are required to\n",
    "      - Search for relevant papers, datasets\n",
    "      - Design appropriate neural network\n",
    "      - Develop the software when possible\n",
    "      - Perform experiments comparing different models, datasets, metrics\n",
    "    - Group members\n",
    "      - 3-4 people.  Group leader will be provided in a google sheet\n",
    "    - Main criteria focuses on learning, in particular\n",
    "      - (1) Related work (each read at least 5) (20%)\n",
    "      - (2) Experiment rigour (rigorous comparisons) (20%)\n",
    "      - (3) Model superiority (interesting, new, not too simple) (20%)\n",
    "      - (4) Evaluation methods (appropriate) (20%)\n",
    "      - (5) Effort (a lot of engineering and trial and errors; plus for making a prototype) (20%)\n",
    "    - Submission deliverables:  \n",
    "      - Github link containing:\n",
    "        - (1) Python files (e.g., notebook, .py)\n",
    "        - (2) Presentation file (e.g., .pdf, .ppt) \n",
    "        - (3) Dataset (links)\n",
    "3. Flipped Classroom Quiz (5%)\n",
    "    - Containing few MC questions regarding this coming week lecture (usually two lecture slides) - starting from the second lecture!\n",
    "    - Any late work (indicated by Google Classroom) will be deducted 30%.  **NO excuses** will be accepted.\n",
    "4. Midterm Exam (15%)\n",
    "5. Final Exam (20%)\n",
    "\n",
    "Note: Midterm and Final are open book, open internet, MCQ + practical exams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for doing NLP project\n",
    "\n",
    "#### 1. Find relevant papers\n",
    "\n",
    "- Read from the top-2 NLP venues - ACL and EMNLP\n",
    "- Each member reads around 5 papers in the same area\n",
    "- Identify some interesting techniques/models\n",
    "\n",
    "#### 2. Find datasets\n",
    "\n",
    "You can either collect your own datasets via scrapping, or use standard datasets.  These links contain many resources:\n",
    "\n",
    "- check out https://nlpprogress.com\n",
    "- check out https://machinelearningmastery.com/datasets-natural-language-processing\n",
    "- check out https://github.com/niderhoff/nlp-datasets\n",
    "- check out http://statmt.org\n",
    "- check out http://huggingface.co/datasets\n",
    "\n",
    "#### 3. Implement\n",
    "\n",
    "##### 3.1 Emulate\n",
    "Try to emulate the success of at least 3 key papers.\n",
    "\n",
    "##### 3.2 Tweak / Improve\n",
    "Try to see whether you can tweak or further improve\n",
    "\n",
    "##### 3.3 Do experiments\n",
    "Try comparing or do some ablation studies\n",
    "\n",
    "#### Appendix\n",
    "\n",
    "Some tips when you are implementing:\n",
    "\n",
    "- use github for inspiration but don't copy or plagiarize\n",
    "- use nltk / spacy / torchtext to help your work\n",
    "- use validation set to finetune; never touch the test set until the very end\n",
    "- becareful of data leakage, especially from splitting after preprocessing\n",
    "- start from simple strong baselines, then slowly make more complex models\n",
    "- start with subset of the whole dataset, to speed up your experimentation\n",
    "- use wandb or tensorboard\n",
    "- Compare your metric with past work and make a nice table\n",
    "- Try to see whether your work can generalize by using multiple different datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
