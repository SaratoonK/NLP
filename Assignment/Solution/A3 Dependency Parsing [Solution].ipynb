{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Dependency Parsing\n",
    "\n",
    "### Estimated Time: ~10 hours\n",
    "\n",
    "This assignment will build a neural dependency parser using PyTorch.  In part 1, we will review two general neural network techniques (Adam optimization and Dropout).  In part 2, we will implement and train a dependency parser using techniques from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Adam Optimization and Dropout\n",
    "\n",
    "### a) Adam\n",
    "\n",
    "Recall the SGD update rule:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "\n",
    "where $\\theta$ is a vector containing all of the model parameters, $J$ is the loss function, $\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$ is the gradient of the loss function, and $\\alpha$ is the learning rate.  Adam is another possible update rule with two additional steps.\n",
    "\n",
    "- (2 pts) First, Adam uses a trick called momentum by keep track of $\\mathbf{m}$, a rolling average of the gradients:\n",
    "\n",
    "$$\\mathbf{m} = \\beta_1 \\mathbf{m} + (1-\\beta_1)\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "$$\\theta = \\theta - \\alpha \\mathbf{m}$$\n",
    "\n",
    "  where $\\beta_1$ is a hyperparameter between 0 and 1 (often set to 0.9).  Briefly explain in 2-4 sentences (just give an intuition) how using $\\mathbf{m}$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\n",
    "  \n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "#### <font color=\"blue\">Solution</font> \n",
    "\n",
    "The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.  Gain faster convergence and reduced oscillation.\n",
    "\n",
    "- (2 pts) Adam extends the idea of momentum with the trick of adaptive learning rates by keep track of $\\mathbf{v}$, a rolling average of the magnitudes of the gradients:\n",
    "\n",
    "$$\\mathbf{m} = \\beta_1 \\mathbf{m} + (1 - \\beta_1)\\triangledown_\\theta J_{\\text{minibatch}}(\\theta)$$\n",
    "$$\\mathbf{v} = \\beta_2 \\mathbf{v} + (1 - \\beta_2)(\\triangledown_\\theta J_{\\text{minibatch}}(\\theta) \\circ \\triangledown_\\theta J_{\\text{minibatch}}(\\theta))$$\n",
    "$$\\theta = \\theta - \\alpha \\mathbf{m} \\mathbin{/} \\sqrt{\\mathbf{v}}$$\n",
    "\n",
    "where $\\circ$ and $\\mathbin{/}$ denote elementwise multiplication and division (not dot product!).  $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99).  Since Adam divides the update by $\\sqrt{\\mathbf{v}}$, what kinds of weights will receive larger update and smaller update?  Give some simple example of how.  Why might this help with learning?\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "#### <font color=\"blue\">Solution</font> \n",
    "\n",
    "Weights that receive high gradients will have their effective learning rate reduced and vice versa for small gradients which will receive increased learning rate. A simple example is let's say $\\mathbf{v} = [9, 2, 1]$, its square root is $\\sqrt{\\mathbf{v}} = [3, \\sqrt{2}, 1]$.  Thus by dividing $\\mathbf{m}$ by this $\\sqrt{\\mathbf{v}}$, it is essentially scaling the bigger one to be a bit smaller, and the smaller one will become relatively larger.  This might be helpful because it can get recent stagnant parameters moving more efficiently along their axes and in turn expedite convergence.\n",
    "\n",
    "### b) Dropout\n",
    "\n",
    "Dropout is a regularization technique.  During training, dropout randomly sets units in the hidden layer $\\mathbf{h}$ to zero with probabilty $p_{\\text{drop}}$ (dropping different units each minibatch), and then multiplies $\\mathbf{h}$ by a constant $\\gamma$.  We can write this as:\n",
    "\n",
    "$$\\mathbf{h}_{\\text{drop}} = \\gamma \\mathbf{d} \\circ \\mathbf{h}$$\n",
    "\n",
    "where $\\mathbf{d} \\in \\{0, 1\\}^{D_h}$ ($D_h$ is the size of $\\mathbf{h}$) is a mask vector where each entry is 0 with probability $p_{\\text{drop}}$ and 1 with probability ($1 - p_{\\text{drop}}$). For the gamma constant, $\\gamma$ is chosen such that the expected value of $\\mathbf{h}_{\\text{drop}}$ is $\\mathbf{h}$ \n",
    "\n",
    "$$\\mathbb{E}_{\\text{p_drop}}[\\mathbf{h}_{\\text{drop}}]_i = h_i$$\n",
    "\n",
    "for all $i \\in \\{1, \\cdots, D_h\\}$\n",
    "\n",
    "- (2 pts) What must $\\gamma$ equal in term of $p_\\text{drop}$?  Briefly justify your answers or show your math derivation using the equations given above.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "#### <font color=\"blue\">Solution</font> \n",
    "\n",
    "During training we drop units at a rate of $p_{\\text{drop}}$, resulting in roughly $p_{\\text{keep}} = 1 - p_{\\text{drop}}$ fraction of units left over. At test time we’d like to have the effect of keeping a similar fraction, $p_{\\text{keep}}$, of units on. By scaling down the layer units by $γ = \\frac{1}{1-p_{\\text{drop}}} = \\frac{1}{p_{\\text{keep}}}$, we effectively level out their magnitudes so that both phases of learning share very similar expected outputs.\n",
    "\n",
    "- (2pts) Why shoup dropout be applied only during training? Why should dropout NOT be applied during evaluation?\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "#### <font color=\"blue\">Solution</font> \n",
    "\n",
    "The goal of dropout is to reduce overfitting. We’re interested in updating unit weights so as to form a network that performs well across different datasets. Now, during evaluation we’re concerned with how well the model handles unseen data. When we dropout units, we’re “thinning” out the network which in many cases will add noise to predictions and dampen accuracy. Thus, if we were to apply dropout during evaluation time, we would not be able to fairly assess the generalization power of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
