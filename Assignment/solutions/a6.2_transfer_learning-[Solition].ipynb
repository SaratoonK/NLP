{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958506ff-4a0a-42f0-8a55-4024e9abf922",
   "metadata": {},
   "source": [
    "# Assignment 6 : Pretrain and Transfer Learning (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b968508-17cd-4e48-94b7-8cbe5b5c3153",
   "metadata": {},
   "source": [
    "### Before working on the assignment please read papers as following \n",
    "- SUPERVISED CONTRASTIVE LEARNING FOR PRE-TRAINED LANGUAGE MODEL FINE-TUNING\n",
    "  - link: https://openreview.net/pdf?id=cu7IUiO\n",
    "- Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning\n",
    "  - link: https://arxiv.org/abs/2109.06349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e9183-c1e1-40e4-be4d-6d74fb02ece4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question 5: Training on text classification task  on combine two losses Cross Entropy and Supervised Contrastive. (3.5 pts)\n",
    "\n",
    "- Cross Entropy loss\n",
    "$$\n",
    "\\mathcal{L}_\\text{CE} =-\\frac{1}{m} \\sum_{i=1}^{m} yi \\cdot log(\\hat{yi})\n",
    "$$\n",
    "\n",
    "- Supervised Contrastive learning loss\n",
    "$$\n",
    "\\mathcal{L}_\\text{S_cl} = -\\frac{1}{T}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\boldsymbol{1_{yi=yj}}\\enspace log \\frac{e^{sim(hi,hj) / \\tau}}{\\sum_{n=1}^{N} e^{sim(hi,hn) / \\tau}}\n",
    "$$\n",
    "     * detail \n",
    "       * ui ~ sentence i \n",
    "       * hi ~ BERT(ui) in our case using Roberta as a encoder\n",
    "       * hi : (batch_size,sequence_len,embed_size)\n",
    "       * hi is the output of model which is last hidden layers before classifier head in the model architecture\n",
    "       * 1yi=yj ~ we select only the sample that come from the same class to compute in each i and j\n",
    "       * T ~ the number of pairs that come from the same classes\n",
    "       * $\\tau$ ~ temperature parameter\n",
    "       * Sim(x1,x2) : cosine similarity [-1, 1]\n",
    "       - $\\lambda'$ is just weighted of cross entropy loss \n",
    "       * Sim function is the cosine similarity \n",
    "       * N ~ the number of samples in a batch\n",
    "$$\n",
    "sim(A,B) = \\cos{(\\theta)} = \\frac{A\\cdot B}{|\\!|A|\\!||\\!|B|\\!|}\n",
    "$$\n",
    "\n",
    "\n",
    "- Loss total\n",
    "$$\n",
    "  \\mathcal{L}_\\text{total} = \\mathcal{L}_\\text{s_cl} + \\lambda ' \\mathcal{L}_{CE}\n",
    "$$\n",
    "\n",
    "* you can get cross entropy loss like below \n",
    "    * outputs = model(input_ids, labels=labels)\n",
    "    * loss, logits = outputs[:2]\n",
    "    * loss : this is cross entropy loss\n",
    "      \n",
    "- hint : for this question you will utilize the function CustomTextDataset to force dataloader to have at least one pair that come from the same class\n",
    "     * eg. batch_size = 4 \n",
    "     * the labels in a batch should be like [ 0, 21, 43, 0]  \n",
    "     \n",
    "5. training this model in the code below on loss_total by do experiment the same as question 4.1, 4.2, 4.3, 4.4, 4.5, 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd5fa34c-b735-4a50-a4c6-eddd2fe89a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/st121532/work/NLP/NLP/Assignment/solutions\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31687e10-7a63-4592-94a3-dae140324863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from utils import create_supervised_pair, supervised_contrasive_loss, Similarity\n",
    "import matplotlib.pyplot as plt\n",
    "#comment this if you are not using puffer\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e301228-0c6e-49b5-9435-3ac6ab20c13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25d8b12a-4d82-483a-86ce-51d52ff1d10c",
   "metadata": {},
   "source": [
    "## To download data from file directory both text samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbbb405-efb2-4c1b-81fb-d7056b023e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(file_path, do_lower_case=True):\n",
    "    examples = []\n",
    "    \n",
    "    with open('{}/seq.in'.format(file_path),'r',encoding=\"utf-8\") as f_text, open('{}/label'.format(file_path),'r',encoding=\"utf-8\") as f_label:\n",
    "        for text, label in zip(f_text, f_label):\n",
    "            \n",
    "            e = Inputexample(text.strip(),label=label.strip())\n",
    "            examples.append(e)\n",
    "            \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfac055-f91e-4f35-9f02-ca7f0bda7619",
   "metadata": {},
   "source": [
    "## Each sample has a sentence and label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed774baf-945e-4fa9-8b52-621ea34a5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputexample(object):\n",
    "    def __init__(self,text_a,label = None):\n",
    "        self.text = text_a\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49eb741e-7cf4-4432-97ef-87a0c3a91fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset class\n",
    "# ===  =  Hint =  ===\n",
    "# can train on two condition \n",
    "# 1.) trainig training with supervise contrastive loss and cross entropy loss using in question 5.) \n",
    "#    when self.repeated_label == True:\n",
    "# 2.) train only cross entropy loss use in question 4.)\n",
    "#    when self.repeated_label == False:\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,labels,text,batch_size,repeated_label:bool=False):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size \n",
    "        self.count = 0 \n",
    "        self.batch_labels = []\n",
    "        self.repeated_label = repeated_label\n",
    "        \n",
    "        if self.repeated_label == True:\n",
    "            print(\"Train on Combine between Supervised Contrastive and Cross Entropy loss\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Train on Cross Entropy loss\")\n",
    "            \n",
    "        \n",
    "        print(\"len of dataset :\",len(self.labels))\n",
    "              \n",
    "     \n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # write code here for 1)\n",
    "        if self.repeated_label == True:\n",
    "        \n",
    "            if len(np.unique(self.batch_labels)) == self.batch_size - 1:\n",
    "\n",
    "\n",
    "                while True:\n",
    "                    idx = np.random.choice(len(self.labels))\n",
    "\n",
    "                    if self.labels[idx]  in self.batch_labels:\n",
    "\n",
    "                       \n",
    "                        break\n",
    "\n",
    "        self.batch_labels.append(self.labels[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        data = self.text[idx]\n",
    "        \n",
    "        sample = {\"Class\": label,\"Text\": data}\n",
    "\n",
    "\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2f966-fc6a-479f-bf71-e6bfe8b37743",
   "metadata": {},
   "source": [
    "### Train With Combine loss between Cross Entropy and SuperVised Contrastive loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce7446a5-a283-4a58-a327-daa46974d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_learnig(model,optimizer,train_loader,tokenizer,valid_loader,device,epochs:int=30):\n",
    "    \n",
    "    print(\" device using :\",device)\n",
    "    \n",
    "    train_loss_hist = [] \n",
    "    valid_loss_hist = []\n",
    "\n",
    "    test_acc = []\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    train_loss = 0.0 \n",
    "\n",
    "    for e in range(epochs):  # loop over the dataset multiple times\n",
    " \n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        for (idx, batch) in enumerate(train_loader):\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "            #assert len(np.unique(batch[\"Class\"])) < len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "            if h_i is None:\n",
    "                print(\"skip this batch\")\n",
    "                skip_time +=1\n",
    "                continue\n",
    "\n",
    "            # supervised contrastive loss \n",
    "            \n",
    "            loss_s_cl = supervised_contrasive_loss(device,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "\n",
    "        for (idx, batch) in enumerate(valid_loader):\n",
    "            \n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "            #assert len(np.unique(batch[\"Class\"])) < len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "            if h_i is None:\n",
    "                print(\"skip this batch\")\n",
    "                skip_time +=1\n",
    "                continue\n",
    "\n",
    "            # supervised contrastive loss \n",
    "            loss_s_cl = supervised_contrasive_loss(device,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "            \n",
    "            # Calculate Loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "        # 5.3 add code to collect loss \n",
    "        train_loss_hist.append(train_loss / len(train_loader)) \n",
    "        valid_loss_hist.append(valid_loss / len(valid_loader))\n",
    "\n",
    "        print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(valid_loader)}')\n",
    "\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = valid_loss   \n",
    "            \n",
    "                    # Saving State Dict\n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            \n",
    "    return train_loss_hist, valid_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f656717-e0e7-4e51-8541-6ab4c2b25c59",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e365ee63-74ce-421f-964b-b644b9fdc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "valid_samples = []\n",
    "valid_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 4 \n",
    "lr= 1e-5  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd1ea-56ff-4e4c-a2ef-36787f04c2d1",
   "metadata": {},
   "source": [
    "### The Aim of these training is to fine tuning on few shot setting on text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361f0ce-c287-4cb2-9ebd-d8487c3f35bc",
   "metadata": {},
   "source": [
    "Path example of train, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e58a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_5shot = f'./HWU64/train_5/'\n",
    "valid_path = f'./HWU64/valid/'\n",
    "test_path = f'./HWU64/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07260f49-9987-48ea-95b5-09cf84f8ae85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f24b0c99-e4fe-4be3-a9dc-15d707ee4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== small train set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 320\n",
      "===== validation set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 1076\n",
      "===== test set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 1076\n"
     ]
    }
   ],
   "source": [
    "# Download data fewshot \n",
    "# https://downgit.github.io/#/home?url=https:%2F%2Fgithub.com%2Fjianguoz%2FFew-Shot-Intent-Detection%2Ftree%2Fmain%2FDatasets%2FHWU64\n",
    "\n",
    "# load data\n",
    "train_samples = load_examples(path_5shot)\n",
    "valid_samples = load_examples(valid_path)\n",
    "test_samples = load_examples(test_path)\n",
    "\n",
    "\n",
    "print(\"===== small train set ====\")\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "    data.append(train_samples[i].text)\n",
    "    labels.append(train_samples[i].label)\n",
    "\n",
    "\n",
    "train_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== validation set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(valid_samples)):\n",
    "    data.append(valid_samples[i].text)\n",
    "    labels.append(valid_samples[i].label)\n",
    "\n",
    "valid_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"===== test set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "    \n",
    "for i in range(len(test_samples)):\n",
    "    data.append(test_samples[i].text)\n",
    "    labels.append(test_samples[i].label)\n",
    "\n",
    "test_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# got the number of unique classes from dataset\n",
    "num_class = len(np.unique(np.array(labels)))\n",
    "\n",
    "# get text label of uniqure classes\n",
    "unique_label = np.unique(np.array(labels))\n",
    "\n",
    "# map text label to index classes\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5c6bd-cbe4-4ad4-a044-7b4e77e82539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb838e0-a43b-4dbd-a385-900266c70098",
   "metadata": {},
   "source": [
    "### 5.1  freeze weight from pretrain model all layer except classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb71a7f-7980-4a5b-9f78-75c703e9fc0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Download Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dc1fb27-ba93-4d66-a39b-a709a8737ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download config of Roberta config \n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\",output_hidden_states=True)\n",
    "\n",
    "#chnage modifying the number of classes\n",
    "config.num_labels = num_class\n",
    "# Download pretrain models weight \n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "# change from binary classification to muli-classification and loss automatically change to cross entropy loss\n",
    "model.num_labels = config.num_labels\n",
    "# change the output of last layer to num_class that we want to predict\n",
    "model.classifier.out_proj = nn.Linear(in_features=embed_dim,out_features=num_class)\n",
    "# move to model to device that we set\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33bc0813-91cd-40e6-a8c3-4e7364470d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1348164-9eaa-4df1-98f5-64be2da1d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer that use to tokenize sentence into words by using Pretrain from roberta-base\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db0d8278-47d1-4848-8f43-5f1daf687593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "## Fine-Tune  model on SuperVised Contrastive loss \n",
    "# 5.1 freeze weight from pretrain model all layer except classifier \n",
    "model = freeze_layers(model,freeze_layers_count=9)\n",
    "# Using adam optimizer \n",
    "optimizer= AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399de358-1c71-4ce9-9ee1-e9ece0a13685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device using : cuda:1\n",
      "Epoch 1 \t\t Training Loss: 1.4296314895153046 \t\t Validation Loss: 1.4283918975454282\n",
      "Validation Loss Decreased(inf--->384.237420) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 2.855183680355549 \t\t Validation Loss: 1.4286323169793338\n",
      "Epoch 3 \t\t Training Loss: 4.2766884624958035 \t\t Validation Loss: 1.4291732377722361\n",
      "Epoch 4 \t\t Training Loss: 5.693604393303394 \t\t Validation Loss: 1.4304552268804671\n",
      "Epoch 5 \t\t Training Loss: 7.104439316689968 \t\t Validation Loss: 1.434097200078149\n",
      "Epoch 6 \t\t Training Loss: 8.509288670122624 \t\t Validation Loss: 1.4401414212684205\n",
      "Epoch 7 \t\t Training Loss: 9.911508025228978 \t\t Validation Loss: 1.4451070332615792\n",
      "Epoch 8 \t\t Training Loss: 11.312921841442584 \t\t Validation Loss: 1.4478552611787079\n",
      "Epoch 9 \t\t Training Loss: 12.713139644265175 \t\t Validation Loss: 1.446641223581307\n",
      "Epoch 10 \t\t Training Loss: 14.113011240959167 \t\t Validation Loss: 1.4518223974341353\n",
      "Epoch 11 \t\t Training Loss: 15.512352658808231 \t\t Validation Loss: 1.454423796288586\n",
      "Epoch 12 \t\t Training Loss: 16.911441616714 \t\t Validation Loss: 1.4571421345813567\n",
      "Epoch 13 \t\t Training Loss: 18.310273449122906 \t\t Validation Loss: 1.4568356958020576\n",
      "Epoch 14 \t\t Training Loss: 19.708601965010168 \t\t Validation Loss: 1.4582005340370547\n",
      "Epoch 15 \t\t Training Loss: 21.106855706870554 \t\t Validation Loss: 1.4597127083065784\n",
      "Epoch 16 \t\t Training Loss: 22.5047297090292 \t\t Validation Loss: 1.4602310471375193\n",
      "Epoch 17 \t\t Training Loss: 23.902009269595148 \t\t Validation Loss: 1.4599652698048873\n",
      "Epoch 18 \t\t Training Loss: 25.29911327958107 \t\t Validation Loss: 1.4633400511121217\n",
      "Epoch 19 \t\t Training Loss: 26.69502032250166 \t\t Validation Loss: 1.4648478554083955\n",
      "Epoch 20 \t\t Training Loss: 28.089078190922738 \t\t Validation Loss: 1.4672471324750482\n",
      "Epoch 21 \t\t Training Loss: 29.48141579478979 \t\t Validation Loss: 1.4673831702165\n",
      "Epoch 22 \t\t Training Loss: 30.87112239599228 \t\t Validation Loss: 1.4687724175506365\n",
      "Epoch 23 \t\t Training Loss: 32.260540856420995 \t\t Validation Loss: 1.4701101172812365\n",
      "Epoch 24 \t\t Training Loss: 33.649639962613584 \t\t Validation Loss: 1.4701043265459706\n",
      "Epoch 25 \t\t Training Loss: 35.034980569779876 \t\t Validation Loss: 1.4695891110870474\n",
      "Epoch 26 \t\t Training Loss: 36.41871177703142 \t\t Validation Loss: 1.471332492881548\n",
      "Epoch 27 \t\t Training Loss: 37.80174607336521 \t\t Validation Loss: 1.4717822992225562\n",
      "Epoch 28 \t\t Training Loss: 39.182682174444196 \t\t Validation Loss: 1.4717129743675317\n",
      "Epoch 29 \t\t Training Loss: 40.56235717087984 \t\t Validation Loss: 1.4707122469480154\n",
      "Epoch 30 \t\t Training Loss: 41.9418822735548 \t\t Validation Loss: 1.4718392750587606\n"
     ]
    }
   ],
   "source": [
    "train_log, valid_log = train_contrastive_learnig(model,optimizer,train_loader,tokenizer,valid_loader,device,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97ec1470-40ce-4697-9111-30ec584c4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(torch.tensor(train_log, device= 'cpu')))\n",
    "# plt.plot(list(torch.tensor(valid_log, device= 'cpu')))\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.title(\"Combine loss by freezing 9 layers\")\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698c61b-fe8f-4c6c-8976-224c007365ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9e0a45a-a63d-4476-a363-b10cff519a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 43\n"
     ]
    }
   ],
   "source": [
    "#test_acc = test(model,test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f18f9314-8187-4167-9d2c-2ac6e489c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Accuracy : {100 * test_acc} %') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ecebf-79c4-41c6-9cc7-c5b83d4c1f6b",
   "metadata": {},
   "source": [
    "### 5.2  freeze all from top embeddings to encoder layers (9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a84bf-458b-40e7-9ffd-2adf69dbabd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Download Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4ca63a6-2637-42bc-99d3-a91d9ee2c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model,freeze_layers_count:int=0):\n",
    "\n",
    "        \"\"\"\n",
    "        model : model object that we create \n",
    "        freeze_layers_count : the number of layers to freeze \n",
    "        \"\"\"\n",
    "        # write the code here\n",
    "    \n",
    "        # should not more than the number of layers in a backbone\n",
    "        assert freeze_layers_count <= 12  \n",
    "        if freeze_layers_count <= 0:\n",
    "            pass\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                # print(type(name))\n",
    "\n",
    "                keys = name.split(\".\")\n",
    "\n",
    "                if str(freeze_layers_count) in keys or 'classifier' in keys:\n",
    "                    break\n",
    "\n",
    "                param.requires_grad = False \n",
    "\n",
    "\n",
    "        #print all parameter that we want to train from scratch \n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            \n",
    "            if param.requires_grad == True:\n",
    "                 \n",
    "                print(name)\n",
    "        \n",
    "    \n",
    "        return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec6d137d-73c6-4d15-9ef7-df08b3698dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download config of Roberta config \n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\",output_hidden_states=True)\n",
    "\n",
    "#chnage modifying the number of classes\n",
    "config.num_labels = num_class\n",
    "# Download pretrain models weight \n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "# change from binary classification to muli-classification and loss automatically change to cross entropy loss\n",
    "model.num_labels = config.num_labels\n",
    "# change the output of last layer to num_class that we want to predict\n",
    "model.classifier.out_proj = nn.Linear(in_features=embed_dim,out_features=num_class)\n",
    "# move to model to device that we set\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "590266de-7916-4053-aa6f-9d2c2ee235a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "## Fine-Tune  model on SuperVised Contrastive loss \n",
    "# 5.1 freeze weight from pretrain model all layer except classifier \n",
    "model = freeze_layers(model,freeze_layers_count=12)\n",
    "# Using adam optimizer \n",
    "optimizer= AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61ee41e2-8569-4210-aaad-491c3b0dd9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device using : cuda:1\n",
      "Epoch 1 \t\t Training Loss: 1.444173736870289 \t\t Validation Loss: 1.428385520513173\n",
      "Validation Loss Decreased(inf--->384.235705) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 2.909568244218826 \t\t Validation Loss: 1.4286169846261745\n",
      "Epoch 3 \t\t Training Loss: 4.340028534829616 \t\t Validation Loss: 1.4287136367705675\n",
      "Epoch 4 \t\t Training Loss: 5.768017871677875 \t\t Validation Loss: 1.4291461595372197\n",
      "Epoch 5 \t\t Training Loss: 7.184777577221394 \t\t Validation Loss: 1.4295452460923603\n",
      "Epoch 6 \t\t Training Loss: 8.63200129121542 \t\t Validation Loss: 1.4300799644569482\n",
      "Epoch 7 \t\t Training Loss: 10.053215716779231 \t\t Validation Loss: 1.4309153547960587\n",
      "Epoch 8 \t\t Training Loss: 11.490088197588921 \t\t Validation Loss: 1.4320489460650874\n",
      "Epoch 9 \t\t Training Loss: 12.962871822714806 \t\t Validation Loss: 1.4333633800421506\n",
      "Epoch 10 \t\t Training Loss: 14.401282618939877 \t\t Validation Loss: 1.4346479877663367\n",
      "Epoch 11 \t\t Training Loss: 15.837058457732201 \t\t Validation Loss: 1.4362616671948627\n",
      "Epoch 12 \t\t Training Loss: 17.270420168340205 \t\t Validation Loss: 1.4379108997968937\n",
      "Epoch 13 \t\t Training Loss: 18.683485914766788 \t\t Validation Loss: 1.439747535163142\n",
      "Epoch 14 \t\t Training Loss: 20.11793523579836 \t\t Validation Loss: 1.441257690408416\n",
      "Epoch 15 \t\t Training Loss: 21.534391345083712 \t\t Validation Loss: 1.443125601151619\n",
      "Epoch 16 \t\t Training Loss: 22.94248693883419 \t\t Validation Loss: 1.444685977630899\n",
      "Epoch 17 \t\t Training Loss: 24.35827429294586 \t\t Validation Loss: 1.446283411359255\n",
      "Epoch 18 \t\t Training Loss: 25.800294314324855 \t\t Validation Loss: 1.4476389308844357\n",
      "Epoch 19 \t\t Training Loss: 27.234272404015066 \t\t Validation Loss: 1.4490534200101093\n",
      "Epoch 20 \t\t Training Loss: 28.66232139915228 \t\t Validation Loss: 1.4501627909649704\n",
      "Epoch 21 \t\t Training Loss: 30.119241397082806 \t\t Validation Loss: 1.4514603951606608\n",
      "Epoch 22 \t\t Training Loss: 31.5476200401783 \t\t Validation Loss: 1.4527150091185446\n",
      "Epoch 23 \t\t Training Loss: 32.966886532306674 \t\t Validation Loss: 1.45384724432651\n",
      "Epoch 24 \t\t Training Loss: 34.38913951963186 \t\t Validation Loss: 1.4552723359884383\n",
      "Epoch 25 \t\t Training Loss: 35.7971081584692 \t\t Validation Loss: 1.4561207449569136\n",
      "Epoch 26 \t\t Training Loss: 37.22524271458387 \t\t Validation Loss: 1.4572061406635441\n",
      "Epoch 27 \t\t Training Loss: 38.665709753334525 \t\t Validation Loss: 1.458242981850436\n",
      "Epoch 28 \t\t Training Loss: 40.090438295900825 \t\t Validation Loss: 1.4594141329088175\n",
      "Epoch 29 \t\t Training Loss: 41.52173416614532 \t\t Validation Loss: 1.460205527043254\n",
      "Epoch 30 \t\t Training Loss: 42.92656465172767 \t\t Validation Loss: 1.4611824764194985\n"
     ]
    }
   ],
   "source": [
    "train_log, valid_log = train_contrastive_learnig(model,optimizer,train_loader,tokenizer,valid_loader,device,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825789a-8fc5-4920-b2c3-7bc01ad57573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(torch.tensor(train_log, device= 'cpu')))\n",
    "# plt.plot(list(torch.tensor(valid_log, device= 'cpu')))\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.title(\"Combine loss by freezing 12 layers\")\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09744ea-66f8-4a52-8f42-a06325823eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00ef3eac-0ff0-4f4e-b9f2-d7cc9f8c3316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 43\n"
     ]
    }
   ],
   "source": [
    "#test_acc = test(model,test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "785a5508-1057-4001-93a9-5a44ba7a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Accuracy : {100 * test_acc} %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c45cf-c1e8-4ac4-98cb-550f4aa3946b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9a9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
