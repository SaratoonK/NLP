{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#make sure you got these folder\n",
    "from utils.gradcheck import gradcheck_naive, grad_tests_softmax, grad_tests_negsamp\n",
    "from utils.utils import normalizeRows, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Word2Vec\n",
    "\n",
    "### Estimated Time: ~10 hours\n",
    "\n",
    "**Quick note**:  This assignment may be overwhelming for some of you.  It may be wise to set aside some significant amount of time so you can slowly go over this assignment.  The objective of this assignment is for you to understand the math behind <code>word2vec</code>, which will be a good fundamental background to understand any other NLP embedding algorithms.  We will also attempt to implement those maths into code to further enhance our understandings.\n",
    "\n",
    "Let’s have a quick refresher on the word2vec algorithm. For full details, you may want to rewatch the zoom video we did in our first two lectures.  \n",
    "\n",
    "The key insight behind word2vec is that *a word is known by the company it keeps*. Concretely, suppose we have a **center** word $c$ and a contextual window. We shall refer to words that lie in this contextual window as **outside words** denoting $o$. For example, in Figure 1 we see that the center word $c$ is *banking*. Since the context window size is 2, the outside words are *turning*, *into*, *crises*, and *as*.\n",
    "\n",
    "The goal of the skip-gram word2vec algorithm is to accurately learn the probability distribution $P(O|C)$. Given a specific word $o$ and a specific word $c$, we want to calculate $P (O = o|C = c)$, which is the probability that word $o$ is an *outside* word for $c$, i.e., the probability that $o$ falls within the contextual window of $c$.\n",
    "\n",
    "<img src = \"img/word2vec.png\" width=400>\n",
    "\n",
    "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "\n",
    "$$P (O = o|C = c) = \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}$$\n",
    "\n",
    "Here, $u_o$ is the *outside* vector representing outside word $o$, and $v_c$ is the *center* vector representing center word $c$. To contain these parameters, we have two matrices, $U$ and $V$ . The columns of $U$ are all the *outside* vectors $u_w$. The columns of $V$ are all of the *center* vectors $v_w$. Both $U$ and $V$ contain a vector for every $w \\in \\text{Vocabulary}$.\n",
    "\n",
    "Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:\n",
    "\n",
    "$$\\mathbf{J}_\\text{naive-softmax}(v_c, o, U) = -\\log P(O=o |C=c)$$\n",
    "\n",
    "We can view this loss as the cross-entropy2 between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. \n",
    "\n",
    "Furthermore, the $k$th entry in these vectors indicates the conditional probability of the $k$th word being an *outside word* for the given $c$. The true empirical distribution $y$ is a one-hot vector with a 1 for the true outside word $o$, and $0$ everywhere else. The predicted distribution $\\hat{y}$ is the probability distribution $P (O|C = c)$ given by our model in above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Math behind word2vec (7pts)\n",
    "\n",
    "### Question 1 (1pt)\n",
    "\n",
    "#### <font color=\"red\">Answer the following questions</font> \n",
    "\n",
    "1. What is $U$ and the shape of $U$?\n",
    "2. What is $V$ and the shape of $V$?\n",
    "3. What is $u_o$ and the shape of $u_o$?\n",
    "4. What is $v_c$ and the shape of $v_c$?\n",
    "5. What is $y$ and the shape of $y$?\n",
    "6. What is $\\hat{y}$ and the shape of $\\hat{y}$?\n",
    "7. What is the numeric range of the softmax function P (O = o|C = c)?\n",
    "8. Why use $\\log$ after the softmax function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1pt)\n",
    "\n",
    "Show that the naive-softmax loss is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that\n",
    "\n",
    "$$-\\sum_{w \\in V}y_w \\log(\\hat{y}_w) = -\\log(\\hat{y}_o)$$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "*(you may need to study latex to write your answers)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to $v_c$.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to each of the outside word vectors $u_w$'s.  There will be two cases:  when $w = o$ , the true outside word vector, and $w \\neq o$ for all other words.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1pt)\n",
    "\n",
    "Compute the derivatives of the sigmoid function given by \n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1pt)\n",
    "\n",
    "Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\\cdots,w_K$ and their outside vectors as $u_1,\\cdots,u_K$ For this question, assume that the $K$ negative samples are distinct. In other words, $i \\neq j$ implies $w_i \\neq w_j$ for $i,j \\in \\{1,\\cdots,K\\}$. Note that $o \\notin \\{w_1,\\cdots,w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(v_c, o, U) = -\\log(\\sigma(u_o^Tv_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "Compute the partial derivatives of $\\mathbf{J}_{\\text{neg-sample}}$ with respect of $v_c, u_o, \\text{ and } u_k$.  Please write your answers in terms of the vectors $u_o, v_c, \\text{and } u_k$.  \n",
    "\n",
    "After this, explain with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "(1) **w.r.t. $v_c$**:\n",
    "\n",
    "\n",
    "(2) **w.r.t. $u_o$**:\n",
    "\n",
    "\n",
    "(3) **w.r.t. $u_k$**:\n",
    "\n",
    "\n",
    "### Question 7 (1pt)\n",
    "\n",
    "Suppose the center word is $c = w_t$ and the context window is $[w_{t−m}, \\cdots, w_{t−1}, w_t, w_{t+1}, \\cdots, w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
    "\n",
    "$$\\mathbf{J}_{skip-gram}(v_c, w_{t-m}, \\cdots, w_{t+m}, U) = \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\mathbf{J} (v_c, w_{t+j}, U)$$\n",
    "\n",
    "Here, $\\mathbf{J}(v_c, w_{t+j}, U)$ represents an arbirtary loss term for the center word $c=w_t$ and outside word $w_{t+j}$.  $\\mathbf{J}(v_c, w_{t+j}, U)$ could be $\\mathbf{J}_{\\text{naive-softmax}}$ or $\\mathbf{J}_{\\text{neg-sample}}$ depending on your implementation.\n",
    "\n",
    "Write down three partial derivatives:\n",
    "\n",
    "- (i) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial U}$\n",
    "- (ii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_c}$\n",
    "- (iii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_w} \\text{ where } w \\neq c$\n",
    "\n",
    "Write your answers in terms of $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial U$ and $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial v_c$.  This is very simple - don't overthink - each solution should be one line.  We just want you to write so that you are more clear when you implement.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Code (4pts)\n",
    "\n",
    "Now you are done, you are ready to implement <code>word2vec</code>!  Please complete the implementation below.\n",
    "\n",
    "### Question 1 Implement the sigmoid function (1pt)\n",
    "\n",
    "This should be fairly easy.  Recall that sigmoid function is given by:\n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here (~1 line).\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Test sigmoid function \"\"\"\n",
    "    print(\"=== Sanity check for sigmoid ===\")\n",
    "    assert sigmoid(0) == 0.5\n",
    "    assert np.allclose(sigmoid(np.array([0])), np.array([0.5]))\n",
    "    assert np.allclose(sigmoid(np.array([1,2,3])), np.array([0.73105858, 0.88079708, 0.95257413]))\n",
    "    print(\"Tests for sigmoid passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_sigmoid() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Implement the gradient computation of naive softmax (1pt)\n",
    "\n",
    "Here, this is a function that will return the loss, the gradient with respect to $v_c$ and to $U$.\n",
    "\n",
    "1. For **loss**, recall that the loss is given by \n",
    "\n",
    "$$\\mathbf{J}_\\text{naive-softmax}(v_c, o, U) = -\\log P(O=o |C=c)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$P (O = o|C = c) = \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}$$\n",
    "\n",
    "*Implementation consideration* - use dot product to avoid unnecessary <code>for</code> loop, i.e., <code>yhat = softmax (outsideVectors @ centerWordVec)</code> should give you the dot product of all outside word vectors with the particular center word vector.  To calculate the loss for a specific $u_o$, simply put the <code>outsideWordIdx</code> as index after the softmax function.   For the softmax function, we have provided so please use it. Last, make sure that the loss is simply a scalar, i.e., shape of (1, ).\n",
    "\n",
    "2. For **gradient with respect to $v_c$**, the gradient that you have calculated should be something like this:\n",
    "\n",
    "$$\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial v_c} = -u_o + \\sum_{x \\in V} \\hat{y}_x u_x$$\n",
    "\n",
    "*Implementation consideration* - since the shape of $v_c$ is <code>(embedding_dim, )</code>, its gradient will also has the same shape.  For people who is struggling, it should look something like this <code>-trueOutsideVec + np.sum(outsideVectors * y_hat, axis=0)</code> where <code>trueOutsideVec</code> is simply <code>outsideVectors[outsideWordIdx]</code>\n",
    "\n",
    "3. For **gradient with respect to $U$**, the gradient for true outside vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w=o}} = -(v_c) + \\hat{y}_o \\cdot v_c$$\n",
    "\n",
    "For not true outside vector, it is quite similar\n",
    "\n",
    "$$v_c \\cdot \\hat{y}_{w \\neq o}$$\n",
    "\n",
    "*Implementation consideration* - note that the equation above is simply for one outside word, anyhow, as long as you use dot product, it will handle everything for you, i.e., <code>gradOutsideVecs = np.dot(y_hat, centerWordVec[:, np.newaxis].T)</code> should give you the gradient for all words except the true outside word vector.  By further subtracting it like this <code>gradOutsideVecs[outsideWordIdx] -= centerWordVec</code>, you will obtain the gradient for the true outside word vector.  Similarly above, since the shape of $U$ is <code>(vocab_size, embedding_dim)</code>, its gradient will also has the same shape.\n",
    "\n",
    "Last, you can run <code>test_naiveSoftmaxLossAndGradient()</code> to see whether your work can pass the test.  Note that gradient checking is a sanity test that only checks whether the gradient and loss values produced by your implementation are consistent with each other. Gradient check passing on its own doesn’t guarantee that you have the correct gradients. It will pass, for example, if both the loss and gradient values produced by your implementation are 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models. For those unfamiliar with numpy notation, note \n",
    "    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which\n",
    "    you can effectively treat as a vector with length x.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (embedding_dim, )\n",
    "                    (v_c in our part 1)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in our part 1)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    for all words in vocab (tranpose of U in our part 1)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (embedding_dim, )\n",
    "                     (dJ / dv_c in part 1)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    ### Please use the provided softmax function\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naiveSoftmaxLossAndGradient():\n",
    "    \"\"\" Test naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for naiveSoftmaxLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"naiveSoftmaxLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"naiveSoftmaxLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyObjects():\n",
    "    \"\"\" Helper method for naiveSoftmaxLossAndGradient and negSamplingLossAndGradient tests \"\"\"\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "\n",
    "    dataset = type('dummy', (), {})()\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    return dataset, dummy_vectors, dummy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_naiveSoftmaxLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Implement the gradient computation using negative sampling loss (1pt)\n",
    "\n",
    "1. For **loss**, recall that the negative sampling loss is\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(v_c, o, U) = -\\log(\\sigma(u_o^Tv_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*:  <code>indices</code> are given where the first index belongs to the true outside word, while the remaining $K$ number of indices belong to the negative samples.  For negative sampling, we have provided the function <code>getNegativeSamples</code> so please use it.  One good way to do this is first to calculate the dot product between all relevant outside word vectors within the selected indices and the center word vector like this <code>scores = (outsideVectors[indices] @ centerWordVec)[:, np.newaxis]</code>.  Then for the left side of the equation, use <code>scores[0]</code> as part of the calculation, and for the right side, use <code>-scores[1:]</code>.  The remaining should be easy, applying the already implemented <code>sigmoid</code> function, and <code>log</code> and <code>np.sum</code> accordingly.  Final reminder that the loss is of scalar (1, ) shape.\n",
    "\n",
    "2. For **gradient with respect to $v_c$**, the gradient that you have calculated should be something like this:\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = u_o(1 - \\sigma(u_o^Tv_c)) + \\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*: for the left side of the equation, you may want to use <code>outsideVectors[outsideWordIdx]</code>, and for the right side of the equation, use <code>outsideVectors[negSampleWordIndices]</code>.  Other than that, this should be fairly simple.  Remind that the output shape is <code>(embedding_dim, )</code>\n",
    "\n",
    "3. For **gradient with respect to $U$**, there are two parts, the gradient for true outside vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_o} = -v_c(1 - \\sigma(u_o^Tv_c))$$\n",
    "\n",
    "The gradient for negative vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_k} = v_c(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*:  Both of the gradient should be simple to implement using the indexing approach we have done before.  There is some technicality, i.e., the same word may be negatively sampled multiple times. For example if an outside word is sampled twice, you shall have to double count the gradient with respect to this word. Thrice if it was sampled three times, and so forth.  A good way to do this is to first count the occurrences of indices like this:  <code>indexCount = np.bincount(indices)[:, np.newaxis]</code>, then loop through all distinct indices and multiply the gradients with the number of occurences like this: <code>for i in np.unique(indices): gradOutsideVecs[i] *= indexCount[i]</code>\n",
    "\n",
    "Last, for sanity checking, run the <code>test_negSamplingLossAndGradient</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have provided the function for getting negative samples\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices #[K, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you.\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negSamplingLossAndGradient():\n",
    "    \"\"\" Test negSamplingLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for negSamplingLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"negSamplingLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"negSamplingLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_negSamplingLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 Implement the skipgram model (1pt)\n",
    "\n",
    "First, the string of the current center word will be send as one of the argument to the function. What you want to do is to obtain its index using the <code>word2Ind</code> function.  Also, you want to obtain the center word vector by passing the index to the <code>centerWordVectors</code>.\n",
    "\n",
    "Second, loop through the list of <code>outsideWords</code>, in each loop, get the index using the <code>word2Ind</code> function, then pass whatever required into the function like this <code>currLoss, currGradCenter, currGradOutside = word2vecLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset)</code>, then accumulate the loss and gradients of $v_c$ and $U$, e.g., <code>loss += currLoss</code>.   \n",
    "\n",
    "Last, recall Question 7 Part 1 where the gradient is 0 when $w \\neq c$, thus after the for loop, we have to clear out the gradients of non-center word like this:  <code>gradCenterVecs[np.arange(gradCenterVecs.shape[0]) != currCenterWordIdx] = 0</code>\n",
    "\n",
    "Note that <code>word2vecLossAndGradient</code> is a wrapper function that can call both <code>negSamplingLossAndGradient</code> and <code>naiveSoftmaxLossAndGradient</code>.  You do not have to do anything; it's already handle for you internallly in the test.\n",
    "\n",
    "For sanity check, feel free to run <code>test_skipgram</code> and <code>test_word2vec</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) is in shape \n",
    "                        (vocab_size, embedding_dim) \n",
    "                        for all words in vocab\n",
    "    outsideVectors -- outside vectors is in shape \n",
    "                        (vocab_size, embedding_dim) \n",
    "                        for all words in vocab\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J)\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (embedding_dim, )\n",
    "                     (dJ / dv_c)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "    \n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skipgram():\n",
    "    \"\"\" Test skip-gram with naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_skipgram()  #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations\"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "            [tokens[random.randint(0, 4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                  dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    )\n",
    "    )\n",
    "\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Skip-Gram with negSamplingLossAndGradient\")\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5, :],\n",
    "                  dummy_vectors[5:, :], dataset, negSamplingLossAndGradient)\n",
    "    )\n",
    "    )\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put the code in action!\n",
    "\n",
    "In this part, you do not have to do anything, just simply run the remaining code and see how your word2vec works out on a real dataset.  This will take around 1.5 hours (40,000 iterations) so go and take some rest.  The output will be a image of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        def postprocessing(x): return x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        loss = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        loss, grad = f(x)\n",
    "        # Take step in direction of gradient.\n",
    "        x -= step * grad\n",
    "\n",
    "        # END YOUR CODE\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    def quad(x): return (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print(\"Running sanity checks...\")\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 1 result:\", t1)\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 2 result:\", t2)\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 3 result:\", t3)\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ALL TESTS PASSED\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.treebank import StanfordSentiment\n",
    "# import os.path as op\n",
    "# import time\n",
    "\n",
    "# # Reset the random seed to make sure that everyone gets the same results\n",
    "# random.seed(314)\n",
    "# dataset = StanfordSentiment()\n",
    "# tokens = dataset.tokens()\n",
    "# nWords = len(tokens)\n",
    "\n",
    "# # We are going to train 10-dimensional vectors for this assignment\n",
    "# dimVectors = 10\n",
    "\n",
    "# # Context size\n",
    "# C = 5\n",
    "\n",
    "# # Reset the random seed to make sure that everyone gets the same results\n",
    "# random.seed(31415)\n",
    "# np.random.seed(9265)\n",
    "\n",
    "# startTime=time.time()\n",
    "# wordVectors = np.concatenate(\n",
    "#     ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "#        dimVectors, np.zeros((nWords, dimVectors))),\n",
    "#     axis=0)\n",
    "# wordVectors = sgd(\n",
    "#     lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "#         negSamplingLossAndGradient),\n",
    "#     wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# # Note that normalization is not called here. This is not a bug,\n",
    "# # normalizing during training loses the notion of length.\n",
    "\n",
    "# print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "# print(\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n",
    "# # concatenate the input and output word vectors\n",
    "# wordVectors = np.concatenate(\n",
    "#     (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "#     axis=0)\n",
    "\n",
    "# visualizeWords = [\n",
    "#     \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "#     \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "#     \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "#     \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "# visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "# visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "# temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "# covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "# U,S,V = np.linalg.svd(covariance)\n",
    "# coord = temp.dot(U[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for i in range(len(visualizeWords)):\n",
    "#     plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "#         bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "# plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "# plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "# plt.savefig('word_vectors.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
