{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#make sure you got these folder\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Word2Vec\n",
    "\n",
    "### Estimated Time: ~10 hours\n",
    "\n",
    "**Quick note**:  This assignment may be overwhelming for some of you.  It may be wise to set aside some significant amount of time so you can slowly go over this assignment.\n",
    "\n",
    "Letâ€™s have a quick refresher on the word2vec algorithm. For full details, you may want to rewatch the zoom video we did in our first two lectures.  \n",
    "\n",
    "The key insight behind word2vec is that *a word is known by the company it keeps*. Concretely, suppose we have a **center** word $c$ and a contextual window. We shall refer to words that lie in this contextual window as **outside words** denoting $o$. For example, in Figure 1 we see that the center word $c$ is *banking*. Since the context window size is 2, the outside words are *turning*, *into*, *crises*, and *as*.\n",
    "\n",
    "The goal of the skip-gram word2vec algorithm is to accurately learn the probability distribution $P(O|C)$. Given a specific word $o$ and a specific word $c$, we want to calculate $P (O = o|C = c)$, which is the probability that word $o$ is an *outside* word for $c$, i.e., the probability that $o$ falls within the contextual window of $c$.\n",
    "\n",
    "<img src = \"img/word2vec.png\" width=400>\n",
    "\n",
    "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "\n",
    "$$P (O = o|C = c) = \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}$$\n",
    "\n",
    "Here, $u_o$ is the *outside* vector representing outside word $o$, and $v_c$ is the *center* vector representing center word $c$. To contain these parameters, we have two matrices, $U$ and $V$ . The columns of $U$ are all the *outside* vectors $u_w$. The columns of $V$ are all of the *center* vectors $v_w$. Both $U$ and $V$ contain a vector for every $w \\in \\text{Vocabulary}$.\n",
    "\n",
    "Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:\n",
    "\n",
    "$$\\mathbf{J}_\\text{naive-softmax}(v_c, o, U) = -\\log P(O=o |C=c)$$\n",
    "\n",
    "We can view this loss as the cross-entropy2 between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. \n",
    "\n",
    "Furthermore, the $k$th entry in these vectors indicates the conditional probability of the $k$th word being an *outside word* for the given $c$. The true empirical distribution $y$ is a one-hot vector with a 1 for the true outside word $o$, and $0$ everywhere else. The predicted distribution $\\hat{y}$ is the probability distribution $P (O|C = c)$ given by our model in above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Math behind word2vec\n",
    "\n",
    "### Question 1 (1pt)\n",
    "\n",
    "#### <font color=\"red\">Answer the following questions</font> \n",
    "\n",
    "1. What is $U$ and the shape of $U$?\n",
    "2. What is $V$ and the shape of $V$?\n",
    "3. What is $u_o$ and the shape of $u_o$?\n",
    "4. What is $v_c$ and the shape of $v_c$?\n",
    "5. What is $y$ and the shape of $y$?\n",
    "6. What is $\\hat{y}$ and the shape of $\\hat{y}$?\n",
    "7. What is the numeric range of the softmax function P (O = o|C = c)?\n",
    "8. Why use $\\log$ after the softmax function?\n",
    "\n",
    "Solution:\n",
    "\n",
    "1. $U$ is the outside words matrix of shape <code>(vocab_size, embedding_dim)</code>\n",
    "2. $V$ is the center word matrix of shape <code>(vocab_size, embedding_dim)</code>\n",
    "3. $u_o$ is the embedding vector holding a particular outside word $o$ of shape <code>(embedding_dim, 1)</code>\n",
    "4. $v_c$ is the embedding vector holding a particular center word $c$ of shape <code>(embedding_dim, 1)</code> \n",
    "5. $y$ is the true distribution one-hot vector with 1 for the true outside word $o$ and 0 everywhere else; has shape of <code>(vocab_size, 1)</code>\n",
    "6. $\\hat{y}$ is the predicted distribution one-hot vector given by our model $P(O|C = c)$; has shape of <code>(vocab_size, 1)</code>\n",
    "7. 0 to 1\n",
    "8. $\\log$ has many nice properties; (1) helps numerically because the product of a large number of small probabilities can easily underflow; this is resolved by computing instead the sum of the log probabilities; (2) cancel out nicely with $\\exp$, (3) we can use $\\log$ because it is a monotically increasing function, thus it won't affect the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1pt)\n",
    "\n",
    "Show that the naive-softmax loss is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that\n",
    "\n",
    "$$-\\sum_{w \\in V}y_w \\log(\\hat{y}_w) = -\\log(\\hat{y}_o)$$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "*(you may need to study latex to write your answers)*\n",
    "\n",
    "Because $y$ is a one-hot encoder vector with zeros everywhere except at the index $w=o$  where $y_o = 1$, the sum is actually just\n",
    "\n",
    "$$-\\sum_{w \\in V} y_w \\log(\\hat{y}_w) = -(y_1 \\log(\\hat{y}_1) + \\cdots + y_o \\log(\\hat{y}_o) + \\cdots + y_{|V|} \\log (\\hat{|V|}) = \\log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to $v_c$.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial v_c} &= \\frac{\\partial}{\\partial v_c}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial v_c}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= -\\frac{\\partial}{\\partial v_c}[\\log{\\exp({u_o^{T} v_c})} - \\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\\n",
    "&= -\\frac{\\partial}{\\partial v_c}[\\log{\\exp({u_o^{T} v_c})}] + \\frac{\\partial}{\\partial v_c} [\\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\ \n",
    "&= -\\frac{\\partial}{\\partial v_c}[u_o^{T} v_c] + \\frac{\\partial}{\\partial v_c} [\\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\\n",
    "&= -(u_o) + (\\frac{1}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}\\sum_{x \\in V}u_x \\dot \\exp({u_x^T v_c}))\\\\\n",
    "&= -u_o + \\sum_{x \\in V}\\frac{\\exp({u_x^T v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}u_x\\\\\n",
    "&= -u_o + \\sum_{x \\in V} p(u_x | v_c) u_x \\\\\n",
    "&= -u_o + \\sum_{x \\in V} \\hat{y}_x u_x\n",
    "\\end{align*}$$\n",
    "\n",
    "This says that the gradient of the loss function w.r.t. the center word is equal to the difference between the observed representation of the outside context word and the expected word according to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to each of the outside word vectors $u_w$'s.  There will be two cases:  when $w = o$ , the true outside word vector, and $w \\neq o$ for all other words.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "**Case 1** - the outside word vector is the true context word vector:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w=o}} &= \\frac{\\partial}{\\partial  u_{w=o}}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w=o}}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= -\\frac{\\partial}{\\partial u_{w=o}}[u_o^Tv_c] + \\frac{\\partial}{\\partial u_{w=o}}[\\log \\sum_{w \\in V} \\exp({u_w^{T} v_c})]\\\\\n",
    "&= -(v_c) + (\\frac{1}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}(\\exp({u_o^{T} v_c}) \\cdot v_c))\\\\\n",
    "&= -(v_c) + (\\frac{\\exp({u_o^{T} v_c})}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}\\cdot v_c)\\\\\n",
    "& = -(v_c) + \\hat{y}_o \\cdot v_c\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "**Case 2** - the outside word vector is the NOT true context word vector:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w\\neq o}} &= \\frac{\\partial}{\\partial  u_{w \\neq o}}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w \\neq o}}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w \\neq o}}[u_o^T v_c] +  \\frac{\\partial}{\\partial u_{w \\neq o}}[\\log \\sum_{w \\in V} \\exp{(u_w^T v_c)}]\\\\\n",
    "&= 0 + (\\frac{1}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}(\\exp({u_{w \\neq o}^{T} v_c}) \\cdot v_c))\\\\\n",
    "&= v_c \\cdot \\hat{y}_{w \\neq o}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1pt)\n",
    "\n",
    "Compute the derivatives of the sigmoid function given by \n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "The derivative of sigmoid function is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{dg}{dx} &= \\frac{0(1 + e^{-x}) - (-1)(e^{-x}))}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{e^{-x}}{(1 + e^{-x})^2}  = \\frac{e^{-x} + 1 - 1}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{1}{(1 + e^{-x})} - \\frac{1}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{1}{(1 + e^{-x})} \\big(1 - \\frac{1}{(1 + e^{-x})}\\big)\\\\\n",
    "    &= g(1 - g)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1pt)\n",
    "\n",
    "Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\\cdots,w_K$ and their outside vectors as $u_1,\\cdots,u_K$ For this question, assume that the $K$ negative samples are distinct. In other words, $i \\neq j$ implies $w_i \\neq w_j$ for $i,j \\in \\{1,\\cdots,K\\}$. Note that $o \\notin \\{w_1,\\cdots,w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(v_c, o, U) = -\\log(\\sigma(u_o^Tv_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "Compute the partial derivatives of $\\mathbf{J}_{\\text{neg-sample}}$ with respect of $v_c, u_o, and u_k$.  Please write your answers in terms of the vectors $u_o, v_c, \\text{and} u_k$.  \n",
    "\n",
    "After this, explain with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "(1) **w.r.t. $v_c$**:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = \\frac{\\partial}{\\partial v_c}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial v_c}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]$$\n",
    "\n",
    "For the first term, let $S = \\sigma(u_o^Tv_c)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial v_c}[-\\log(\\sigma(u_o^Tv_c))]  &= (\\frac{\\partial}{\\partial v_c}[-\\log(S)])(\\frac{\\partial}{\\partial v_c}[S])\\\\\n",
    "&= (-\\frac{1}{\\sigma(u_o^Tv_c)})(u_o\\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c)))\\\\\n",
    "&= u_o(1 - \\sigma(u_o^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "For the second term, let $S = \\sigma(-u_k^Tv_c)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial v_c}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))] &= \\sum_{k=1}^{K} \\frac{\\partial}{\\partial v_c}[\\log(S)] \\frac{\\partial}{\\partial v_c}[S]\\\\\n",
    "&= \\sum_{k=1}^{K} \\frac{-u_k\\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))}{\\sigma(-u_k^Tv_c)}\\\\\n",
    "&= -\\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "Combining the two answers, we get:\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = u_o(1 - \\sigma(u_o^Tv_c)) + \\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "(2) **w.r.t. $u_o$**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_o} &= \\frac{\\partial}{\\partial u_o}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial u_o}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_o}[-\\log(\\sigma(u_o^Tv_c))] - 0\\\\\n",
    "&= -[\\frac{1}{\\sigma(u_o^Tv_c)}][\\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))v_c]\\\\\n",
    "&= -v_c(1 - \\sigma(u_o^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "(3) **w.r.t. $u_k$**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_k} &= \\frac{\\partial}{\\partial u_k}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial u_k}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= 0 - \\frac{\\partial}{\\partial u_k}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= -\\frac{-v_c\\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))}{\\sigma(-u_k^Tv_c)}\\\\\n",
    "&= v_c(1 - \\sigma(-u_k^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "This loss function is much more efficient because it only takes $O(K)$, whereas the naive method requires us to normalize looking all word vectors, taking $O(|V|)$\n",
    "\n",
    "### Question 7 (1pt)\n",
    "\n",
    "Suppose the center word is $c = w_t$ and the context window is $[w_{tâˆ’m}, \\cdots, w_{tâˆ’1}, w_t, w_{t+1}, \\cdots, w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
    "\n",
    "$$\\mathbf{J}_{skip-gram}(v_c, w_{t-m}, \\cdots, w_{t+m}, U) = \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\mathbf{J} (v_c, w_{t+j}, U)$$\n",
    "\n",
    "Here, $\\mathbf{J}(v_c, w_{t+j}, U)$ represents an arbirtary loss term for the center word $c=w_t$ and outside word $w_{t+j}$.  $\\mathbf{J}(v_c, w_{t+j}, U)$ could be $\\mathbf{J}_{\\text{naive-softmax}}$ or $\\mathbf{J}_{\\text{neg-sample}}$ depending on your implementation.\n",
    "\n",
    "Write down three partial derivatives:\n",
    "\n",
    "- (i) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial U}$\n",
    "- (ii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_c}$\n",
    "- (iii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_w} \\text{ where } w \\neq c$\n",
    "\n",
    "Write your answers in terms of $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial U$ and $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial v_c$.  This is very simple - don't overthink - each solution should be one line.  We just want you to write so that you are more clear when you implement.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "- (i) $\\displaystyle\\sum_{-m \\leq j \\leq m, j \\neq 0} \\partial \\mathbf{J}_{skip-gram}(v_c, w_{t+j}, U)/\\partial U$\n",
    "- (ii) $\\displaystyle\\sum_{-m \\leq j \\leq m, j \\neq 0} \\partial \\mathbf{J}_{skip-gram}(v_c, w_{t+j}, U)/\\partial v_c$\n",
    "- (iii) 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Code\n",
    "\n",
    "Now you are done, you are ready to implement <code>word2vec</code>!  Please complete the implementation below.\n",
    "\n",
    "### Question 1 Implement the sigmoid function (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here (~1 line).\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Test sigmoid function \"\"\"\n",
    "    print(\"=== Sanity check for sigmoid ===\")\n",
    "    assert sigmoid(0) == 0.5\n",
    "    assert np.allclose(sigmoid(np.array([0])), np.array([0.5]))\n",
    "    assert np.allclose(sigmoid(np.array([1,2,3])), np.array([0.73105858, 0.88079708, 0.95257413]))\n",
    "    print(\"Tests for sigmoid passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity check for sigmoid ===\n",
      "Tests for sigmoid passed!\n"
     ]
    }
   ],
   "source": [
    "test_sigmoid() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Implement the gradient computation of naive softmax (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models. For those unfamiliar with numpy notation, note \n",
    "    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which\n",
    "    you can effectively treat as a vector with length x.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (embedding_dim, )\n",
    "                    (v_c in our part 1)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in our part 1)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    for all words in vocab (tranpose of U in our part 1)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (embedding_dim, )\n",
    "                     (dJ / dv_c in part 1)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    ### Please use the provided softmax function\n",
    "    \n",
    "    \n",
    "    scores = outsideVectors @ centerWordVec  # UT @ v_c : [vocab_size, embed] @ [embed, 1] = [vocab_size, ]\n",
    "    y_hat = softmax(scores)[:, np.newaxis]  #y_hat: [vocab_size, 1]\n",
    "\n",
    "    loss = float(-np.log(y_hat[outsideWordIdx])) #naive-softmax loss: scalar (1, )\n",
    "\n",
    "    #dj / dv_c\n",
    "    trueOutsideVec = outsideVectors[outsideWordIdx] #trueOutsideVec:  [embed, ]\n",
    "    gradCenterVec = -trueOutsideVec + np.sum(outsideVectors * y_hat, axis=0) # [embed, ] + [embed, ] = [embed, ]  (refer to broadcasting rule if you don't understand here)\n",
    "\n",
    "    #dJ / dU\n",
    "    gradOutsideVecs = np.dot(y_hat, centerWordVec[:, np.newaxis].T) # y_hat @ centerWordVec : [vocab_size, 1] @ [1, embed] = [vocab_size, embed]\n",
    "    gradOutsideVecs[outsideWordIdx] -= centerWordVec #[vocab_size, embed]\n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naiveSoftmaxLossAndGradient():\n",
    "    \"\"\" Test naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for naiveSoftmaxLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"naiveSoftmaxLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"naiveSoftmaxLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyObjects():\n",
    "    \"\"\" Helper method for naiveSoftmaxLossAndGradient and negSamplingLossAndGradient tests \"\"\"\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "\n",
    "    dataset = type('dummy', (), {})()\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    return dataset, dummy_vectors, dummy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "test_naiveSoftmaxLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Implement the gradient computation using negative sampling loss (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have provided the function for getting negative samples\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negSamplingLossAndGradient():\n",
    "    \"\"\" Test negSamplingLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for negSamplingLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"negSamplingLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"negSamplingLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_negSamplingLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Implement the skipgram model (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (V in pdf handout)\n",
    "    outsideVectors -- outside vectors is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (transpose of U in the pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "    \n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skipgram():\n",
    "    \"\"\" Test skip-gram with naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_skipgram()  #turn on when you are ready to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
